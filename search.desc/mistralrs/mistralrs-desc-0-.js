searchState.loadedDescShard("mistralrs", 0, "This crate provides an asynchronous API to <code>mistral.rs</code>.\nAllow automatic selection of any given tool, or none.\nChat completion streaming request chunk.\nAn OpenAI compatible chat completion response.\nTemplate for chat models including bos/eos/unk as well as …\nChat completion choice.\nChat completion streaming chunk choice.\nCompletion request choice.\nChat completion streaming chunk choice.\nCompletion request choice.\nAn OpenAI compatible completion response.\nControl the constraint with llguidance.\nCustomizable logits processor.\nThe different types of elements allowed in tensors.\nThe scheduler method controld how sequences are scheduled …\nDelta in content for streaming response.\nRequest to detokenize some text.\nMetadata to initialize the device mapper.\nA loader for a vision (non-quantized) model.\nA builder for a loader for a vision (non-quantized) model.\nThe architecture to load the vision model as.\nSelect a diffusion plain model, without quantization or …\nConfig specific to loading a vision model.\nEngine instructions, per Engine (MistralRs) ID.\nContains the error value\nSelect a GGML model.\nA loader for a GGML model.\nA builder for a GGML loader.\nConfig for a GGML loader.\nSelect a GGUF model.\nLoader for a GGUF model.\nA builder for a GGUF loader.\nConfig for a GGUF loader.\n<code>NormalLoader</code> for a Gemma model.\n<code>VisionLoader</code> for an Idefics 2 Vision model.\nImage generation response format\n<code>VisionLoader</code> for an LLaVA Vision model.\n<code>VisionLoader</code> for an LLaVANext Vision model.\nA device mapper which does device mapping per hidden layer.\nA value of type <code>L</code>.\n<code>NormalLoader</code> for a Llama model.\nThe <code>Loader</code> trait abstracts the loading process. The …\nA builder for a loader using the selected model.\nAll local paths and metadata necessary to load a model.\nLogprobs per token.\nSelect a LoRA architecture\nSelect a GGML model with LoRA.\nSelect a GGUF model with LoRA.\nThe MistralRs struct handles sending requests to the …\nThe MistralRsBuilder takes the pipeline and a scheduler …\nCategory of the model. This can also be used to extract …\nDType for the model.\nThe kind of model to build.\n<code>ModelPaths</code> abstracts the mechanism to get all necessary …\nOnly quantize MoE experts, if applicable. The enables MoQE.\nDisallow selection of tools.\nA loader for a “normal” (non-quantized) model.\nA builder for a loader for a “normal” (non-quantized) …\nThe architecture to load the normal model as.\nA normal request request to the <code>MistralRs</code>.\nConfig specific to loading a normal model.\nContains the success value\nAdapter model ordering information.\nAll memory counts in MB. Default for block size is 32.\n<code>NormalLoader</code> for a Phi 2 model.\n<code>NormalLoader</code> for a Phi 3 model.\n<code>VisionLoader</code> for a Phi 3 Vision model.\nSelect a plain model, without quantization or adapters\n<code>NormalLoader</code> for a Qwen 2 model.\nA request to the Engine, encapsulating the various …\nMessage or messages for a <code>Request</code>.\nThe response enum contains 3 types of variants:\nA logprob with the top logprobs for this token.\nChat completion response message.\nA value of type <code>R</code>.\nSampling params are used to control sampling.\nMetadata for a speculative pipeline\nA loader for a speculative pipeline using 2 <code>Loader</code>s.\nSpeculative decoding pipeline: …\n<code>NormalLoader</code> for a Starcoder2 model.\nStop sequences or ids.\nTerminate all sequences on the next scheduling step. Be …\nThe core struct for manipulating tensors.\nThe source of the HF token.\nRequest to tokenize some messages or some text.\nSelect the model from a toml file\nForce selection of a given tool.\nTop-n logprobs element\nType which can be converted to a DType\nOpenAI compatible (superset) usage during a request.\nA loader for a vision (non-quantized) model.\nA builder for a loader for a vision (non-quantized) model.\nThe architecture to load the vision model as.\nSelect a vision plain model, without quantization or …\nPrepend a vision tag appropriate for the model to the …\nConfig specific to loading a vision model.\nSelect an X-LoRA architecture\nSelect a GGML model with X-LoRA.\nSelect a GGUF model with X-LoRA.\nThis operation multiplies the input tensor by <code>mul</code> then …\nLogits and sequence context (prompt and generated tokens), …\nRun the <code>forward</code> method of <code>m</code> on <code>self</code>.\nApplies a unary custom op.\nApplies a unary custom op without backward support\nApplies a binary custom op.\nApplies a binary custom op without backward support\nApplies a ternary custom op.\nApplies a ternary custom op without backward support\nRun the <code>forward</code> method of <code>m</code> on <code>self</code>.\nCreates a new 1D tensor with values from the interval …\nCreates a new 1D tensor with values from the interval …\nReturns the indices that sort the tensor along the last …\nSimilar to <code>argmax_keepdim</code> but the target dimension is …\nSimilar to <code>argmin_keepdim</code> but the target dimension is …\nConvert the response into a result form.\nString representation for dtypes.\n2D average pooling over an input tensor with multiple …\nSame as <code>avg_pool2d</code> but with a <code>stride</code> that can be set to a …\nReturn <code>BF16</code> for devices that support it, otherwise default …\nBroadcast the input tensor to the target shape. This …\nReturns a new tensor duplicating data from the original …\nMatrix-multiplication with broadcasting support.\nMatrix-multiplication with broadcasting support and fused …\nBroadcasting version of <code>pow</code>.\nIf the loader type is not specified, loader type is …\nConcatenates two or more tensors along a particular …\nJinja format chat templating for chat completion.\nSplit a tensor into the specified number of chunks, this …\nClamp the tensor values to be between <code>min</code> and <code>max</code>.\nElement-wise comparison between two tensors, e.g. …\nReturns a tensor that is in row major order. This is the …\nApplies a 1D convolution over the input tensor.\nApplies a 2D convolution over the input tensor.\nApplies a 1D transposed convolution over the input tensor.\nApplies a 2D transposed convolution over the input tensor.\nCompared to clone, this copies the actual storage but may …\nThe cross-entropy loss.\nReturns the cumulative sum of elements of the input tensor …\nImage dimensions will be 720x1280.\nReturns a new tensor detached from the current graph, …\nThis sets up the parameters so that there is:\nThe device on which the input tensor is located.\nThe dimension size for a specified dimension index.\nThe dimension size for this tensor on each axis.\nThe dtype for the elements stored in the input tensor.\nA device mapper to not map device.\nThe number of elements stored in this tensor.\nApplies the Exponential Linear Unit (ELU) function on each …\nReturns a tensor with the values from the <code>self</code> tensor at …\nCreates a new tensor filled with uninitialized memory.\nCreate an empty topology.\nCreates a new tensor filled with uninitialized memory of …\nElement-wise equality.\nAn alias for broadcast_as.\nReturns a matrix with a diagonal of ones of size n by n.\nFlattens the input tensor on the dimension indexes from …\nFlattens the input tensor by reshaping it into a one …\nFlattens the input tensor on the dimension indexes from …\nFlattens the input tensor on the dimension indexes from <code>0</code> …\nReturns a tensor that is in row major order. This always …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreates a new 1D tensor from an iterator.\nCreates a new tensor initialized with values from the …\nCreates a new tensor initialized with values from the …\nReturns a new tensor with all the elements having the same …\nγ completions to run of the draft model\nGather values across the target dimension.\nElement-wise comparison with greater-equal, the returned …\nReturns the sub-tensor fixing the index at <code>i</code> on the first …\nConfiguration of optional adapters. <code>(String, String)</code> is of …\nOptional adapter files. <code>(String, PathBuf)</code> is of the form …\nGet the explicit chat template. If specified, this …\n<code>XLoraConfig</code> for the XLORA classifier\nFilepath for the XLORA classifier\nRetrieve the <code>PretrainedConfig</code> file.\nGet the current seed for the device RNG.\nFilepath for general model configuration.\nInformation for preloading LoRA adapters (adapter name, …\nAmount of available memory in bytes.\nReturns the sub-tensor fixing the index at <code>index</code> on the …\nReturn the defined ordering of adapters and layers within …\nGet the preprocessor config (for the vision models). This …\nGet the processor config (for the vision models). This is …\nFile where the content is expected to deserialize to …\nA serialised <code>tokenizers.Tokenizer</code> HuggingFace object.\nAmount of total memory in bytes.\nModel weights files (multiple files supported).\nElement-wise comparison with greater-than, the returned …\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nsee [TensorIndex#method.i]\nThe unique identifier for this tensor.\nAccumulate element from <code>source</code> at indexes <code>indexes</code> and add …\nSelect values for the input tensor at the target indexes …\nThis should be called to initialize the debug flag and …\nApplies a unary custom op in place.\nApplies a unary custom op in place (for the first tensor).\nApplies a ternary custom op in place (for the first …\nInterpolate the input tensor to the <code>target_size</code> size, …\nInterpolate the input tensor to the <code>(target_h, target_w)</code> …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the data is stored in a C contiguous (aka …\nReturns true if the data is stored in a Fortran contiguous …\nWhether this tensor is a variable or not. A variable is a …\nThe layout of the input tensor, this stores both the shape …\nElement-wise comparison with lower-equal, the returned …\nIf <code>revision</code> is None, then it defaults to <code>main</code>. If <code>dtype</code> is …\nLoad a model from the specified paths. Also initializes …\nReturns log(sum(exp(tensor), dim)).\nIf <code>training == true</code>, <code>loss_csv_path</code> will not save anything. …\nElement-wise comparison with lower-than, the returned …\nReturns the matrix-multiplication of the input tensor with …\nReturns the matrix-multiplication of the input tensor with …\nReturns the matrix-multiplication of the input tensor with …\nSimilar to <code>max_keepdim</code> but the target dimension is …\nGathers the maximum value across the selected dimension. …\n2D max pooling over an input tensor with multiple channels.\nSame as <code>max_pool2d</code> but with a <code>stride</code> that can be set to a …\nReturns the mean of all elements in the input tensor. The …\nReturns the mean of all elements in the input tensor. The …\nCreates grids of coordinates specified by the 1D inputs.\nSimilar to <code>min_keepdim</code> but the target dimension is …\nGathers the minimum value across the selected dimension. …\nReturns a new tensor that is a narrowed version of the …\nElement-wise non-equality.\nCreates a new tensor on the specified device using the …\nNOTE: Until v0.4.0, you should make sure to call …\nCreate a loader builder for a GGUF model. <code>tok_model_id</code> is …\nNOTE: Until v0.4.0, you should make sure to call …\nNormalize a ‘relative’ axis value: positive values are …\nCreates a new tensor filled with ones.\nCreates a new tensor filled with ones with same shape, …\nPad the input tensor using same values along dimension <code>dim</code>…\nPad the input tensor using 0s along dimension <code>dim</code>. This …\nParse ISQ value: one of\nReturns a tensor with the same data as the input where the …\nPointwise pow operation.\nRaise the tensor to some float exponent <code>e</code>.\nPrefix for inclusion in messages (may do nothing if the …\nCreates a new tensor initialized with values sampled …\nCreates a new tensor initialized with values sampled from …\nThe number of dimensions for this tensor, 0 for a scalar …\nReads a npy file and return the stored multi-dimensional …\nReads a npz file and returns the stored multi-dimensional …\nReads a npz file and returns the stored multi-dimensional …\nRepeat this tensor along the specified dimensions.\nReshape returns a tensor with the target shape provided …\nRoll the tensor input along the given dimension. Elements …\nRound element of the input tensor to the nearest integer.\nThe tensor shape, i.e. dimension sizes on each axis.\nThe size used by each element in bytes, i.e. 1 for <code>U8</code>, 4 …\nReturns a copy of <code>self</code> where the values within <code>ranges</code> have …\nEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor …\nEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor …\nSet the values on <code>self</code> using values from <code>src</code>. The copy …\nSorts the tensor along the last dimension, returns the …\nCreates a new tensor with the specified dimension removed …\nStacks two or more tensors along a particular dimension.\nReturns the total of model execution time.\nThe storage used by this tensor, together with the layout …\nSimilar to <code>strided_index</code> but returns the position of the …\nReturns an iterator over position of the elements in the …\nReturns the sum of all elements in the input tensor. The …\nComputes the sum of all the elements in this tensor and …\nReturns the sum of all elements in the input tensor. The …\nReturns a tensor that is a transposed version of the …\nIf the target device is the same as the tensor device, …\nCasts the input tensor to the target <code>dtype</code>.\nRetrieves the single scalar value hold in the tensor. If …\nAn alias for <code>to_scalar</code>.\nReturns the data contained in a 1D tensor as a vector of …\nReturns the data contained in a 2D tensor as a vector of …\nReturns the data contained in a 3D tensor.\nReturns true if the computation graph should track this …\nReturns a tensor that is a transposed version of the …\nReturns a lower triangular matrix of ones of size n by n.\nReturns an upper triangular matrix of ones of size n by n.\nReturns a view of which contains all slices of size <code>size</code> …\nCreates a new tensor with a dimension of size one inserted …\nAlias for <code>interpolate1d</code>.\nAlias for <code>interpolate2d</code>.\nThis will be the API as of v0.4.0. Other APIs will <em>not</em> be …\nReturns the unbiased variance over the selected dimension.\nReturns the unbiased variance over the selected dimension.\nReturns a tensor with the same shape as the input tensor, …\nThis setting is only applicable on CUDA. If set to false …\nWrites a multi-dimensional array in the npy format.\nWrites multiple multi-dimensional arrays using the npz …\nCreates a new tensor filled with zeros.\nCreates a new tensor filled with ones with same shape, …\nModel ID to load LoRA from. This may be a HF hub repo or a …\nModel ID to load LoRA from. This may be a HF hub repo or a …\nModel ID to load LoRA from. This may be a HF hub repo or a …\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nGenerate and utilize an imatrix to enhance GGUF …\nGenerate and utilize an imatrix to enhance GGUF …\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\n.toml file containing the selector configuration.\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nGQA value\nGQA value\nGQA value\n.imatrix file to enhance GGUF quantizations with. …\nAutomatically resize and pad images to this maximum edge …\nModel ID to load from. This may be a HF hub repo or a …\nForce a base model ID to load from instead of using the …\nForce a base model ID to load from instead of using the …\nModel ID to load from. This may be a HF hub repo or a …\nModel ID to load from. This may be a HF hub repo or a …\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nISQ organization: <code>default</code> or <code>moqe</code> (Mixture of Quantized …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename.\nQuantized filename.\nQuantized filename.\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nIndex of completion tokens to generate scalings up until. …\nIndex of completion tokens to generate scalings up until. …\nIndex of completion tokens to generate scalings up until. …\n<code>tok_model_id</code> is the local or remote model ID where you can …\n<code>tok_model_id</code> is the local or remote model ID where you can …\n<code>tok_model_id</code> is the local or remote model ID where you can …\nModel ID to load the tokenizer from. This may be a HF hub …\nModel ID to load the tokenizer from. This may be a HF hub …\nModel ID to load the tokenizer from. This may be a HF hub …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nRoPE for Llama3\nMatrix multiplication, configurable to be via f16 (to use …\nRoPE supporting LongRope\n(cos, sin)\nExpands a mask from (bs, seq_len) to (bs, 1, tgt_len, …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCompute matrix-matrix product, optionally casting to f16 …\nCompute matrix-matrix product, optionally casting to f16 …\nGemma uses weight + 1.0\nCompute quantized matrix-matrix product, optionally …\nCompute quantized matrix-matrix product, optionally …\nComputes softmax(QK^T*sqrt(d_k))V\nGemma uses weight + 1.0. Undo for UQFF generation.\nCheck whether the current parser state forces the sequence …\ncommit_token() is a top-level method in this file and is …\nCheck if there are any tokens to fast-forward, forced by …\nThis computes token sampling mask. It typically takes up …\nCompute and then consume fast-forward tokens.\nExtend the current state of the parser with given token. …\nThis is the primary interface for llguidance – the one …\nLogs to be sent to the user.\nThis returns parser outputs to be passed back to the user. …\nThis can be called before the first compute_mask() to walk …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConstruct a state machine for a sequence constraint.\nYou can call this first with the prompt from the user, …\nParse HF tokenizer.json file and return bytes for every …\nReturns how many of the passed tokens can be accepted by …\nIntersection of the regexes\nMatches this byte only. If byte is not in 0..127, it may …\nMatches this string of bytes only. Can lead to invalid …\nMatches any byte in the set, expressed as bitset. Can lead …\nConcatenation of the regexes\ncbindgen:ignore\nMatches the empty string. Same as Concat([]).\nTop-level parser allowed EOS (as it was in an accepting …\nGenerate according to regex.\nGenerate according to specified grammar.\nSomething went wrong with creating a nested parser.\nGenerate all of the nodes in sequence.\nLexeme in a greedy grammar.\nThe lexer is too complex\nMatches this string only\nMatches the regex; should be at the end of the main regex. …\nmax_tokens limit on the number of tokens in the top-level …\nmax_tokens limit on the total number of tokens has been …\nTop-level parser indicates that no more bytes can be added.\nTop-level parser indicates that no more bytes can be …\nMatches nothing. Same as Or([]).\nOptional fields allowed on any Node\nMatches everything the regex doesn’t match. Can lead to …\nParser has not emitted stop() yet.\nUnion of the regexes\nThe parser is too complex\nCompile the regex using the regex_syntax crate\nRepeat the regex at least min times, at most max times\nGenerate one of the options.\nUsed for special tokens.\nForce generation of the specific string.\nThis represents a collection of grammars, with a designated\nIf set, the grammar will allow skip_rx as the first lexeme.\nIf set, the grammar will allow invalid utf8 byte sequences.\nRegular expression matching the body of generation.\nThe default value for ‘contextual’ in Lexeme nodes.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThis is no longer used. When enabled, the grammar can use …\nOnly applies to greedy_lexer grammars. This adds a new …\nHow much “fuel” are we willing to spend to build …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe JSON schema that the grammar should generate. When …\nThe Lark grammar that the grammar should generate. When …\nLazy gen()s take the shortest match. Non-lazy take the …\nMaximum size of the grammar (symbols in productions) …\nFor non-ambiguous grammars, this is the maximum “…\nMaximum number of lexer states. Default: 10_000\nThe name of this grammar, can be used in GenGrammar nodes.\nNormally, when a sequence of bytes is forced by grammar, …\nThe start symbol is at nodes[0] When nodes is empty, then …\nWhen set, the regexps can be referenced by their id …\nMaximum lexer fuel for computation of the whole token mask.\nNumber of Earley items created for the whole token mask. …\nWhen set, the string matching <code>stop_rx</code> will be output as a …\nThe whole generation must match <code>body_rx + stop_rx</code>. …\nOverride sampling temperature.\nOverride sampling temperature.\nIf false, all other lexemes are excluded when this lexeme …\nIt lists the allowed escape sequences, typically one of: …\nWhen set and json_string is also set, “…” will not …\nWhen set, the lexeme will be quoted as a JSON string. For …\nThe regular expression that will greedily match the input.\nOverride sampling temperature.\nIntersection of the regexes\nMatches this byte only. If byte is not in 0..127, it may …\nMatches this string of bytes only. Can lead to invalid …\nMatches any byte in the set, expressed as bitset. Can lead …\nConcatenation of the regexes\nThe current state is dead. Should be only true for …\nMatches the empty string. Same as Concat([]).\nReference previously built regex\nTransition via any other byte, or EOI leads to a dead …\nTransition via any byte leads to a dead state but EOI is …\nMatches this string only\nMatches the regex; should be at the end of the main regex. …\nMatches nothing. Same as Or([]).\nMatches everything the regex doesn’t match. Can lead to …\nUnion of the regexes\nAll prefixes of the words matched by the regex (including …\nCompile the regex using the regex_syntax crate\nRepeat the regex at least min times, at most max times …\nTransition via some bytes <em>may be</em> possible.\nWhich escapes to allow (after ). Represents a set of …\nEnable or disable the case insensitive flag by default.\nRegex is empty iff self ⊆ big\nEnable or disable the “dot matches any character” flag …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nEnable verbose mode in the regular expression.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if the there is only one transition out of state. …\nEstimate the size of the regex tables in bytes.\nWhen set, “…” will not be added around the final …\nEnable or disable the Unicode flag (<code>u</code>) by default.\nWhen disabled, translation will permit the construction of …\nA hashconsing data structure for vectors of u32. Given a …\nFinish insertion process for a vector. Returns the unique …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet vector with given unique id. Panics if id is out of …\nInsert a given vector and return its unique id.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturn number of elements in the hashcons (also largest …\nCreate a new hashcons.\nEstimate the size of the regex tables in bytes.\nEstimate number of bytes used by the hashcons.\nAdd a slice to the vector being inserted. Requires …\nAdd an element to the vector being inserted. Requires …\nStart insertion process for a vector. Panics if …\nThis is a top-level method in this file.  It is called by …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns how many bytes can be applied.\nLexemeIdx is an index into the lexeme table. It …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCheck if the lexeme always matches bytes, and has at least …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe current state is dead. Should be only true for …\nTransition via any other byte, or EOI leads to a dead …\nTransition via any byte leads to a dead state but EOI is …\nTransition via some bytes <em>may be</em> possible.\nPart of the interface for “subsumption”, a feature …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate and return the initial state of a DFA for this …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturn index of lowest matching regex if any. Lazy regexes …\nCheck if the there is only one transition out of state. …\nEstimate the size of the regex tables in bytes.\n“Subsumption” is a feature implementing regex …\nGiven a transition (a from-state and a byte) of the DFA …\nFunction which llg calls when an operation is done.\nRepresents result from llg_commit_token()\nTokenization function Will not write more than …\nNo value.\nNo value.\nSome value of type <code>T</code>.\nSome value of type <code>T</code>.\nDoes the engine support backtracking? (Removing tokens …\nThe constraint to compute mask for.\nDoes the engine support fast-forward tokens? (Appending …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nShould the sequence stop?\nShould the sequence stop?\nThe resource limits for the parser Default values will be …\nClone the constraint\nClone a tokenizer. This increments a reference count and …\nCommit the token sampled with the mask returned from …\nCompute mask for the next token sampling It typically …\nSet the default values for the ConstraintInit Disables …\nGet the logs from the constraint, since last call to this …\nFree the constraint\nFree the tokenizer. Should <em>NOT</em> be called while there are …\nGet the error message from the constraint or null if there …\nGet the current temperature of the constraint. It is …\nCheck if constraint is stopped (cannot be extended …\nCreate a new constraint from a grammar JSON string Always …\nCreate a new constraint with specified type Type can be …\nCreate a new constraint from a given JSON schema Always …\nCreate a new constraint from a given lark grammar Always …\nCreate a new constraint from a given regular expression …\nConstruct a new tokenizer from the given TokenizerInit\nCompute mask for several constraints in parallel.\nReturn a string representation of the tokens, useful for …\nTokenize the given bytes and return the tokens. Always …\nTokenize the given bytes and return the tokens. Special …\nThe log level for the buffer that is kept inside of the …\nThe log level for writing to stderr\nThe length of the mask_dest array in bytes (not elements).\nPointer to memory where the mask should be written.\nThe number of tokens in the tokens array (can be 0)\nOne bit per vocab token This is valid until any call to llg…\nTemperature to use for sampling\nThe token ID for the end of sentence token For chat mode, …\nA pointer to the token strings The length of this the sum …\nAn array of the lengths of the token strings (vocab_size …\nSet to true to enable hack that works around the …\nTokenization function, see LlgTokenizeFn docs. It should …\nUser data to pass to the tokenize_fn\nThe tokenizer to use, created with llg_new_tokenizer()\nInstead of passing token_lens and token_bytes, this can be …\nThe tokens to append to the output if any This is valid …\nSet to true to not use tokenize_fn and instead tokenize …\nThe number of tokens in the vocabulary\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nDefines what is allowed in Branch\nDescribes what to do after sampling.\nBacktracking is allowed.\nSampling result for the previous iteration. For simple …\nBacktrack this much before appending this sequence (this …\ncheck if stack.top() transitions via byte to a viable state\nReturn how many tokens and bytes need to chopped off …\n“Collapse” the stack so that it consists only of its …\nConditional (and unconditional) splices are allowed.\nEnd of sentence token\nUnconditional splice is allowed.\nAppend these tokens after backtracking.\nMore than one branch is allowed.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCheck if there are any errors to be reported to the user.\nCheck if add_bias() would have returned any tokens.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nself |= other &amp; !minus\nfor _ in 0..num { stack.pop() }\nIf None, no sampling is performed. If Some(set), only …\nIf None, no sampling is performed. If Some(set), only …\nThe token that was sampled (after applying the mask), …\ncheck if stack.top() transitions via tok to a viable state\nDescribes what to do after sampling. If no sampling, there …\nDescribes what to do after sampling. If no sampling, there …\nOverride temperature for sampling. It may or may not be …\nOverride temperature for sampling. It may or may not be …\nAssociated trie.\nTokenize a string coming from user. It may or may not …\nTokenize a given byte sequence. It may or may not …\nTokenize a given byte sequence. It will interpret text …\nIf this returns true, this tokenizer always returns …\nTokenize a string. It will interpret &lt;|special_tokens|&gt; as …\nCalled when iteration over the trie is finished Stack has …\nCalled when iteration over the trie is started\nThis combines <code>push_byte</code> and <code>byte_allowed</code> into one function …\nIf one of the tokens in when_sampled is sampled, this …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet error message if recognizer is in error state.\nInitial state\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if given special token is allowed in given state.\nExtend the recognizer with given byte if allowed.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConfigure a text model with the various parameters for …\nWrapper of <code>GgufModelBuilder</code> for LoRA models.\nConfigure a text GGUF model with the various parameters …\nWrapper of <code>GgufModelBuilder</code> for X-LoRA models.\nWrapper of <code>TextModelBuilder</code> for LoRA models.\nThe object used to interact with the model. This can be …\nBuilder for PagedAttention metadata.\nA way to add messages with finer control given.\nA type which can be used as a chat request.\nA chat message role.\nPlain text (chat) messages.\nConfigure a text model with the various parameters for …\nText (chat) messages with images.\nConfigure a vision model with the various parameters for …\nWrapper of <code>TextModelBuilder</code> for X-LoRA models.\nActivate certain adapters on the model, they will be used …\nThis handles adding the <code>&lt;image&gt;</code> prefix to the prompt.\nAdd a message to the request.\nThis handles adding the <code>&lt;|image_{N}|&gt;</code> prefix to the prompt.\nAdd a message with the output of a tool call.\nThis handles adding the <code>&lt;|image|&gt;</code> prefix to the prompt.\nGets the best device, cpu, cuda if compiled with CUDA, or …\nRetrieve some information about this model.\nDetokenize some tokens.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nAutomatically resize and pad images to this maximum edge …\nPath to read a UQFF file from.\nPath to read a UQFF file from.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nA few defaults are applied here:\nA few defaults are applied here:\nA few defaults are applied here:\nA few defaults are applied here:\nReapply ISQ to the model. This will be done on whatever …\nGenerate with the model.\nGenerate with the model, returning raw logits of the first …\nSet the sampling parameters for deterministic generation. …\nSet the sampling parameters as given.\nThe default tool choice is auto.\nGenerate with the model.\nTokenize some text or messages.\nUtilise this calibration file to collcet an imatrix. …\nUtilise this calibration_file file during ISQ\nLiteral Jinja chat template OR Path (ending in <code>.json</code>) to …\nLiteral Jinja chat template OR Path (ending in <code>.json</code>) to …\nLiteral Jinja chat template OR Path (ending in <code>.json</code>) to …\nProvide metadata to initialize the device mapper. …\nProvide metadata to initialize the device mapper. …\nProvide metadata to initialize the device mapper. …\nLoad the model in a certain dtype.\nLoad the model in a certain dtype.\nLoad the model in a certain dtype.\nForce usage of the CPU device. Do not use PagedAttention …\nForce usage of the CPU device. Do not use PagedAttention …\nForce usage of the CPU device. Do not use PagedAttention …\nForce usage of the CPU device. Do not use PagedAttention …\nSet the revision to use for a Hugging Face remote model.\nSet the revision to use for a Hugging Face remote model.\nSet the revision to use for a Hugging Face remote model.\nSet the revision to use for a Hugging Face remote model.\nUtilise this imatrix file during ISQ. Incompatible with …\nUse ISQ of a certain type. If there is an overlap, the …\nUse ISQ of a certain type. If there is an overlap, the …\nManually set the model loader type. Otherwise, it will …\nEnable logging.\nEnable logging.\nEnable logging.\nEnable logging.\nSet the maximum number of sequences which can be run at …\nSet the maximum number of sequences which can be run at …\nSet the maximum number of sequences which can be run at …\nSet the maximum number of sequences which can be run at …\nOrganize ISQ to enable MoQE (Mixture of Quantized Experts, …\nDisable KV cache. Trade performance for memory usage.\nDisable KV cache. Trade performance for memory usage.\nEnable PagedAttention. Configure PagedAttention with a …\nEnable PagedAttention. Configure PagedAttention with a …\nSet the number of sequences to hold in the prefix cache. …\nSet the number of sequences to hold in the prefix cache. …\nSet the prompt batchsize to use for inference.\nSet the prompt batchsize to use for inference.\nSet the prompt batchsize to use for inference.\nSource the tokenizer and chat template from this model ID …\nSource of the Hugging Face token.\nSource of the Hugging Face token.\nSource of the Hugging Face token.\nSource of the Hugging Face token.\nPath to a discrete <code>tokenizer.json</code> file.\nPath to a discrete <code>tokenizer.json</code> file.\nPath to a discrete <code>tokenizer.json</code> file.\nSet the model topology for use during loading. If there is …\nSet the model topology for use during loading. If there is …\nSet the model topology for use during loading. If there is …\nPath to write a UQFF file to.\nPath to write a UQFF file to.")