searchState.loadedDescShard("mistralrs_quant", 0, "Device/configurable intelligent matrix multiplication\nQuantized method for a quantized matmul.\nReal (for Metal) and Fake (for CUDA)\nUsed to gate access to quantizing onto the host device\nExtension trait adding <code>argsort</code> / <code>sort</code> convenience calls on …\nOffset for the quant type. UQFF always serializes the …\nAcquire the quantize drop guard to protect the critical …\nAdd a delta weight from LoRA to the weights. This should …\nIf the quant is backed by a qmatmul.\nBegin tracking stats into an ImatrixLayerStats\nClear all LoRA adapters for the current engine thread\nWeight dtype and device\nEnd tracking stats into an ImatrixLayerStats. Returns the …\nReturns the indices that would (ascending) sort the tensor …\ninclusive = false, reverse = false\nReturns the tensor’s values (ascending) sorted along <code>axis</code>…\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nGet the LoRA adapters for the current engine thread\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nStatic LoRA in the style of Phi-4 multimodal. Only when …\nCompute matrix-matrix product.\nCompute matrix-matrix product. The result will be divided …\nCompute matrix-matrix product. The result will be divided …\nFactor by which the weight size is reduced over the given …\nPush a LoRA adapter for the current engine thread\nCompute quantized matrix-matrix product.\nCompute quantized matrix-matrix product.\nQuantize the model into HQQ\nIf a quantized method, return the activation dtype.\nNOT meant for external calling\nController for the CUBLASLT handle and inhibition flag.\nFused batch matmul + add + Relu/Gelu activation using …\nFused batch matmul + add + Relu/Gelu activation using …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet the handle if not inhibited.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSet whether to inhibit CUBLASLT usage.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLoads the ring backend config from a path at <code>RING_CONFIG</code>\nThis layer has a weight that is parallelized along the …\nThis layer has no parallelization\nThis layer has a weight that is parallelized along the …\nCompute the appropriate KV shard. This handles KV head …\nCompute the number of KV groups, taking into account KV …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nNote: we only support AFQ and unquantized here because …\nThe Client holds its persistent connection inside a Mutex …\nThe Server maintains persistent connections.\nBroadcasts the given ID over all persistent connections.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nBinds the listener and then accepts exactly <code>n_nodes</code> …\nReceives the broadcasted ID from the persistent stream.\nApply Rotary position encoding inplace\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a wrapper around multiple memory mapped file and …\nCreates a wrapper around a memory mapped file and …\nInitializes a <code>VarBuilder</code> that retrieves tensors stored in …")