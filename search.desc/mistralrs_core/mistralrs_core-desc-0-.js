searchState.loadedDescShard("mistralrs_core", 0, "Raw audio input consisting of PCM samples and a sample …\nAutomatic device mapping (recommended).\nAllow automatic selection of any given tool, or none.\nAutomatically selects between a normal or vision loader …\nEmbedding model used for ranking web search results …\nChat completion streaming request chunk.\nAn OpenAI compatible chat completion response.\nTemplate for chat models including bos/eos/unk as well as …\nChat completion choice.\nChat completion streaming chunk choice.\nCompletion request choice.\nChat completion streaming chunk choice.\nCompletion request choice.\nAn OpenAI compatible completion response.\nControl the constraint with llguidance.\nCustomizable logits processor.\nThe scheduler method controld how sequences are scheduled …\nDelta in content for streaming response.\nRequest to detokenize some text.\nMetadata to initialize the device mapper.\nA loader for a vision (non-quantized) model.\nA builder for a loader for a vision (non-quantized) model.\nThe architecture to load the vision model as.\nSelect a diffusion model, without quantization or adapters\nDummy device mapping for a NCCL pipeline\nEngine instructions, per Engine (MistralRs) ID.\nSelect a GGML model.\nA loader for a GGML model.\nA builder for a GGML loader.\nConfig for a GGML loader.\nSelect a GGUF model.\nLoader for a GGUF model.\nA builder for a GGUF loader.\nConfig for a GGUF loader.\n<code>NormalLoader</code> for a Gemma model.\n<code>VisionLoader</code> for an Idefics 2 Vision model.\nImage generation response format\n<code>VisionLoader</code> for an LLaVA Vision model.\n<code>VisionLoader</code> for an LLaVANext Vision model.\nA device mapper which does device mapping per hidden layer.\nA value of type <code>L</code>.\n<code>NormalLoader</code> for a Llama model.\nThe <code>Loader</code> trait abstracts the loading process. The …\nA builder for a loader using the selected model.\nAll local paths and metadata necessary to load a model.\nLogprobs per token.\nSelect a LoRA architecture\nSelect a GGML model with LoRA.\nSelect a GGUF model with LoRA.\nManual device mapping.\nThe MistralRs struct handles sending requests to the …\nThe MistralRsBuilder takes the pipeline and a scheduler …\nCategory of the model. This can also be used to extract …\nDType for the model.\nThe kind of model to build.\n<code>ModelPaths</code> abstracts the mechanism to get all necessary …\nOnly quantize MoE experts, if applicable. The enables MoQE.\nPrepend a vision tag appropriate for the model to the …\nReal device mapping for a NCCL pipeline\nDisallow selection of tools.\nA loader for a “normal” (non-quantized) model.\nA builder for a loader for a “normal” (non-quantized) …\nThe architecture to load the normal model as.\nA normal request request to the <code>MistralRs</code>.\nConfig specific to loading a normal model.\nAdapter model ordering information.\nAll memory counts in MB. Default for block size is 32.\n<code>NormalLoader</code> for a Phi 2 model.\n<code>NormalLoader</code> for a Phi 3 model.\n<code>VisionLoader</code> for a Phi 3 Vision model.\nSelect a plain model, without quantization or adapters\n<code>NormalLoader</code> for a Qwen 2 model.\nA request to the Engine, encapsulating the various …\nMessage or messages for a <code>Request</code>.\nThe response enum contains 3 types of variants:\nA logprob with the top logprobs for this token.\nChat completion response message.\nA value of type <code>R</code>.\nSelect a model for running via auto loader\nSampling params are used to control sampling.\nCallback used to override how search results are gathered. …\nMetadata for a speculative pipeline\nA loader for a speculative pipeline using 2 <code>Loader</code>s.\nSpeculative decoding pipeline: …\n<code>NormalLoader</code> for a Starcoder2 model.\nStop sequences or ids.\nTerminate all sequences on the next scheduling step. Be …\nThe source of the HF token.\nRequest to tokenize some messages or some text.\nSelect the model from a toml file\nForce selection of a given tool.\nCallback used for custom tool functions. Receives the …\nCollection of callbacks keyed by tool name.\nTop-n logprobs element\nType which can be converted to a DType\nOpenAI compatible (superset) usage during a request.\nA loader for a vision (non-quantized) model.\nA builder for a loader for a vision (non-quantized) model.\nThe architecture to load the vision model as.\nSelect a vision plain model, without quantization or …\nConfig specific to loading a vision model.\nSelect an X-LoRA architecture\nSelect a GGML model with X-LoRA.\nSelect a GGUF model with X-LoRA.\nLogits and sequence context (prompt and generated tokens), …\nConvert the response into a result form.\nIf the loader type is not specified, loader type is …\nJinja format chat templating for chat completion.\nImage dimensions will be 720x1280.\nThis sets up the parameters so that there is:\nA device mapper to not map device.\nA device mapper to not map device.\nCreate an empty topology.\nOverride the description for the extraction tool.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nDecode audio bytes using <code>symphonia</code>.\nγ completions to run of the draft model\nGet adapter paths.\nGet the explicit chat template.\nRetrieve the <code>PretrainedConfig</code> file.\nFilepath for general model configuration.\nAmount of available memory in bytes.\nGet the preprocessor config (for the vision models). This …\nGet the processor config (for the vision models). This is …\nFile where the content is expected to deserialize to …\nA serialised <code>tokenizers.Tokenizer</code> HuggingFace object.\nAmount of total memory in bytes.\nModel weights files (multiple files supported).\nThis should be called to initialize the debug flag and …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nIf <code>revision</code> is None, then it defaults to <code>main</code>. If <code>dtype</code> is …\nLoad a model from the specified paths. Also initializes …\nIf <code>training == true</code>, <code>loss_csv_path</code> will not save anything. …\nCreates a new builder with the given pipeline, scheduler …\nCreate a loader builder for a GGUF model. <code>tok_model_id</code> is …\nFactor by which the weight size is reduced over the given …\n<code>true</code> if built with CUDA (requires Unix) /Metal\nParse ISQ value.\nPrefix for inclusion in messages (may do nothing if the …\nPrefix for inclusion in messages (may do nothing if the …\nPrefix for inclusion in messages (may do nothing if the …\nPrefix for inclusion in messages (may do nothing if the …\nRead a wav file from disk.\nOverride the description for the search tool.\nReturns the total of model execution time.\nReturns the total of model execution time.\nConvert multi channel audio to mono by averaging channels.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>true</code> if built with the <code>flash-attn</code> or <code>flash-attn-v3</code> …\nUse a custom callback to gather search results.\nRegister a custom callback for the specified tool name.\nModel ID to load LoRA from. This may be a HF hub repo or a …\nModel ID to load LoRA from. This may be a HF hub repo or a …\nModel ID to load LoRA from. This may be a HF hub repo or a …\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nGenerate and utilize an imatrix to enhance GGUF …\nGenerate and utilize an imatrix to enhance GGUF …\nGenerate and utilize an imatrix to enhance GGUF …\nDAC Model ID to load from. If not provided, this is …\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\n.toml file containing the selector configuration.\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nGQA value\nGQA value\nGQA value\nCache path for Hugging Face models downloaded locally.\nCache path for Hugging Face models downloaded locally\nCache path for Hugging Face models downloaded locally\nCache path for Hugging Face models downloaded locally\nCache path for Hugging Face models downloaded locally\n.imatrix file to enhance GGUF quantizations with.\n.imatrix file to enhance GGUF quantizations with. …\n.cimatrix file to enhance GGUF quantizations with. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nAutomatically resize and pad images to this maximum edge …\nAutomatically resize and pad images to this maximum edge …\nMaximum expected image size will have this edge length on …\nMaximum expected image size will have this edge length on …\nMaximum prompt number of images to expect for this model. …\nMaximum prompt number of images to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nModel ID to load from. May be a HF hub repo or a local …\nModel ID to load from. This may be a HF hub repo or a …\nForce a base model ID to load from instead of using the …\nForce a base model ID to load from instead of using the …\nModel ID to load from. This may be a HF hub repo or a …\nModel ID to load from. This may be a HF hub repo or a …\nModel ID to load from. This may be a HF hub repo or a …\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nISQ organization: <code>default</code> or <code>moqe</code>.\nISQ organization: <code>default</code> or <code>moqe</code> (Mixture of Quantized …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename.\nQuantized filename.\nQuantized filename.\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nIndex of completion tokens to generate scalings up until. …\nIndex of completion tokens to generate scalings up until. …\nIndex of completion tokens to generate scalings up until. …\n<code>tok_model_id</code> is the local or remote model ID where you can …\n<code>tok_model_id</code> is the local or remote model ID where you can …\n<code>tok_model_id</code> is the local or remote model ID where you can …\nModel ID to load the tokenizer from. This may be a HF hub …\nModel ID to load the tokenizer from. This may be a HF hub …\nModel ID to load the tokenizer from. This may be a HF hub …\nPath to local tokenizer.json file. If specified, it is …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nRoPE for Llama3\nDevice/configurable intelligent matrix multiplication\nRoPE supporting LongRope\nApplies 2D reflection padding to a tensor of shape (N, C, …\n(cos, sin)\n(cos, sin)\nThe difference between 1.0 and the next smallest …\nExpands a mask from (bs, seq_len) to (bs, 1, tgt_len, …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCompute matrix-matrix product.\nCompute matrix-matrix product. The result will be divided …\nCompute matrix-matrix product. The result will be divided …\nMaximum representable value.\nMinimum representable value.\nGemma uses weight + 1.0\nCompute quantized matrix-matrix product.\nCompute quantized matrix-matrix product.\nComputes softmax(QK^T*sqrt(d_k))V\nGemma uses weight + 1.0. Undo for UQFF generation.")