searchState.loadedDescShard("mistralrs_core", 0, "Configuration for adding a model to MistralRs\nRaw audio input consisting of PCM samples and a sample …\nAutomatic device mapping (recommended).\nAllow automatic selection of any given tool, or none.\nAutomatically selects between a normal or vision loader …\nEmbedding model used for ranking web search results …\nCalled function with name and arguments\nChat completion streaming request chunk.\nAn OpenAI compatible chat completion response.\nTemplate for chat models including bos/eos/unk as well as …\nChat completion choice.\nChat completion streaming chunk choice.\nCompletion request choice.\nChat completion streaming chunk choice.\nCompletion request choice.\nAn OpenAI compatible completion response.\nControl the constraint with llguidance.\nCustomizable logits processor.\nThe scheduler method controld how sequences are scheduled …\nDelta in content for streaming response.\nRequest to detokenize some text.\nMetadata to initialize the device mapper.\nA loader for a vision (non-quantized) model.\nA builder for a loader for a vision (non-quantized) model.\nThe architecture to load the vision model as.\nSelect a diffusion model, without quantization or adapters\nDummy device mapping for a NCCL pipeline\nEngine instructions, per Engine (MistralRs) ID.\nConfiguration for creating an engine instance\nFunction definition for a tool\nSelect a GGML model.\nA loader for a GGML model.\nA builder for a GGML loader.\nConfig for a GGML loader.\nSelect a GGUF model.\nLoader for a GGUF model.\nA builder for a GGUF loader.\nConfig for a GGUF loader.\n<code>NormalLoader</code> for a Gemma model.\nHTTP-based MCP server using JSON-RPC over HTTP\n<code>VisionLoader</code> for an Idefics 2 Vision model.\nImage generation response format\n<code>VisionLoader</code> for an LLaVA Vision model.\n<code>VisionLoader</code> for an LLaVANext Vision model.\nA device mapper which does device mapping per hidden layer.\nA value of type <code>L</code>.\n<code>NormalLoader</code> for a Llama model.\nThe <code>Loader</code> trait abstracts the loading process. The …\nA builder for a loader using the selected model.\nAll local paths and metadata necessary to load a model.\nLogprobs per token.\nSelect a LoRA architecture\nSelect a GGML model with LoRA.\nSelect a GGUF model with LoRA.\nManual device mapping.\nMCP client that manages connections to multiple MCP servers\nConfiguration for MCP client integration\nConfiguration for an individual MCP server\nSupported MCP server transport sources\nInformation about a tool discovered from an MCP server\nThe MistralRs struct handles sending requests to multiple …\nThe MistralRsBuilder takes the pipeline and a scheduler …\nCategory of the model. This can also be used to extract …\nDType for the model.\nThe kind of model to build.\n<code>ModelPaths</code> abstracts the mechanism to get all necessary …\nOnly quantize MoE experts, if applicable. The enables MoQE.\nSelect multi-model mode with configuration file\nPrepend a vision tag appropriate for the model to the …\nReal device mapping for a NCCL pipeline\nDisallow selection of tools.\nA loader for a “normal” (non-quantized) model.\nA builder for a loader for a “normal” (non-quantized) …\nThe architecture to load the normal model as.\nA normal request request to the <code>MistralRs</code>.\nConfig specific to loading a normal model.\nAdapter model ordering information.\nAll memory counts in MB. Default for block size is 32.\n<code>NormalLoader</code> for a Phi 2 model.\n<code>NormalLoader</code> for a Phi 3 model.\n<code>VisionLoader</code> for a Phi 3 Vision model.\nSelect a plain model, without quantization or adapters\nLocal process-based MCP server using stdin/stdout …\n<code>NormalLoader</code> for a Qwen 2 model.\nA request to the Engine, encapsulating the various …\nMessage or messages for a <code>Request</code>.\nThe response enum contains 3 types of variants:\nA logprob with the top logprobs for this token.\nChat completion response message.\nA value of type <code>R</code>.\nSelect a model for running via auto loader\nSampling params are used to control sampling.\nCallback used to override how search results are gathered. …\nMetadata for a speculative pipeline\nA loader for a speculative pipeline using 2 <code>Loader</code>s.\nSpeculative decoding pipeline: …\n<code>NormalLoader</code> for a Starcoder2 model.\nStop sequences or ids.\nTerminate all sequences on the next scheduling step. Be …\nThe source of the HF token.\nRequest to tokenize some messages or some text.\nSelect the model from a toml file\nTool definition\nForce selection of a given tool.\nCallback used for custom tool functions. Receives the …\nA tool callback with its associated Tool definition.\nCollection of callbacks keyed by tool name.\nType of tool\nTop-n logprobs element\nType which can be converted to a DType\nOpenAI compatible (superset) usage during a request.\nA loader for a vision (non-quantized) model.\nA builder for a loader for a vision (non-quantized) model.\nThe architecture to load the vision model as.\nSelect a vision plain model, without quantization or …\nConfig specific to loading a vision model.\nWebSocket-based MCP server for real-time bidirectional …\nSelect an X-LoRA architecture\nSelect a GGML model with X-LoRA.\nSelect a GGUF model with X-LoRA.\nAdd a new model engine to the MistralRs instance\nLogits and sequence context (prompt and generated tokens), …\nApply fade in/out to reduce audio artifacts\nConvert the response into a result form.\nWhether to automatically register discovered tools with …\nOptional Bearer token for authentication\nIf the loader type is not specified, loader type is …\nJinja format chat templating for chat completion.\nGet config for a specific model\nImage dimensions will be 720x1280.\nOptional human-readable description of what the tool does\nThis sets up the parameters so that there is:\nA device mapper to not map device.\nA device mapper to not map device.\nCreate an empty topology.\nWhether this server should be activated\nOverride the description for the extraction tool.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nDecode audio bytes using <code>symphonia</code>.\nγ completions to run of the draft model\nGet adapter paths.\nGet the explicit chat template.\nRetrieve the <code>PretrainedConfig</code> file.\nGet the current default model ID\nGet or create a termination flag for the current engine …\nFilepath for general model configuration.\nAmount of available memory in bytes.\nGet model category for a specific model. If model_id is …\nGet the preprocessor config (for the vision models). This …\nGet the processor config (for the vision models). This is …\nGet sender for a specific model. If model_id is None, uses …\nFile where the content is expected to deserialize to …\nA serialised <code>tokenizers.Tokenizer</code> HuggingFace object.\nGet tool callbacks that can be used with the existing tool …\nGet tool callbacks with their associated Tool definitions\nGet discovered tools information\nGet the number of tools available for a specific model …\nAmount of total memory in bytes.\nModel weights files (multiple files supported).\nCheck if MCP client is configured for a specific model\nUnique identifier for this server\nInitialize connections to all configured servers\nThis should be called to initialize the debug flag and …\nJSON schema describing the tool’s input parameters\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nList all available model IDs\nIf <code>revision</code> is None, then it defaults to <code>main</code>. If <code>dtype</code> is …\nLoad a model from the specified paths. Also initializes …\nIf <code>training == true</code>, <code>loss_csv_path</code> will not save anything. …\nMaximum number of concurrent tool calls across all MCP …\nHuman-readable name for this server\nName of the tool as reported by the MCP server\nCreate a new MCP client with the given configuration\nCreates a new builder with the given pipeline, scheduler …\nCreate a loader builder for a GGUF model. <code>tok_model_id</code> is …\nNormalize audio to prevent clipping\nFactor by which the weight size is reduced over the given …\n<code>true</code> if built with CUDA (requires Unix) /Metal\nParse ISQ value.\nPrefix for inclusion in messages (may do nothing if the …\nPrefix for inclusion in messages (may do nothing if the …\nPrefix for inclusion in messages (may do nothing if the …\nPrefix for inclusion in messages (may do nothing if the …\nRead a wav file from disk.\nRemove DC offset (audio centered around 0)\nRemove a model engine from the MistralRs instance\nReset termination flags for the current engine.\nOptional resource URI patterns this server provides\nOverride the description for the search tool.\nDispatch a request to the appropriate engine based on the …\nID of the server this tool comes from\nDisplay name of the server for logging and debugging\nList of MCP servers to connect to\nSet the default model ID\nCheck if the current engine should terminate sequences.\nTransport-specific connection configuration\nReturns the total of model execution time.\nReturns the total of model execution time.\nConvert multi channel audio to mono by averaging channels.\nOptional prefix to add to all tool names from this server\nTimeout for individual tool execution in seconds\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>ToString::to_string</code>, but without panic on OOM.\n<code>true</code> if built with the <code>flash-attn</code> or <code>flash-attn-v3</code> …\nConfigure MCP client to connect to external MCP servers.\nUse a custom callback to gather search results.\nRegister a custom callback for the specified tool name.\nRegister a custom callback with its associated Tool …\nArguments to pass to the command\nCommand to execute (e.g., “mcp-server-filesystem”)\nOptional environment variables for the process\nOptional headers to include in requests (e.g., API keys, …\nOptional headers for the WebSocket handshake\nOptional timeout in seconds for HTTP requests Defaults to …\nOptional timeout in seconds for connection establishment …\nBase URL of the MCP server (http:// or https://)\nWebSocket URL (ws:// or wss://)\nOptional working directory for the process\nModel ID to load LoRA from. This may be a HF hub repo or a …\nModel ID to load LoRA from. This may be a HF hub repo or a …\nModel ID to load LoRA from. This may be a HF hub repo or a …\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nThe architecture of the model.\nGenerate and utilize an imatrix to enhance GGUF …\nGenerate and utilize an imatrix to enhance GGUF …\nGenerate and utilize an imatrix to enhance GGUF …\nMulti-model configuration file path (JSON format)\nDAC Model ID to load from. If not provided, this is …\nDefault model ID to use when no model is specified in …\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\nModel data type. Defaults to <code>auto</code>.\n.toml file containing the selector configuration.\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nUQFF path to load from. If provided, this takes precedence …\nGQA value\nGQA value\nGQA value\nCache path for Hugging Face models downloaded locally.\nCache path for Hugging Face models downloaded locally\nCache path for Hugging Face models downloaded locally\nCache path for Hugging Face models downloaded locally\nCache path for Hugging Face models downloaded locally\n.imatrix file to enhance GGUF quantizations with.\n.imatrix file to enhance GGUF quantizations with. …\n.cimatrix file to enhance GGUF quantizations with. This …\nPath to local Matryoshka Transformer configuration CSV file\nPath to local Matryoshka Transformer configuration CSV file\nPath to local Matryoshka Transformer configuration CSV file\nName of the Matryoshka Transformer slice to use\nName of the Matryoshka Transformer slice to use\nName of the Matryoshka Transformer slice to use\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nMaximum prompt batch size to expect for this model. This …\nAutomatically resize and pad images to this maximum edge …\nAutomatically resize and pad images to this maximum edge …\nMaximum expected image size will have this edge length on …\nMaximum expected image size will have this edge length on …\nMaximum prompt number of images to expect for this model. …\nMaximum prompt number of images to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nMaximum prompt sequence length to expect for this model. …\nModel ID to load from. May be a HF hub repo or a local …\nModel ID to load from. This may be a HF hub repo or a …\nForce a base model ID to load from instead of using the …\nForce a base model ID to load from instead of using the …\nModel ID to load from. This may be a HF hub repo or a …\nModel ID to load from. This may be a HF hub repo or a …\nModel ID to load from. This may be a HF hub repo or a …\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nOrdering JSON file\nISQ organization: <code>default</code> or <code>moqe</code>.\nISQ organization: <code>default</code> or <code>moqe</code> (Mixture of Quantized …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename(s). May be a single filename, or use a …\nQuantized filename.\nQuantized filename.\nQuantized filename.\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nQuantized model ID to find the <code>quantized_filename</code>. This …\nIndex of completion tokens to generate scalings up until. …\nIndex of completion tokens to generate scalings up until. …\nIndex of completion tokens to generate scalings up until. …\n<code>tok_model_id</code> is the local or remote model ID where you can …\n<code>tok_model_id</code> is the local or remote model ID where you can …\n<code>tok_model_id</code> is the local or remote model ID where you can …\nModel ID to load the tokenizer from. This may be a HF hub …\nModel ID to load the tokenizer from. This may be a HF hub …\nModel ID to load the tokenizer from. This may be a HF hub …\nPath to local tokenizer.json file. If specified, it is …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to local tokenizer.json file. If this is specified it …\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nPath to a topology YAML file.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nUQFF path to write to.\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nModel ID to load X-LoRA from. This may be a HF hub repo or …\nRoPE for Llama3\nDevice/configurable intelligent matrix multiplication\nRoPE supporting LongRope\nApplies 2D reflection padding to a tensor of shape (N, C, …\nRoPE for SmolLm3\n(cos, sin)\n(cos, sin)\nThe difference between 1.0 and the next smallest …\nExpands a mask from (bs, seq_len) to (bs, 1, tgt_len, …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCompute matrix-matrix product.\nCompute matrix-matrix product. The result will be divided …\nCompute matrix-matrix product. The result will be divided …\nMaximum representable value.\nMinimum representable value.\nGemma uses weight + 1.0\nGemma 3n uses weight\nCompute quantized matrix-matrix product.\nCompute quantized matrix-matrix product.\nComputes softmax(QK^T*sqrt(d_k))V\nSame as <code>run_attention</code>, but no flash attention\nGemma uses weight + 1.0. Undo for UQFF generation.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.")