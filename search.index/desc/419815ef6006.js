rd_("Ajsee [TensorIndex#method.i]0000ChReturns a tensor that is a transposed version of the \xe2\x80\xa6Ah4-bit float (MX4 format)AjContains the success valueAdUnion of the regexesAfElement-wise equality.ClElement-wise comparison with greater-equal, the returned \xe2\x80\xa6CkElement-wise comparison with greater-than, the returned \xe2\x80\xa6BfThe unique identifier for this tensor.BaUnique identifier for this server0AlUnique ID for this tool call101AmTask ID (same as response ID)BdUnique identifier for this tool callBcUnique identifier for this responseAmGet the ID of the output itemAoUnique identifier for this item0AiID of the referenced itemgItem IDCjElement-wise comparison with lower-equal, the returned \xe2\x80\xa6CiElement-wise comparison with lower-than, the returned \xe2\x80\xa6CaPerforms strict matrix-vector multiplication (\xe2\x80\xa6AjElement-wise non-equality.BmChannel receiver for incoming model responses00AmThe type of tool being calledCkHigh-quality lossy compression, commonly used in mobile \xe2\x80\xa6AkIntersection of the regexesAhContains the error valueBcMinimal reasoning, faster responses0AdLow reasoning effortBbLow detail (faster, less accurate)AfManual device mapping.0CfWidely compatible, lossy compression, good for web \xe2\x80\xa6CmMatches everything the regex doesn\xe2\x80\x99t match. Can lead to \xe2\x80\xa6ChRaw audio data, requires additional format specificationBjSelect a model for running via auto loader0CjUncompressed, largest file sizes but maximum compatibilityfPanicsCgConcatenates two or more tensors along a particular \xe2\x80\xa6CeElement-wise comparison between two tensors, e.g. \xe2\x80\xa6lUse CPU onlyBgUse CPU only (disable GPU acceleration)CcThe dimension size for a specified dimension index.BkComputes the dot product of two 1D tensors.CnApplies the Exponential Linear Unit (ELU) function on each \xe2\x80\xa6BnOptional environment variables for the process000CdThe difference between 1.0 and the next smallest \xe2\x80\xa60ChReturns a matrix with a diagonal of ones of size n by n.DjReturns the sub-tensor fixing the index at <code>i</code> on the first \xe2\x80\xa6CkGet vector with given unique id. Panics if id is out of \xe2\x80\xa6B`Get the handle if not inhibited.iGQA value00000BfCheck if a specific option is includedAiIn-situ quantization type0CjIn-situ quantization to apply. Defaults to Q6K on CPU, \xe2\x80\xa6CkReturn number of elements in the hashcons (also largest \xe2\x80\xa6BkLog all responses and requests to this fileDcSimilar to <code>max_keepdim</code> but the target dimension is \xe2\x80\xa6AlMaximum representable value.0DcSimilar to <code>min_keepdim</code> but the target dimension is \xe2\x80\xa6AlMinimum representable value.0CdCreate a new MCP client with the given configurationCiCreate a new agent with the given model and configurationBoCreate a new agent builder with the given modelB`A few defaults are applied here:0CeCreate a builder for a speculative decoding pipeline.111ClCreate an empty builder. You must add at least one input \xe2\x80\xa62222CjCreates a new tensor on the specified device using the \xe2\x80\xa6ClCreates a new builder with the given pipeline, scheduler \xe2\x80\xa6DjCreate a loader builder for a GGUF model. <code>tok_model_id</code> is \xe2\x80\xa6AoCreate a new MultiModelBuilder.ChCreate a new GPT-OSS rotary embedding with YARN scaling.CdConstruct a state machine for a sequence constraint.AfCreate a new hashcons.BdCreate a new Harmony parsing contextAlCreate a new ThinkTagContext>76140>CbCreates a new HTTP transport for MCP communicationCmCreates a new process transport by spawning an MCP server \xe2\x80\xa6CjCreates a new WebSocket transport connection to an MCP \xe2\x80\xa6CjNote: we only support AFQ and unquantized here because \xe2\x80\xa6DdBinds the listener and then accepts exactly <code>n_nodes</code> \xe2\x80\xa6CeCreates a wrapper around a memory mapped file and \xe2\x80\xa6AlCreate a new background taskBdCreate a new background task managerAlCreate a new in-memory cacheDeCreates a new <code>MistralRsForServerBuilder</code> with default \xe2\x80\xa6DhCreates a new <code>MistralRsServerRouterBuilder</code> with default \xe2\x80\xa6BcCreate a new OpenResponses streamerCcCreate a new IncludeConfig from the request optionsAjCreate a new ResponseUsageAjCreate a new ResponseErrorAmCreate new incomplete detailsCaCreate a new ResponseResource with default valuesAlCreate a new streaming stateBnReturns an instant corresponding to \xe2\x80\x9cnow\xe2\x80\x9d.CkPad an image of shape (c, h, w) to (c, max_h, max_w) by \xe2\x80\xa6AhPointwise pow operation.C`Run the agentic loop with the given user messageCnReturns the amount of time elapsed from another instant to \xe2\x80\xa6ClReturns the sum of all elements in the input tensor. The \xe2\x80\xa6C`Base URL of the MCP server (http:// or https://)0AoWebSocket URL (ws:// or wss://)01010AdThe URL of the imagegThe URLCjReturns the unbiased variance over the selected dimension.BgAutomatic device mapping (recommended).CeAllow automatic selection of any given tool, or none.10AiAuto-select summary levelAdLet the model decideBfAutomatically truncate input if neededAhAutomatic tier selectionAfAutomatic detail levelClMatches this byte only. If byte is not in 0..127, it may \xe2\x80\xa6CfThe current state is dead. Should be only true for \xe2\x80\xa60BaThe stream has completed entirelylFile contentCjLossless compression, larger file sizes but good audio \xe2\x80\xa6BfFlex tier (lower priority, lower cost)AdSelect a GGML model.0AdSelect a GGUF model.0BfDeep reasoning, more thorough analysis0AeHigh reasoning effortBcHigh detail (slower, more accurate)BnHTTP-based MCP server using JSON-RPC over HTTP000AcLark parser grammarAoA value of type <code>L</code>.0AjSelect a LoRA architecture0BgReal device mapping for a NCCL pipeline0AlDisallow selection of tools.iNo value.01AeDon\xe2\x80\x99t use any toolslNo reasoningCdGood compression efficiency, ideal for real-time \xe2\x80\xa6BbSome value of type <code>T</code>.0CmSynchronous callback (runs in spawn_blocking for parallel \xe2\x80\xa6AgFree-form text responseAcSimple string inputAaPlain text outputAlText content (OpenAI format)lText contentAeSimple string content4BaSelect the model from a toml file0B`Force selection of a given tool.oTool definition100BhTool message (response from a tool call)lUser messageAnThe architecture of the model.0000000000000B`Arguments to pass to the command000jError codeCmCompared to clone, this copies the actual storage but may \xe2\x80\xa6AiBase64 encoded audio dataAkAudio data (base64 encoded)0C`The dimension size for this tensor on each axis.AkMark the response as failedCa.toml file containing the selector configuration.0ClReturns a new tensor with the order of elements reversed \xe2\x80\xa6B`More than one branch is allowed.AoReturns the argument unchanged.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000CnReturns a new tensor with all the elements having the same \xe2\x80\xa6AlFallback for non-CUDA buildsCjCustom GEMV (General Matrix-Vector multiplication) for \xe2\x80\xa6BiIP address to serve on (default: 0.0.0.0)BaCalls <code>U::from(self)</code>.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000oThe output itemAhThe complete output itemCjLazy gen()s take the shortest match. Non-lazy take the \xe2\x80\xa6DeLoads the ring backend config from a path at <code>RING_CONFIG</code>CmReturns the mean of all elements in the input tensor. The \xe2\x80\xa6BcHuman-readable name for this server0BnName of the tool as reported by the MCP server0CjThe name of this grammar, can be used in GenGrammar nodes.CgThe function name (extracted from recipient like \xe2\x80\x9c\xe2\x80\xa632032B`The name of the function to callAcName for the schemaAdName of the function0BdOptional name for the message senderAlName of the function to call0BaFor function calls: function nameBhGet the next event from the agent streamDmComputes the <strong>Frobenius norm</strong> (L2 norm of all elements) of \xe2\x80\xa6BfCreates a new tensor filled with ones.A`The content partAiThe complete content partBbCheck if the connection is healthyBlCheck if the transport connection is healthyCcTests the HTTP connection by sending a ping requestCfTests the process connection by sending a ping requestCfSends a WebSocket ping frame to test connection healthAaPort to serve on.BaPort to listen on (default: 8080)CgRaise the tensor to some float exponent <code>e</code>.ChCreates a new tensor initialized with values sampled \xe2\x80\xa6ClThe number of dimensions for this tensor, 0 for a scalar \xe2\x80\xa6CnThe role of the message sender (\xe2\x80\x9cuser\xe2\x80\x9d, \xe2\x80\x9cassistant\xe2\x80\x9d\xe2\x80\xa6AnThe role of the message senderBlThe role (always \xe2\x80\x9cassistant\xe2\x80\x9d for output)CmRoll the tensor input along the given dimension. Elements \xe2\x80\xa6CeInteger seed to ensure reproducible random number \xe2\x80\xa60BjReturns the total of model execution time.000BcText configuration from the requestAiText output configuration1B`Create a new text output contentA`The text content0AjThe text that is annotated0011ClThe <code>#[tool]</code> attribute macro for defining tools.0CnWhen disabled, translation will permit the construction of \xe2\x80\xa6AbGeneral utilities.CdAn agent that runs an agentic loop with tool callingBkAsynchronous callback (runs natively async)mAudio contentCcThe different types of elements allowed in tensors.BhDelta in content for streaming response.0AfError during executionkError event0AlUser-facing response content0mImage contentBkArray of input items (OpenResponses format)0CkThe object used to interact with the model. This can be \xe2\x80\xa6B`Multiple possible stop sequencesAfArray of content partsCfSelect a plain model, without quantization or adapters0ChCompile the regex using the regex_syntax crate. This \xe2\x80\xa6AjRegular expression grammarAoA value of type <code>R</code>.0CdOpenAI compatible (superset) usage during a request.0AmSelect an X-LoRA architecture0BgOptional alias used as the API model ID0CnLogits and sequence context (prompt and generated tokens), \xe2\x80\xa6DkRun the <code>forward</code> method of <code>m</code> on <code>self</code>.1oBuild the agentCgIf the loader type is not specified, loader type is \xe2\x80\xa6BeBuild the multi-model Model instance.1BjBuilds the configured mistral.rs instance.BbBuilds the configured axum router.ClSplit a tensor into the specified number of chunks, this \xe2\x80\xa6DlClamp the tensor values to be between <code>min</code> and <code>max</code>.AkClose the server connectionAnClose the transport connectionBdCloses the HTTP transport connectionCdTerminates the child process and cleans up resourcesBjGracefully closes the WebSocket connectionnThe text deltaAcThe arguments deltaCfThe dtype for the elements stored in the input tensor.oModel data typeBoModel data type. Defaults to <code>auto</code>.0000000000000100000000000000BbA device mapper to not map device.000CfCreates a new tensor filled with uninitialized memory.AiCreate an empty topology.0BeEnum types for the OpenResponses API.BgError information (if status is failed)Bh\xce\xb3 completions to run of the draft model0BfGet the underlying MistralRs instance.BfThe text content to convert to speech.ClThe input for the response - can be a string or array of \xe2\x80\xa6CfInput and output item types for the OpenResponses API.BeMin-p sampling (mistral.rs extension)BgGet a reference to the underlying modeleModelnModel selector0jModel name1BjThe TTS model to use for audio generation.BaThe model to use for this requestB`The model used for this response3ClCreates a wrapper around multiple memory mapped file and \xe2\x80\xa6BdNumber of generations tokens to run.AbOrdering JSON file000000000CaParse a channel name string into a HarmonyChannel0BaParse a string into a MessageRoleAcQuote from the fileCmCreates a new tensor initialized with values sampled from \xe2\x80\xa6CdThe tensor shape, i.e. dimension sizes on each axis.ChStacks two or more tensors along a particular dimension.mCurrent stateAnUnderlying mistral.rs instance00B`All steps taken during executionAmWhether to store the responseCaWhether to store the response for later retrieval1AaTitle of the pageBgGet a reference to the registered toolsBaTool definitions from the requestC`Tool definitions available for the model to call1BeTop-k sampling (mistral.rs extension)BiDefault top_k for generation. Default: 40AfTop-p from the requestBhTop-p (nucleus) sampling parameter (0-1)1CdDefault top_p for generation (0.0-1.0). Default: 0.9CiReturns a lower triangular matrix of ones of size n by n.CjReturns an upper triangular matrix of ones of size n by n.BlCustom types used in mistral.rs server core.AgToken usage informationBgCreates a new tensor filled with zeros.AgChat completion choice.0CmThe Client holds its persistent connection inside a Mutex \xe2\x80\xa6AlConcatenation of the regexesAcCpu, Cuda, or MetalCm6-bit float with 2 exponent bits and 3 mantissa bits (MX6 \xe2\x80\xa6Cm6-bit float with 3 exponent bits and 2 mantissa bits (MX6 \xe2\x80\xa6Cd8-bit float with 8 exponent bits and 0 mantissa bitskTask failedAmResponse failed with an errorDdThe <code>Loader</code> trait abstracts the loading process. The \xe2\x80\xa60CeDevice/configurable intelligent matrix multiplication00AhBalanced reasoning depth0BaMedium reasoning effort (default)AjOpenAI Chat format contentBgRaw text prompt that will be tokenized.nTask is queuedBaResponse is queued for processingCjRepeat the regex at least min times, at most max times \xe2\x80\xa6BlThe Server maintains persistent connections.mSimple choiceAdSingle stop sequenceEhExtension trait adding <code>argsort</code> / <code>sort</code> convenience calls on \xe2\x80\xa6BkSpeech models for text-to-speech generationBdDescribes what to do after sampling.nSystem messageCbExplicitly typed input item (has \xe2\x80\x9ctype\xe2\x80\x9d field)BiThe core struct for manipulating tensors.AdPre-tokenized input.DgThis operation multiplies the input tensor by <code>mul</code> then \xe2\x80\xa6CiCreates a new 1D tensor with values from the interval \xe2\x80\xa6DfSimilar to <code>argmax_keepdim</code> but the target dimension is \xe2\x80\xa6DfSimilar to <code>argmin_keepdim</code> but the target dimension is \xe2\x80\xa6BaString representation for dtypes.CbConvert to string representation for chat template0BoReturns the string representation of the status0BmReturns the string representation of the roleBoReturns the string representation of the reasonAnMark the response as cancelledDiAll chunks received during streaming (if <code>store_chunks</code> is \xe2\x80\xa600AmGet the client configuration.BjGet a reference to the agent configurationBkRetrieve some information about this model.AoGet config for a specific modelCaMulti-model configuration file path (JSON format)4104BoApplies a 1D convolution over the input tensor.BoApplies a 2D convolution over the input tensor.CnReturns the cumulative sum of elements of the input tensor \xe2\x80\xa6CiReturns a new tensor detached from the current graph, \xe2\x80\xa6BdOptional detail level for processingAeOptional detail level0C`The device on which the input tensor is located.AkDevice to load the model on0ClThis is the primary interface for llguidance \xe2\x80\x93 the one \xe2\x80\xa6BnEffort level for reasoning (low, medium, high)C`Streaming event types for the OpenResponses API.AjAn alias for broadcast_as.AfFormat for text outputBiAudio format (e.g., \xe2\x80\x9cwav\xe2\x80\x9d, \xe2\x80\x9cmp3\xe2\x80\x9d)00BjGather values across the target dimension.BoInsert a given vector and return its unique id.CnThe layout of the input tensor, this stores both the shape \xe2\x80\xa6CmThe resource limits for the parser Default values will be \xe2\x80\xa6CnReturns the matrix-multiplication of the input tensor with \xe2\x80\xa6AnCompute matrix-matrix product.00CjReturns a new tensor that is a narrowed version of the \xe2\x80\xa6BcObject type (always \xe2\x80\x9cresponse\xe2\x80\x9d)B`OpenAI compatible functionality.BcOutput items generated by the modelAhOutput from the functionAiReason for incompletenessCbRepeat this tensor along the specified dimensions.BlThe result: Ok(output) or Err(error_message)AfJSON Schema definitionBkGet a specific server connection by its ID.00BjWhether to suppress logging during loading0CmTokenizer partitions for the slicer optimization. This is \xe2\x80\xa6BkTransport-specific connection configuration000CnModel status: \xe2\x80\x9cloaded\xe2\x80\x9d, \xe2\x80\x9cunloaded\xe2\x80\x9d, or \xe2\x80\x9creloading\xe2\x80\xa6AnCurrent status of the responseBaGet the status of the output itemAbStatus of the item0CgWhether to stream the response using server-sent eventsBgWhether to use strict schema validationAnWhether strict mode is enabledCkThe tokens to append to the output if any This is valid \xe2\x80\xa6DiReturns a view of which contains all slices of size <code>size</code> \xe2\x80\xa6CnMatches any byte in the set, expressed as bitset. Can lead \xe2\x80\xa6BoA unique ID of a symbol in the compiled grammarAjGenerate a concise summarylDefault tierB`Reference previously built regexBoGrammar specification for structured generationCiA measurement of a monotonically nondecreasing clock. \xe2\x80\xa6AhMatches this string onlyCjThis is meant to be used in server-side scenarios. The \xe2\x80\xa6BaA message item with explicit typeCiA bare message (just role + content, no type field) - \xe2\x80\xa6nA message itemnMessage outputBmRepresents a single message in a conversationB`Matches nothing. Same as Or([]).CeLocal process-based MCP server using stdin/stdout \xe2\x80\xa6000AfRefusal output contentCfA request to the Engine, encapsulating the various \xe2\x80\xa60C`Multiply the pixe values by the provided factor.CjThe stream is actively processing and sending response \xe2\x80\xa6CkAcquire the quantize drop guard to protect the critical \xe2\x80\xa6DkRun the <code>forward</code> method of <code>m</code> on <code>self</code>.CcRegular expression matching the body of generation.BnCreate a new builder for an embedding request.CjUnique identifier for the call (used to match with output)BhUnique identifier for this function callBmID of the function call this is a response toAkFor function calls: call_idAlCall ID of the function call0CfCommand to execute (e.g., \xe2\x80\x9cmcp-server-filesystem\xe2\x80\x9d)000CcGet the accumulated content (outside think blocks).0AcThe message contentAjThe content of the message0AoContent parts for message itemsBhContent types for the OpenResponses API.BbImage dimensions will be 720x1280.0CaCreates a new builder with default configuration.0CfReturns the amount of time elapsed since this instant.BgWhether this server should be activated000gFile ID0BgFile ID (for previously uploaded files)0CkFlattens the input tensor on the dimension indexes from \xe2\x80\xa6F`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa6CiGrammar for constrained generation (mistral.rs extension)CaOpenAI Harmony format parsing for GPT-OSS models.0ClOptional headers to include in requests (e.g., API keys, \xe2\x80\xa60BlOptional headers for the WebSocket handshake01010Ca.imatrix file to enhance GGUF quantizations with.Ce.imatrix file to enhance GGUF quantizations with. \xe2\x80\xa6Ck.cimatrix file to enhance GGUF quantizations with. This \xe2\x80\xa6210CgSpecifies additional content to include in the responsemInitial stateAiShould the sequence stop?0BdCheck if this is simple text contentCkComputes the max of all the elements in this tensor and \xe2\x80\xa6mError messageB`Create a new message output item1CkComputes the min of all the elements in this tensor and \xe2\x80\xa6CeOptional callback to execute when streaming completes00BhThe raw include options from the requestCnReturns a tensor with the same data as the input where the \xe2\x80\xa6BhCompute quantized matrix-matrix product.00BcCreate a new refusal output contentAcThe refusal messageCkReshape returns a tensor with the target shape provided \xe2\x80\xa6CiThe token that was sampled (after applying the mask), \xe2\x80\xa6ChGet a reference to all connected MCP server connections.BaList of MCP servers to connect to01001DjInitializes a <code>VarBuilder</code> that retrieves tensors stored in \xe2\x80\xa6CnDescribes what to do after sampling. If no sampling, there \xe2\x80\xa60CmCreates a new tensor with the specified dimension removed \xe2\x80\xa6DcThe whole generation must match <code>body_rx + stop_rx</code>. \xe2\x80\xa6CkComputes the sum of all the elements in this tensor and \xe2\x80\xa6BjWhether to generate a summary of reasoningCjConvert multi channel audio to mono by averaging channels.00B`Extract text from MessageContentCkConvert to a text string, extracting text from parts if \xe2\x80\xa6BdAn alias for <code>to_scalar</code>.ClReturns the data contained in a 1D tensor as a vector of \xe2\x80\xa6ClReturns the data contained in a 2D tensor as a vector of \xe2\x80\xa6BjReturns the data contained in a 3D tensor.CmThe token ID for the end of sentence token For chat mode, \xe2\x80\xa6CoEnable or disable the Unicode flag (<code>u</code>) by default.ChChain-of-thought reasoning (internal, not for end users)0BbAgent finished with final responseAkGenerate a detailed summaryCcDisable truncation (may error if input is too long)AdFile path annotationAnFunction definition for a tool00mFunction toolBaImage URL content (OpenAI format)BdImage URL structure for image inputsAcLogprobs per token.0AnSelect a GGML model with LoRA.0AnSelect a GGUF model with LoRA.0BbNo tool calls and no text responseBcAdapter model ordering information.0AoPriority tier (higher priority)nForce tool useBoThe response enum contains 3 types of variants:0AaSpecific functionCmConvert an image to a tensor. This converts the data from \xe2\x80\xa6BlRepresents a tool call made by the assistantlType of tool00AlAccumulated analysis content0BcModel category (Text, Vision, etc.)0Cl\xe2\x80\x9cCollapse\xe2\x80\x9d the stack so that it consists only of its \xe2\x80\xa6AnMark the response as completedAoDRY base (mistral.rs extension)hFilename0BbFinalize parsing at end of stream.0CiCreates a new tensor initialized with values from the \xe2\x80\xa6AiThe function call detailsoFunction to useAcFunction definitionAoGet the current state of a taskBmGet the text content if this is a text outputAlGet a specific tool by name.00BkCheck if a tool with the given name exists.00BcWhether to return log probabilitiesClAutomatically resize and pad images to this maximum edge \xe2\x80\xa6000CaMaximum running sequences at any time. If the \xe2\x80\xa6AmPort to serve MCP protocol onChCreates grids of coordinates specified by the 1D inputs.CaUser-provided metadata (up to 16 key-value pairs)AfUser-provided metadataBjGet the default model ID for this builder.CjModel ID to load from. May be a HF hub repo or a local \xe2\x80\xa6CiModel ID to load from. This may be a HF hub repo or a \xe2\x80\xa6CkForce a base model ID to load from instead of using the \xe2\x80\xa60111121001111CgConfiguration key for this model (human-friendly label)AoNumber of prompt tokens to run.CcThe number of tokens in the tokens array (can be 0)CfOptional callback to process each chunk before sending00Ajself |= other &amp; !minusCiPing all connected servers and return results per server.00CiAdd an element to the vector being inserted. Requires \xe2\x80\xa6AkQuantize the model into HQQCjWhen set, \xe2\x80\x9c\xe2\x80\xa6\xe2\x80\x9d will not be added around the final \xe2\x80\xa6ClReads a npy file and return the stored multi-dimensional \xe2\x80\xa6CmReads a npz file and returns the stored multi-dimensional \xe2\x80\xa6AjRead a wav file from disk.00C`ResponseResource type for the OpenResponses API.BdThe model\xe2\x80\x99s response for this stepAeThe response resource0AkThe final response resourceBlThe response resource with error informationBmThe response resource with incomplete details3CiRound element of the input tensor to the nearest integer.BiIP to serve on. Defaults to \xe2\x80\x9c0.0.0.0\xe2\x80\x9dBkGracefully shutdown all server connections.00ChCasts the input tensor to the target <code>dtype</code>.A`Associated trie.AoTokenize some text or messages.CiTokenize a string coming from user. It may or may not \xe2\x80\xa6AmPath to a topology YAML file.00000000000000000000000CkReturns true if the computation graph should track this \xe2\x80\xa6BiSets whether to force CPU-only execution.CjUse ISQ of a certain type. If there is an overlap, the \xe2\x80\xa600AoSets the logging configuration.BjOptional working directory for the process000BoRepresents a single step in the agent executionAaAssistant messageAbTask was cancelledAfResponse was cancelledAkTask completed successfullyAoResponse completed successfullyAkItem completed successfullyChDeveloper message (similar to system but lower priority)CeRepresents the current state of a streaming response.BhDummy device mapping for a NCCL pipeline0CkSelect an embedding model, without quantization or adapters0ClTransition via any byte leads to a dead state but EOI is \xe2\x80\xa60AbFile input contentBkInput item types for the OpenResponses API.AbText input contentCkQuote the regex as a JSON string. For example, [A-Z\\n]+ \xe2\x80\xa6CnMatches the regex; should be at the end of the main regex. \xe2\x80\xa6CkMCP client that manages connections to multiple MCP servers000CmThe MistralRs struct handles sending requests to multiple \xe2\x80\xa60AkThe kind of model to build.0BcOptional fields allowed on any NodeCkNormalize the image data based on the mean and standard \xe2\x80\xa6BaText content delta from the modelCjWebSocket-based MCP server for real-time bidirectional \xe2\x80\xa6000B`Select a GGML model with X-LoRA.0B`Select a GGUF model with X-LoRA.0AlAdd a new model dynamically.C`Add a new model engine to the MistralRs instanceCnAdd a model. The model ID will be the pipeline name (e.g., \xe2\x80\xa61CnAdd a model with just an ID and ModelSelected (convenience \xe2\x80\xa6BdIf the quant is backed by a qmatmul.AnThe JSON arguments as a string0BdThe function arguments (JSON string)BgJSON-encoded arguments for the function0BiFor function calls: accumulated argumentsAfThe complete argumentsBhConvert the response into a result form.0AhBacktracking is allowed.CjSampling result for the previous iteration. For simple \xe2\x80\xa6ClBacktrack this much before appending this sequence (this \xe2\x80\xa6CfDirectly call a tool by name with the given arguments.0AjCall a tool on this server1DiReturns a tensor with the values from the <code>self</code> tensor at \xe2\x80\xa6AeEnd index in the text00AeEnd of sentence tokenB`Unconditional splice is allowed.BgAppend these tokens after backtracking.AhBase64 encoded file data0AaFile path detailsBiCreates a new 1D tensor from an iterator.BiCreate a new MessageContent from a stringD`Path to read a <code>.uqff</code> file from. Other necessary \xe2\x80\xa600CnUQFF path to load from. If provided, this takes precedence \xe2\x80\xa600000000000C`Fused GLU activation: output = activation(a) * bCiCheck if there are any errors to be reported to the user.CbGet error message if recognizer is in error state.BkGet information about all discovered tools.00mThe image URL0AdThe image URL objectEgAccumulate element from <code>source</code> at indexes <code>indexes</code> and add \xe2\x80\xa6CnTreat stop_rx as suffix, i.e., do not hide it from the LLM \xe2\x80\xa6BdItem type (message or function_call)CcPointer to memory where the mask should be written.DcThis crate provides an asynchronous API to <code>mistral.rs</code>.BaNumber of completions to generateAgGemma uses weight + 1.00CkCheck if the there is only one transition out of state. \xe2\x80\xa60BcNormalize audio to prevent clipping00BoEstimate the size of the regex tables in bytes.0BnEstimate number of bytes used by the hashcons.1CjCreates a new tensor filled with ones with same shape, \xe2\x80\xa6C`Polls the stream for the next Server-Sent Event.0Aofor _ in 0..num { stack.pop() }BmConfiguration for reasoning/thinking behaviorCaReasoning/chain-of-thought content from the modelBiReconnect to a specific server by its ID.00CcOptional resource URI patterns this server provides000BoResponses API functionality and route handlers.BeID of the server this tool comes from000AaGet the server IDB`The default tool choice is auto.EdSet the values on <code>self</code> using values from <code>src</code>. The copy \xe2\x80\xa6B`Stop sequences to end generationAhSSE streaming utilities.CbCreate a text content part for multimodal messagesCjIf the target device is the same as the tensor device, \xe2\x80\xa6ClRetrieves the single scalar value hold in the tensor. If \xe2\x80\xa6CfThe tokenizer to use, created with llg_new_tokenizer()B`Name of the tool that was callediTool typeChReturns a tensor that is a transposed version of the \xe2\x80\xa6CnCreates a new tensor with a dimension of size one inserted \xe2\x80\xa6BfCreate a new ResponseError with a codeCfSets the random seed for deterministic model behavior.oSet text configAlAdd a tool with its callbackCcWrites a multi-dimensional array in the npy format.CjWrites multiple multi-dimensional arrays using the npz \xe2\x80\xa6BeEvents yielded during agent streamingBbAnnotation for output text contentCjRaw audio input consisting of PCM samples and a sample \xe2\x80\xa600BgAudio input structure for OpenAI formatCiAutomatically selects the appropriate loader based on \xe2\x80\xa60BdTool call preambles and explanations0BgControl the constraint with llguidance.0CiTransition via any other byte, or EOI leads to a dead \xe2\x80\xa60AjA loader for a GGML model.0AhLoader for a GGUF model.0AcTask is in progressBeResponse is currently being processedAcItem is in progressCbResponse was incomplete (e.g., max tokens reached)AcItem was incompleteAcAudio input contentAmAudio content (OpenAI format)AcImage input contentBcStatus of an individual output itemAbJSON object outputAcJSON schema grammarBkStructured response following a JSON schemaB`JSON output with optional schemaAbLLGuidance grammarAaMCP content typesAdDType for the model.0Dh<code>ModelPaths</code> abstracts the mechanism to get all necessary \xe2\x80\xa60BoSelect multi-model mode with configuration file0CiMultipleOf(d, s) matches if the input, interpreted as \xe2\x80\xa6BbParser has not emitted stop() yet.BkOutput item types for the OpenResponses APIAcText output contentBl<code>NormalLoader</code> for a Phi 2 model.0Bl<code>NormalLoader</code> for a Phi 3 model.0DeTransition via some bytes <em>may be</em> possible. The bytes are \xe2\x80\xa60AfStop sequences or ids.0BgStop token configuration for generationAiText output configurationAiText format configurationBaA single tool completed executionAjResult of a tool executionAfTop-n logprobs element0DkTransforms to apply, starting with the <code>input</code> and then with \xe2\x80\xa6AiAdd a single text prompt.CdDynamically add and connect a new server at runtime.00BbAdd a single pre-tokenized prompt.BkApply fade in/out to reduce audio artifacts00Ci2D average pooling over an input tensor with multiple \xe2\x80\xa6BbWhether request runs in backgroundC`Whether to run the request in background (async)1CnPagedAttention KV cache type (auto or f8e4m3). Defaults to \xe2\x80\xa60CnCheck whether the current parser state forces the sequence \xe2\x80\xa6AnAccumulated commentary content0BcThe constraint to compute mask for.ClReturns a tensor that is in row major order. This is the \xe2\x80\xa6AaCreated timestampBlUnix timestamp when the response was createdAbCreation timestampAgDetokenize some tokens.BgDisconnect a specific server by its ID.00BhCurrent state of the streaming operation00BmThe number of elements stored in this tensor.BfOpenAI-compatible embeddings endpoint.ClCreates a new tensor filled with uninitialized memory of \xe2\x80\xa6jError type0AiGet the event type stringDjFlattens the input tensor on the dimension indexes from <code>0</code> \xe2\x80\xa6AlLogs to be sent to the user.C`Decode audio bytes using <code>symphonia</code>.00CaCreate a new MessageContent from multimodal partsCiCreates a new tensor initialized with values from the \xe2\x80\xa6DhReturns the sub-tensor fixing the index at <code>index</code> on the \xe2\x80\xa6CnGet sender for a specific model. If model_id is None, uses \xe2\x80\xa60AiBase64 encoded image data0C`Initialize connections to all configured servers00CnThis wraps the EmbeddingModelBuilder, so users should take \xe2\x80\xa6CkThis wraps the VisionModelBuilder, so users should take \xe2\x80\xa60BkCheck if the custom GEMV kernel is enabled.CiSpecial nodes can\xe2\x80\x99t be removed in grammar optimizationsBdTotal number of iterations performedAaList all task IDsBeList available tools from this serverBaLogit bias for token manipulationCk2D max pooling over an input tensor with multiple channels.BmDefault max tokens to generate. Default: 2048BbMCP client configuration file pathClNormally, when a sequence of bytes is forced by grammar, \xe2\x80\xa6CmEnable PagedAttention on Metal. Because PagedAttention is \xe2\x80\xa60BcJSON Schema for function parametersCfAdd a slice to the vector being inserted. Requires \xe2\x80\xa6CgReceives the broadcasted ID from the persistent stream.BcRun the agent with streaming outputBgGet a list of all connected server IDs.00C`Directory for storing generated speech wav filesDlParser for <code>&lt;think&gt;...&lt;/think&gt;</code> tags in model output.0ClAn array of the lengths of the token strings (vocab_size \xe2\x80\xa6BeTool calls made in this step (if any)CdOptional list of tool calls (for assistant messages)BiGet the total number of registered tools.00CkGiven a transition (a from-state and a byte) of the DFA \xe2\x80\xa6CeTruncation strategy when input exceeds context windowAhTruncation strategy usedCaExtend the recognizer with given byte if allowed.CbGemma uses weight + 1.0. Undo for UQFF generation.0BfThe number of tokens in the vocabularyCmReturns a tensor with the same shape as the input tensor, \xe2\x80\xa6BbLoad the model in a certain dtype.0000AiSet the error informationAjSets the model to be used.nSet store flagmSet the toolsiSet top_pAiSet the usage informationDfPath to write a <code>.uqff</code> file to and serialize the other \xe2\x80\xa600AfUQFF path to write to.00000000000CcUpgrades an HTTP request to a WebSocket connection.CkCreates a new tensor filled with zeros with same shape, \xe2\x80\xa6BbConfiguration for the agentic loopBgStream of agent events during executionCjMatches this string of bytes only. Can lead to invalid \xe2\x80\xa6BgChat completion streaming chunk choice.0BkDevice/configurable intelligent convolutionBmMatches the empty string. Same as Concat([]).Bl<code>NormalLoader</code> for a Gemma model.0CeRegex string used to extract image URLs from prompts.BdImage detail level for vision inputsAhResponse was interruptedCd<code>VisionLoader</code> for an LLaVA Vision model.0Bl<code>NormalLoader</code> for a Llama model.0CcFunction which llg calls when an operation is done.CjDistinguish at runtime which kind of model we have loaded.CfInformation about a tool discovered from an MCP server000BeRole of a message in the conversationCgConfiguration for a single model in a multi-model setupCbModel information metadata about an available modeBfModel status for loaded/unloaded state0CkTop-level parser indicates that no more bytes can be added.CcJSON-RPC error codes based on MCPEx.Protocol.ErrorsCc<code>VisionLoader</code> for a Phi 3 Vision model.0BhQuantized method for a quantized matmul.Bm<code>NormalLoader</code> for a Qwen 2 model.0BkA type which can be used as a chat request.CnCompile the regex using the regex_syntax crate, but do not \xe2\x80\xa6CkThe stream has finished processing and is about to send \xe2\x80\xa6BgService tier for request prioritizationAkThe source of the HF token.0AgURL citation annotationAdURL citation detailsClA hashconsing data structure for vectors of u32. Given a \xe2\x80\xa6ChSelect a vision plain model, without quantization or \xe2\x80\xa60AkGet all accumulated content0ClAdd a delta weight from LoRA to the weights. This should \xe2\x80\xa6AmAdd a message to the request.BbAdd multiple text prompts at once.AdOptional annotationsCiCreates a new 1D tensor with values from the interval \xe2\x80\xa6CmGets the best device, cpu, cuda if compiled with CUDA, or \xe2\x80\xa6G`Returns <code>Some(t)</code> where <code>t</code> is the time <code>self + duration</code> if <code>t</code> \xe2\x80\xa6G`Returns <code>Some(t)</code> where <code>t</code> is the time <code>self - duration</code> if <code>t</code> \xe2\x80\xa6ChReturn how many tokens and bytes need to chopped off \xe2\x80\xa6BlCompletions functionality and route handler.BoOpenAI-compatible completions endpoint handler.CbNumber of concurrent requests to run. Default is 1BnCreate a new background task and return its IDmDelete a taskCiOptional human-readable description of what the tool does000AkDescription of the functionCiExpands a mask from (bs, seq_len) to (bs, 1, tgt_len, \xe2\x80\xa60Bbinclusive = false, reverse = falseBkNew final response content since last delta0ChFlattens the input tensor by reshaping it into a one \xe2\x80\xa6DaGet embedding model <code>modules.json</code> compatible with \xe2\x80\xa60BcCheck if this delta has any content0AdOptional HF revision0B`Mark the response as in progressBcApplies a unary custom op in place.CjApplies a unary custom op in place (for the first tensor).CgApplies a ternary custom op in place (for the first \xe2\x80\xa6AfThe audio input objectBiConvert to Either for internal processingCdConvert to a normalized form for internal processingCmWhether this tensor is a variable or not. A variable is a \xe2\x80\xa6CjThe JSON schema that the grammar should generate. When \xe2\x80\xa6CeList all available model IDs (aliases if configured).AlList all available model IDs0BcReturns log(sum(exp(tensor), dim)).AkUpdate task to failed stateClGathers the maximum value across the selected dimension. \xe2\x80\xa6ClMaximum prompt sequence length to expect for this model. \xe2\x80\xa6000000000000000000000CjMaximum sequence length for context. If not specified, \xe2\x80\xa6ClGathers the minimum value across the selected dimension. \xe2\x80\xa6A`Use no KV cache.AjOpenAPI doc functionality.BnConcatenated text from all text output contentCmFactor by which the weight size is reduced over the given \xe2\x80\xa600BbSignal end of stream to the parser0BdNumber of times to repeat each test.kResponse IDCnOne bit per vocab token This is valid until any call to llg\xe2\x80\xa6CiIf None, no sampling is performed. If Some(set), only \xe2\x80\xa60CdDisplay name of the server for logging and debugging000AcGet the server nameBiEnable or disable the custom GEMV kernel.BfSet whether to inhibit CUBLASLT usage.AgStart index in the text00AeWhy the agent stoppedClReturns the sum of all elements in the input tensor. The \xe2\x80\xa6AoTemperature to use for samplingCkOverride temperature for sampling. It may or may not be \xe2\x80\xa60AnOverride sampling temperature.0AlTemperature from the requestAnTemperature for sampling (0-2)1CjDefault temperature for generation (0.0-2.0). Default: 0.7ClRepeated flag for text-only models (HuggingFace model ID \xe2\x80\xa6kText tokens0CmA pointer to the token strings The length of this the sum \xe2\x80\xa6ClTokenization function, see LlgTokenizeFn docs. It should \xe2\x80\xa6BdTool choice strategy (default: Auto)BjTool choice configuration from the requestBaControls how the model uses tools1CiOptional prefix to add to all tool names from this server000CkAccepts multipart text file upload and returns the file \xe2\x80\xa6CjReturns the unbiased variance over the selected dimension.CjSet the main device to load this model onto. Automatic \xe2\x80\xa6000CbSets the Candle device to use for model execution.AdSet the output itemsBoEnable searching compatible with the OpenAI \xe2\x80\xa600nSet the statusBmBuilder for creating agents with a fluent APIAmA streaming response handler.CmTemplate for chat models including bos/eos/unk as well as \xe2\x80\xa60BmConfiguration for creating an engine instance0AhFile citation annotationAeFile citation detailsAeFile path informationBeA function call made by the assistantAdA function call itemAdFunction call outputBfIncremental delta from Harmony parsing0BnInput content types for the OpenResponses API.BeTransport layer for MCP communicationAnCollection of available modelsCbA loader for a \xe2\x80\x9cnormal\xe2\x80\x9d (non-quantized) model.0AjFailed to reload the model0BiTool definition for the OpenResponses APIAkPlain text (chat) messages.CaModel produced a text response with no tool callsCiCallback used for custom tool functions. Receives the \xe2\x80\xa6000BfType which can be converted to a DType0BlA loader for a vision (non-quantized) model.0lAudio tokens0CjDoes the engine support backtracking? (Removing tokens \xe2\x80\xa6CiFused batch matmul + add + Relu/Gelu activation using \xe2\x80\xa6BhOptional Bearer token for authentication000ChBroadcast the input tensor to the target shape. This \xe2\x80\xa6ChBroadcasts the given ID over all persistent connections.Ckcheck if stack.top() transitions via byte to a viable stateClcommit_token() is a top-level method in this file and is \xe2\x80\xa6CfUnix timestamp when the response was completed (if \xe2\x80\xa6CmThis is a top-level method in this file.  It is called by \xe2\x80\xa6CjCompute which tokens can be consumed in the current state.ClThis computes token sampling mask. It typically takes up \xe2\x80\xa6AoRegex is empty iff self \xe2\x8a\x86 bigC`Whether the response.created event has been sentCgDAC Model ID to load from. If not provided, this is \xe2\x80\xa60CkDoes the engine support fast-forward tokens? (Appending \xe2\x80\xa6CkFlattens the input tensor on the dimension indexes from \xe2\x80\xa6CnThis can be called before the first compute_mask() to walk \xe2\x80\xa6CmCreates a fresh tensor structure based on a storage and a \xe2\x80\xa6BdGet the response resource for a taskB`Retrieve a response object by IDAkGet response by ID endpointCnGet current settings (default generation params and search \xe2\x80\xa6CbHandle route / generation errors and logging them.0CiHelper function to handle image generation errors and \xe2\x80\xa6CjHelper function to handle speech generation errors and \xe2\x80\xa6B`Core functionality for handlers.lImage tokensClSelect values for the input tensor at the target indexes \xe2\x80\xa6CdJSON schema describing the tool\xe2\x80\x99s input parameters000AfNumber of input tokensCiAdditional instructions that guide the model\xe2\x80\x99s behaviorBaSystem instructions (if provided)CnCan the grammar be finished in the current state? In other \xe2\x80\xa6CbCheck if a specific server is currently connected.00CkThe Lark grammar that the grammar should generate. When \xe2\x80\xa6CmReturns the mean of all elements in the input tensor. The \xe2\x80\xa6CcCheck if a model is known (loaded, unloaded, or \xe2\x80\xa60AdGemma 3n uses weight0ClISQ organization: <code>default</code> or <code>moqe</code>.EeISQ organization: <code>default</code> or <code>moqe</code> (Mixture of Quantized \xe2\x80\xa610AhIndex of the output item000000B`Current output items being builtCkPrefix for inclusion in messages (may do nothing if the \xe2\x80\xa600000BoProcess incremental text from the model output.0CkReapply ISQ to the model. This will be done on whatever \xe2\x80\xa6BcReload a previously unloaded model.CmManually reload a previously unloaded model. This is also \xe2\x80\xa60AeRemove a model by ID.CaRemove a model engine from the MistralRs instance0CmDispatch a request to the appropriate engine based on the \xe2\x80\xa60BnSend a JSON-RPC request and receive a responseCgSends an MCP request over HTTP and returns the responseCmSends an MCP request to the child process and returns the \xe2\x80\xa6CiSends an MCP request over WebSocket and waits for the \xe2\x80\xa6CaSends a request to the model processing pipeline.BdGet the number of connected servers.00BeSet the sampling parameters as given.EhReturns a copy of <code>self</code> where the values within <code>ranges</code> have \xe2\x80\xa6CeReturn all the nodes that lead to this value in a \xe2\x80\xa6CcStart insertion process for a vector. Panics if \xe2\x80\xa6CcWhether to store chunks for the completion callback00CmOptional timeout in seconds for HTTP requests Defaults to \xe2\x80\xa60ClOptional timeout in seconds for connection establishment \xe2\x80\xa601010Dk<code>tok_model_id</code> is the local or remote model ID where you can \xe2\x80\xa600CmModel ID to load the tokenizer from. This may be a HF hub \xe2\x80\xa600111000AfSource of the HF token0CiSource of the token for authentication. Can be in the \xe2\x80\xa6BkThe tool call ID this result corresponds toChTool call ID this message is responding to (for tool \xe2\x80\xa6AlResults from tool executionsAmTop logprobs from the requestBiNumber of top log probabilities to return1AfTotal number of tokensBnCalled when iteration over the trie is startedCcUnload a model from memory (can be reloaded later).CcUnload a model from memory while preserving its \xe2\x80\xa60DkAccepts multipart audio upload, stores under <code>cache/uploads/</code>\xe2\x80\xa6CcAccepts multipart image upload, stores it under \xe2\x80\xa6AdURL citation detailsCiIf one of the tokens in when_sampled is sampled, this \xe2\x80\xa6CkUtilise this imatrix file during ISQ. Incompatible with \xe2\x80\xa6oEnable logging.00000AmFinal response from the agentAdContent was filteredCkTop-level parser allowed EOS (as it was in an accepting \xe2\x80\xa6AhHTTP-based MCP transportCaConfiguration for what to include in the responseBeInclude options for response content.CcSomething went wrong with creating a nested parser.AlReference to a previous itemCbTokenization function Will not write more than \xe2\x80\xa6C`A builder for a loader using the selected model.0AjMaximum iterations reachedAiMCP initialization resultAfMCP server informationAdMCP tool call resultBkOpenAI-compatible tool schema for MCP toolsCkCategory of the model. This can also be used to extract \xe2\x80\xa60CiThe requested model was not found (neither loaded nor \xe2\x80\xa60CgA normal request request to the <code>MistralRs</code>.0AlOpenResponses format contentBnOutput content types for the OpenResponses APIAkTrait for caching responsesB`Error information for a responseB`Usage information for a responseAlStream options configurationBkCollection of callbacks keyed by tool name.00CfContext for managing vision messages and image buffer.AjApplies a unary custom op.AkApplies a binary custom op.AlApplies a ternary custom op.BiBroadcasting version of <code>pow</code>.mCached tokensCaJinja format chat templating for chat completion.AfChat template override10DdChat template file with a JINJA file with <code>messages</code>, \xe2\x80\xa6AlModel-specific chat template0CjPart of the interface for \xe2\x80\x9csubsumption\xe2\x80\x9d, a feature \xe2\x80\xa6ClExtend the current state of the parser with given token. \xe2\x80\xa6C`Index of the content part within the output item00BmThis sets up the parameters so that there is:0BoEnable searching compatible with the OpenAI \xe2\x80\xa6CaEnable web search tool (requires embedding model)AdEngine configuration0DkReturns the tensor\xe2\x80\x99s values (ascending) sorted along <code>axis</code>\xe2\x80\xa6AeFile citation detailsAmGet accumulated final contentAiAccumulated final content10CmFinish insertion process for a vector. Returns the unique \xe2\x80\xa6ClAutomatically resize and pad images to this maximum edge \xe2\x80\xa6BfCreate a new function call output itemCkDetermine the base cache directory for the application. \xe2\x80\xa6AdPer-connection task.BjCheck if there\xe2\x80\x99s a tool call in progress0CfCache path for Hugging Face models downloaded locally.CeCache path for Hugging Face models downloaded locally0000100000AnIn-situ quantization to apply.0BcModel-specific in-situ quantization0BbInclude usage statistics in streamCiCreate and return the initial state of a DFA for this \xe2\x80\xa6DfInterpolate the input tensor to the <code>target_size</code> size, \xe2\x80\xa6DiInterpolate the input tensor to the <code>(target_h, target_w)</code> \xe2\x80\xa6CgConverts the chat completion responder into an HTTP \xe2\x80\xa6ChConverts the completion responder into an HTTP response.ChConverts the image generation responder into an HTTP \xe2\x80\xa6CiConverts the speech generation responder into an HTTP \xe2\x80\xa6CmReturns true if the data is stored in a C contiguous (aka \xe2\x80\xa6CnGet the error message from the constraint or null if there \xe2\x80\xa6CkOptional loader config for enabling model unload/reload \xe2\x80\xa6BdConfiguration to recreate the loader10EhIf <code>training == true</code>, <code>loss_csv_path</code> will not save anything. \xe2\x80\xa60CjThe length of the mask_dest array in bytes (not elements).CbModel Context Protocol (MCP) Client ImplementationCmDisable PagedAttention on CUDA. Because PagedAttention is \xe2\x80\xa60AgNumber of output tokensDjPad the input tensor using same values along dimension <code>dim</code>\xe2\x80\xa6BoParses and validates a chat completion request.BjParses and validates a completion request.C`Parses and validates a image generation request.CaParses and validates a speech generation request.C`Process incremental bytes from the model output.0C`Process a token and return any new delta content0B`Read a resource from this serverBmRe-discover tools from all connected servers.00CjMake sure given regex can be used inside /\xe2\x80\xa6/ in Lark \xe2\x80\xa6CiRegister a tool using a tuple of (Tool, ToolCallbackType)BoDisconnect and remove a server from the client.00BaComputes softmax(QK^T*sqrt(d_k))V0DiThe size used by each element in bytes, i.e. 1 for <code>U8</code>, 4 \xe2\x80\xa6EfEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor \xe2\x80\xa6CjSorts the tensor along the last dimension, returns the \xe2\x80\xa6ClRepeated flag for speech models (HuggingFace model ID or \xe2\x80\xa6ClReturns an iterator over position of the elements in the \xe2\x80\xa6BdOptional system prompt for the agentBcDefault system prompt for all chatsCmCalled when iteration over the trie is finished Stack has \xe2\x80\xa6EhThis combines <code>push_byte</code> and <code>byte_allowed</code> into one function \xe2\x80\xa6Ck<code>ToString::to_string</code>, but without panic on OOM.00000000000000000000000000000000000000000ClRepeated flag for vision models (HuggingFace model ID or \xe2\x80\xa6C`Sets the maximum number of concurrent sequences.A`Set the metadataClOverride the model ID used by MistralRs. Defaults to the \xe2\x80\xa60CnSet the model topology for use during loading. If there is \xe2\x80\xa6000BmConfiguration for adding a model to MistralRs0BjA background task for processing responsesBgCalled function with name and arguments00CjSelect a diffusion model, without quantization or adapters0C`Represents a function call made by the assistantCeController for enabling/disabling custom GEMV kernel.AoChannel types in Harmony format0CgContext for tracking Harmony parsing state within a \xe2\x80\xa60Ch<code>VisionLoader</code> for an Idefics 2 Vision model.0CiA transform over an image. The input may vary but the \xe2\x80\xa6Ckmax_tokens limit on the total number of tokens has been \xe2\x80\xa6CmMessage content that can be either simple text or complex \xe2\x80\xa6BeThe model is currently being reloaded0CkOnly quantize MoE experts, if applicable. The enables MoQE.0BoModel does not have loader config for reloading0CkA callback function that is executed when the streaming \xe2\x80\xa6AfOutput item done eventBgEmitted when an output item is completeAcType of output itemAnGlobal response cache instanceBoA way to add messages with finer control given.CkRequest context carrying parameters to echo back in the \xe2\x80\xa6BoMessage or messages for a <code>Request</code>.0AeResponse failed eventAoEmitted when the response failsB`Response format for model outputBmStatus of a response in the OpenResponses APIAhResponse streaming chunkAlResponse delta for streamingnResponse errorCnInput that can be a string, array of input items, or array \xe2\x80\xa6AjResponse usage informationBmSampling params are used to control sampling.0CnCallback used to override how search results are gathered. \xe2\x80\xa600CcAll streaming event types for the OpenResponses APIBeState tracker for streaming responsesCgConvert an image to a tensor without normalizing to \xe2\x80\xa6BlModel is calling tools (with the tool calls)BoText (chat) messages with images and/or audios.BoNew analysis/reasoning content since last delta0CmAppends a partial assistant response (or any role) to the \xe2\x80\xa6CkReturns a new tensor duplicating data from the original \xe2\x80\xa6CdBuild the pipeline and configuration for this model.CnAdvance the parser by one token. Also checks if the parser \xe2\x80\xa6BlCreate incomplete details for content filterBfDefault generation parameters from CLIBeDRY multiplier (mistral.rs extension)CnReturns the amount of time elapsed from another instant to \xe2\x80\xa6AlFinal text response (if any)CnThis returns parser outputs to be passed back to the user. \xe2\x80\xa6F`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa600BjGenerate an image using the default model.CnGet the handle only if the given device matches the device \xe2\x80\xa6AlGet all completed tool calls0CfCheck if MCP client is configured for a specific model0ChCreate an image URL content part for multimodal messagesAlExplicit Jinja template path0CmExplicit JINJA chat template file (.jinja) to be used. If \xe2\x80\xa6AmModel-specific JINJA template0CmIndex of lowest matching regex if any. Lazy regexes match \xe2\x80\xa6BiList available resources from this serverCmGet the logs from the constraint, since last call to this \xe2\x80\xa6CfCheck if constraint is stopped (cannot be extended \xe2\x80\xa6AfMark task as cancelledAnUpdate task to completed stateClMaximum prompt batch size to expect for this model. This \xe2\x80\xa6000000000000000000000CjMaximum number of iterations before stopping (default: 10)CmMaximum prompt number of images to expect for this model. \xe2\x80\xa6000CeMax tool calls from the request (even if unsupported)BeMaximum number of tool calls allowed.1CkThe model selection configuration (Plain, GGUF, Vision, \xe2\x80\xa60CnNormalize a \xe2\x80\x98relative\xe2\x80\x99 axis value: positive values are \xe2\x80\xa6DhPad the input tensor using 0s along dimension <code>dim</code>. This \xe2\x80\xa6ClTotal context length to allocate the KV cache for (total \xe2\x80\xa60ChNumber of prefix caches to hold on the device. Other \xe2\x80\xa6CjYou can call this first with the prompt from the user, \xe2\x80\xa6BhCompute quantized matrix-matrix product.00AnRequest cancellation of a taskAmWhether web search is enabledAdSets PagedAttention.EfEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor \xe2\x80\xa6ClNumber of Earley items created for the whole token mask. \xe2\x80\xa6BiStore a response object with the given IDCaStream options for controlling streaming behaviorDiSimilar to <code>strided_index</code> but returns the position of the \xe2\x80\xa6CeTokenize a given byte sequence. It may or may not \xe2\x80\xa6CjPath to local tokenizer.json file. If specified, it is \xe2\x80\xa6CnPath to local tokenizer.json file. If this is specified it \xe2\x80\xa60000000CnInstead of passing token_lens and token_bytes, this can be \xe2\x80\xa6211111111CkIf true, include parser state (including tokens so far) \xe2\x80\xa6BgSets a base path prefix for all routes.ClForce usage of the CPU device. Do not use PagedAttention \xe2\x80\xa600000BcSets the shared mistral.rs instanceAiSet the reasoning contentBaAdd a sync tool with its callbackCnModel ID to load X-LoRA from. This may be a HF hub repo or \xe2\x80\xa600000BfReason why the agent stopped executingCmEnum representing all possible model builders that can be \xe2\x80\xa6BkApplication of transforms to the Self type.AgContent part done eventBgEmitted when a content part is completeBlA loader for a vision (non-quantized) model.000BmGlobal controller for the custom GEMV kernel.BfA tool call parsed from Harmony format0ClDo not include special tokens, and keep invalid UTF-8 as \xe2\x80\xa6Ch<code>VisionLoader</code> for an LLaVANext Vision model.0AhThe lexer is too complexBiRepresents result from llg_commit_token()CgAll local paths and metadata necessary to load a model.0AfMaximum tokens reachedCmmax_tokens limit on the number of tokens in the top-level \xe2\x80\xa6BhConfiguration for MCP client integration000BjConfiguration for an individual MCP server000BfSupported MCP server transport sources000ChTop-level parser indicates that no more bytes can be \xe2\x80\xa6CiA callback function that processes streaming response \xe2\x80\xa6AgOutput item added eventBgEmitted when a new output item is addedBfState of an output item being streamedA`Text delta eventBlEmitted when text is added to a content partClReasoning configuration for models that support extended \xe2\x80\xa6ClReasoning effort level for models that support it (e.g., \xe2\x80\xa60CkReasoning effort level for models that support extended \xe2\x80\xa6CmApplies 2D reflection padding to a tensor of shape (N, C, \xe2\x80\xa60BaThis layer has no parallelizationAfResponse created eventBbEmitted when a response is createdBoA logprob with the top logprobs for this token.0BaChat completion response message.0oResponse objectAdResponse output itemChTagged input item types (with explicit \xe2\x80\x9ctype\xe2\x80\x9d field)AdA chat message role.EfContext for tracking <code>&lt;think&gt;...&lt;/think&gt;</code> tag parsing state \xe2\x80\xa60CkThis represents a collection of grammars, with a designatedChWhich escapes to allow (after ). Represents a set of \xe2\x80\xa6CiFused batch matmul + add + Relu/Gelu activation using \xe2\x80\xa6AhCancel response endpointCaChat Completions functionality and route handler.CdOpenAI-compatible chat completions endpoint handler.j(cos, sin)00000BlCreate response endpoint - OpenResponses APICmCreates a SSE streamer for chat completions with optional \xe2\x80\xa60B`Get the currently active channelAhCurrently active channel10AnDelete a response object by IDAnDelete response by ID endpointChEnable thinking for interactive mode and models that \xe2\x80\xa6CmEnd tracking stats into an ImatrixLayerStats. Returns the \xe2\x80\xa6CbThe data is of different format, depending on tag:BoGenerate audio given a (model specific) prompt.CiEndpoint to generate speech (.wav) for a given prompt \xe2\x80\xa6CdGet the final content delta since last call (for \xe2\x80\xa60CmThis is used to generate the OpenAPI docs. The mistral.rs \xe2\x80\xa6CjGet the number of tools available for a specific model \xe2\x80\xa60BoNormalize to the internal format for processingBeCheck if a model is currently loaded.CgCheck if a model is currently loaded (as opposed to \xe2\x80\xa60CmCreate a new matcher from the given ConstraintInit Always \xe2\x80\xa6ChLoad MCP configuration from file path or environment \xe2\x80\xa6CeGenerate pixel mask of shape (c, max_h, max_w). 1 \xe2\x80\xa6ClMatches and processes different types of model responses \xe2\x80\xa6000BjNumber of tools available from MCP serversBlAudio utilities for <code>mistral.rs</code>.CmResize the images to the maximum edge length - preserving \xe2\x80\xa6ClParses and loads an audio file from a URL, file path, or \xe2\x80\xa6ClParses and loads an image from a URL, file path, or data \xe2\x80\xa6A`Parse ISQ value.0CbThe desired audio format for the generated speech.ChResponse format (legacy, prefer <code>text</code> field)AoOpenResponses API types module.BeGet the sequence number of this eventBcSequence number for ordering events0000000000000AlFallback for non-CUDA buildsCkMaximum lexer fuel for computation of the whole token mask.CmSimilar to <code>substring_chunks: s.split(&#39;&#39;)</code>ChSimilar to <code>substring_chunks: s.split(/\\s+/)</code>CiWhether tools are available through MCP or tool callbacksEeCheck if this chat template uses <code>&lt;think&gt;...&lt;/think&gt;</code> tags \xe2\x80\xa60BgReturns how many tokens can be applied.BcAdd an async tool with its callbackAcSet background flagCkConfigure MCP client to connect to external MCP servers \xe2\x80\xa6ChConfigure MCP client to connect to external MCP servers.0BbSets the MCP client configuration.CjEnable PagedAttention. Configure PagedAttention with a \xe2\x80\xa600AjCompletion request choice.0AhContent part added eventCfEmitted when a content part is added to an output itemCaA validated embedding request constructed via \xe2\x80\xa6AmDefault generation parametersCkConfigure a text GGUF model with the various parameters \xe2\x80\xa6AnReason for incomplete responseCiWrapper of <code>TextModelBuilder</code> for LoRA models.B`Message item parameter for inputCkThe MistralRsBuilder takes the pipeline and a scheduler \xe2\x80\xa60BmThe architecture to load the normal model as.0AiThe parser is too complexCbProcess-based MCP transport using stdin/stdout \xe2\x80\xa6AoReasoning summary configurationClThe main response resource returned by the OpenResponses \xe2\x80\xa6AeResponse content itemCjThis layer has a weight that is parallelized along the \xe2\x80\xa6AjSimple tool choice optionsCa<code>NormalLoader</code> for a Starcoder2 model.0DbTransforms, with each of <code>inner_transforms</code> applied \xe2\x80\xa6CjConfigure a text model with the various parameters for \xe2\x80\xa6BoUnified tool callback that can be sync or asyncBmThe architecture to load the vision model as.0BeAccumulated text for each output itemCnModel ID to load LoRA from. This may be a HF hub repo or a \xe2\x80\xa60AjAdd a model configuration.BcAdd multiple pre-tokenized prompts.BmAdd a message with the output of a tool call.BoApply generation parameters to a RequestBuilderCbApplies a unary custom op without backward supportCcApplies a binary custom op without backward supportCdApplies a ternary custom op without backward supportCaBackground task management for the Responses API.C`Matrix-multiplication with broadcasting support.CeResponse caching functionality for the Responses API.CcGenerate and utilize an imatrix to enhance GGUF \xe2\x80\xa600000AaCancellation flagCgEnable or disable the case insensitive flag by default.BgNew commentary content since last delta0CjReturn any bytes that are forced by the current parser \xe2\x80\xa6CjCompute the appropriate KV shard. This handles KV head \xe2\x80\xa6BoApplies a 1D convolution over the input tensor.CjApplies a 1D transposed convolution over the input tensor.CjApplies a 2D transposed convolution over the input tensor.CiDefault model ID to use when no model is specified in \xe2\x80\xa60AgWeight dtype and deviceCnReturns the indices that would (ascending) sort the tensor \xe2\x80\xa6ClReturns a tensor that is in row major order. This always \xe2\x80\xa6F`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa6BoGet the status of a model, or None if not found0B`Amount of total memory in bytes.0BiCheck if the lexeme always matches bytes.CaImage generation functionality and route handler.BbImage generation endpoint handler.CdWhether the response.in_progress event has been sentChEnter interactive mode instead of serving a chat server.ChCommit the token sampled with the mask returned from \xe2\x80\xa6CiCompute mask for the next token sampling It typically \xe2\x80\xa6AaFree the matcher.CkThe log level for the buffer that is kept inside of the \xe2\x80\xa6BcThe log level for writing to stderrB`Update task to in_progress stateChMaximum size of the grammar (symbols in productions) \xe2\x80\xa6CmMaximum expected image size will have this edge length on \xe2\x80\xa6000CfFor non-ambiguous grammars, this is the maximum \xe2\x80\x9c\xe2\x80\xa6CbMaximum number of lexer states. Affects memory \xe2\x80\xa6AlModel metadata configuration0CgProc macros for ergonomic tool definition in mistral.rsCgThis crate provides vision utilities for mistral.rs \xe2\x80\xa6BaPresence penalty from the requestAnPresence penalty (-2.0 to 2.0)1CmReads a npz file and returns the stored multi-dimensional \xe2\x80\xa6C`Reasoning effort level for Harmony-format models000CkReasoning effort level for Harmony-format models (GPT-OSS).CkReasoning effort level for models that support extended \xe2\x80\xa6A`Reasoning tokensBjRemove DC offset (audio centered around 0)00AgScheduler configuration0ClThe lexeme should accept any (possibly empty) contiguous \xe2\x80\xa6Ce\xe2\x80\x9cSubsumption\xe2\x80\x9d is a feature implementing regex \xe2\x80\xa6DdTokenize a string. It will interpret &lt;|special_tokens|&gt; as \xe2\x80\xa6Em<code>true</code> if built with the <code>flash-attn</code> or <code>flash-attn-v3</code> \xe2\x80\xa60ChSet the revision to use for a Hugging Face remote model.00000CjManually set the model loader type. Otherwise, it will \xe2\x80\xa600CeDisable KV cache. Trade performance for memory usage.0BlSets whether to disable the key-value cache.AcSet the output textoSet temperatureAlSet the tool choice strategyAcSet the tool choiceCfAsync tool callback type for native async tool supportC`Legacy OpenAI compatible text completion requestBiMetadata to initialize the device mapper.0AlA builder for a GGML loader.0AlA builder for a GGUF loader.0ChActivation enum for fused GLU kernel. Must match the \xe2\x80\xa6B`HTTP-based MCP server connectionBbDetails about incomplete responsesBkResize the image via nearest interpolation.CnIn lark syntax, this can be specified as JSON object after \xe2\x80\xa6CkA device mapper which does device mapping per hidden layer.0AmFunction schema for MCP toolsCkConfiguration for a single model in a multi-model setup \xe2\x80\xa6CnConfiguration for recreating a model loader when reloading \xe2\x80\xa60CbBuilder for creating a Model with multiple models.CfUsed to gate access to quantizing onto the host deviceBfEmitted when the response is cancelledAhResponse completed eventC`Emitted when the response completes successfullyChHelper type for messages field in ResponsesCreateRequestBcMetadata for a speculative pipeline0DaA loader for a speculative pipeline using 2 <code>Loader</code>s.0CaAll tools completed, continuing to next iterationCkWrapper of <code>TextModelBuilder</code> for X-LoRA models.CnModel ID to load LoRA from. This may be a HF hub repo or a \xe2\x80\xa6000CkReturns the indices that sort the tensor along the last \xe2\x80\xa6BnBegin tracking stats into an ImatrixLayerStatsChCreate a BlockwiseFP8Linear for MoE with 3D weights [\xe2\x80\xa6AlBuild a multi-model instanceClClean up old completed/failed tasks older than the given \xe2\x80\xa6ChThis will always return [] for non-canonical tokenizers.ClCheck if there are any tokens to fast-forward, forced by \xe2\x80\xa6ChRetrieve some information about a specific model. If \xe2\x80\xa6BmCompute and then consume fast-forward tokens.BiGet the recipient (for tool calls) if any0BbFrequency penalty from the requestAoFrequency penalty (-2.0 to 2.0)1AbGet adapter paths.0CcGet the LoRA adapters for the current engine threadBjGet the content delta since the last call.0BnEnable verbose mode in the regular expression.BmCheck if reasoning content should be includedCgCheck if this chat template uses OpenAI Harmony format.0BgCheck if currently inside a think block0AbClone the matcher.ClReturn a string representation of the tokens, useful for \xe2\x80\xa6CiResets the matcher to the initial state. A matcher in \xe2\x80\xa6CfConstruct a new tokenizer from the given TokenizerInitCmCompute matrix-matrix product. The result will be divided \xe2\x80\xa600000BbMax output tokens from the requestBkMaximum number of output tokens to generateC`Create incomplete details for max tokens reached2AhMCP client configuration0CnNOTE: This can be omitted to use automatic device mapping! \xe2\x80\xa60AlModel-specific device layers0AmPaged attention configuration0CaPush a LoRA adapter for the current engine threadClReasoning/analysis content from Harmony format (separate \xe2\x80\xa6CnReasoning/analysis content delta from Harmony format. This \xe2\x80\xa6CiGet accumulated reasoning content (analysis + commentary)CmGet reasoning content (analysis + commentary without tool \xe2\x80\xa6CaGet all reasoning content (analysis + commentary)CgGet the accumulated reasoning content (inside think \xe2\x80\xa6543210CmSave the error string to the given pointer. The string is \xe2\x80\xa6BhGenerate with the model (non-streaming).CbSpeech generation functionality and route handler.BcSpeech generation endpoint handler.DjWhen set, the string matching <code>stop_rx</code> will be output as a \xe2\x80\xa6C`Timeout for individual tool execution in seconds000AlSet the completion timestampCgDAC Model ID to load from. If not provided, this is \xe2\x80\xa6BkSets the logging configuration if provided.CkSet the maximum number of sequences which can be run at \xe2\x80\xa600000BmAdd a model to the multi-model configuration.CmSource the tokenizer and chat template from this model ID \xe2\x80\xa6BaSource of the Hugging Face token.00000BiSets the token source for authentication.A`Set top logprobsBiAn OpenAI compatible completion response.0AmA streaming response handler.CgController for the CUBLASLT handle and inhibition flag.ocbindgen:ignoreCcRepresents different types of embeddings responses.AkOutput from a function callAmFunction definition for toolsAiConfig for a GGML loader.0AiConfig for a GGUF loader.0BdAccumulated content for each channel0AnDetailed input token breakdownAgModel is already loaded0CfOpenAI Chat format input content types (with text, \xe2\x80\xa6BiInput type for OpenResponses API requestsCkPer-tensor FP8 Linear layer with static activation scaling.AhRoPE supporting LongRope0AjResponse in progress eventAnEmitted when processing startsAiResponse incomplete eventBgEmitted when the response is incompleteAiTool choice configurationAdSpecific tool choiceCjConfigure a text model with the various parameters for \xe2\x80\xa6AmTruncation strategy for inputCkState preserved when a model is unloaded. This contains \xe2\x80\xa60ClConfigure a vision model with the various parameters for \xe2\x80\xa6AmWebSocket-based MCP transportClIf set, the grammar will allow the %ignore lexeme at the \xe2\x80\xa6CkIf set, the grammar will allow invalid utf8 byte sequences.CnGenerate the appropriate MIME content type string for this \xe2\x80\xa6AgThe cross-entropy loss.AfDevice mapping setting0BiDRY allowed length (mistral.rs extension)CeCache path for Hugging Face models downloaded locallyCfConvenience wrapper for generating a single embedding.CkGet model category for a specific model. If model_id is \xe2\x80\xa60C`Helper function to get the global cache instanceCgGet tool callbacks for use with legacy tool calling \xe2\x80\xa600BhDetails about why response is incompleteChHow much \xe2\x80\x9cfuel\xe2\x80\x9d are we willing to spend to build \xe2\x80\xa6CjThis should be called to initialize the debug flag and \xe2\x80\xa60BjList resources from all connected servers.00DeFree the tokenizer. Should <em>NOT</em> be called while there are \xe2\x80\xa6CmCreate a new constraint from a grammar JSON string Always \xe2\x80\xa6CjTokenize the given bytes and return the tokens. Always \xe2\x80\xa6FeIf <code>revision</code> is None, then it defaults to <code>main</code>. If <code>dtype</code> is \xe2\x80\xa60CkCreate a new ThinkTagContext that starts inside a think \xe2\x80\xa60CnGPU memory to allocate for KV cache with PagedAttention in \xe2\x80\xa60CcIf a quantized method, return the activation dtype.CmQuantized filename(s). May be a single filename, or use a \xe2\x80\xa600AcQuantized filename.00111000DhQuantized model ID to find the <code>quantized_filename</code>. This \xe2\x80\xa600000000000BiRepetition penalty (mistral.rs extension)CkDefault repetition penalty (1.0 = no penalty). Default: 1.1BmOverride the description for the search tool.0CmThe storage used by this tensor, together with the layout \xe2\x80\xa6CkGeneric helper to stream tokens and forward them to the \xe2\x80\xa6BdUser data to pass to the tokenize_fnCnTries to advance the parser by consuming the given tokens. \xe2\x80\xa6BeAlias for <code>interpolate1d</code>.BeAlias for <code>interpolate2d</code>.BiWeb search options (mistral.rs extension)DiLiteral Jinja chat template OR Path (ending in <code>.json</code>) to \xe2\x80\xa600BeSets the chat template configuration.BoSet the default model by its model ID or alias.C`Sets whether to enable web search functionality.BeSets the in-situ quantization method.CjSet the loader config for enabling model unload/reload \xe2\x80\xa6000CeAdd multiple models to the multi-model configuration.BaSets the random seed if provided.BcSet the system prompt for the agentBmRegister a callback for a specific tool name.CgRegister a custom callback for the specified tool name.0CeAudio format options for speech generation responses.AjState of a background taskCjThis layer has a weight that is parallelized along the \xe2\x80\xa6CcRepresents different types of completion responses.BmThe architecture to load the vision model as.0BoEngine instructions, per Engine (MistralRs) ID.0C`The architecture to load the embedding model as.0CeAll local paths and metadata necessary to load an \xe2\x80\xa60BoMaximum batch size supported by the GEMV kernelB`Trait for MCP server connectionsClMessage content that can be a string or array of content \xe2\x80\xa6CmInner content structure for messages that can be either a \xe2\x80\xa6CmA builder for a loader for a \xe2\x80\x9cnormal\xe2\x80\x9d (non-quantized) \xe2\x80\xa60AoDetailed output token breakdownAcResponse annotationBbSpeculative decoding pipeline: \xe2\x80\xa60BoRequest to tokenize some messages or some text.0CjA builder for a loader for a vision (non-quantized) model.0CaAppend a chat message to the specified chat file.CkWhether to automatically register discovered tools with \xe2\x80\xa6000DkReturn <code>BF16</code> for devices that support it, otherwise default \xe2\x80\xa6ChBuild a GGUF model pipeline from a GgufModelBuilder. \xe2\x80\xa6ChBuild a text model pipeline from a TextModelBuilder. \xe2\x80\xa6CeClear all LoRA adapters for the current engine threadBiCollect output text from all output itemsCnCompute which tokens can be consumed in the current state. \xe2\x80\xa6CkCompute the number of KV groups, taking into account KV \xe2\x80\xa6CaOverride the description for the extraction tool.0CmFinalize any pending tool call and return all tool calls. \xe2\x80\xa60AdFP8 vector quantize.CeCache path for Hugging Face models downloaded locally0CmGenerate embeddings for one or more inputs configured via \xe2\x80\xa6C`Retrieve the <code>PretrainedConfig</code> file.0ClGet the reasoning delta since last call (for streaming). \xe2\x80\xa6BlGet the reasoning delta since the last call.10BnCheck if cancellation was requested for a taskCkCheck if a chat template uses Harmony format by looking \xe2\x80\xa60ClClone a tokenizer. This increments a reference count and \xe2\x80\xa6AcFree the constraintChGet the current temperature of the constraint. It is \xe2\x80\xa6CjReturns the maximum supported sequence length for this \xe2\x80\xa6CmGet the maximum supported sequence length for a model, if \xe2\x80\xa60BgWhether parallel tool calls are enabledBeWhether to allow parallel tool calls.1CjParse a special token of the form \\xFF [ 1 2 3 4 ] The \xe2\x80\xa6AnNOT meant for external callingBdGenerate with the model (streaming).CmTokenize some text or messages using a specific model. If \xe2\x80\xa6ClBilinear interpolation to resize the input tensor to the \xe2\x80\xa6ClValidate that a file path is safe and within the uploads \xe2\x80\xa6BlValidate MCP configuration for common issuesCiValidates that the requested model matches one of the \xe2\x80\xa6ClReturns how many of the passed tokens can be accepted by \xe2\x80\xa6CaProvide metadata to initialize the device mapper.000CmExplicit JINJA chat template file (.jinja) to be used. If \xe2\x80\xa600BjSets an explicit JINJA chat template file.BiSets the axum default request body limit.BdSet the maximum number of iterationsAbSet max tool callsCfSets the total context length for KV cache allocation.ClSet the number of sequences to hold in the prefix cache. \xe2\x80\xa600CgSets the number of prefix caches to hold on the device.CdPath to a discrete <code>tokenizer.json</code> file.000CiRequest to append a (partial) assistant message to a chatCiWrapper of <code>GgufModelBuilder</code> for LoRA models.AlReference to an MCP resourceC`Include message input image URLs in the responseAiModel is already unloaded0BjConfig specific to loading a normal model.0CfAll memory counts in MB. Default for block size is 32.0BcProcess-based MCP server connectionAjResponse delta output itemCgEmbedding model used for ranking web search results \xe2\x80\xa60BnThis is the underlying instance of mistral.rs.CdA tool callback with its associated Tool definition.00CkConfigure a UQFF text model with the various parameters \xe2\x80\xa6BjConfig specific to loading a vision model.0CkAdd a model with a custom alias (nickname) used for API \xe2\x80\xa6CfAdd a model with a custom alias used for API requests.BfApply Rotary position encoding inplaceCnEnable or disable the \xe2\x80\x9cdot matches any character\xe2\x80\x9d flag \xe2\x80\xa6BaGet the current default model ID.B`Get the current default model ID0BdAmount of available memory in bytes.0CmGet the processor config (for the vision models). This is \xe2\x80\xa60BoModel weights files (multiple files supported).0CcCheck if add_bias() would have returned any tokens.AnDetailed input token breakdownAkList all unloaded model IDs0AdClone the constraintBjReturn pointer to the mask computed by \xe2\x80\xa6BjCheck if the matcher is in an error state.CmBacktracks the matcher states by num_tokens. Returns 0 on \xe2\x80\xa6CaCompute mask for several constraints in parallel.ClReturn a string representation of the tokens, useful for \xe2\x80\xa6CmCheck if given grammar is valid. This about twice as fast \xe2\x80\xa6CkLoad a model from the specified paths. Also initializes \xe2\x80\xa60BoName of the Matryoshka Transformer slice to use00000CjMaximum number of concurrent tool calls across all MCP \xe2\x80\xa6000AlGet the next sequence numberCkLoad GPT-OSS style MXFP4 experts (combined gate_up_proj \xe2\x80\xa6Ck<code>true</code> if built with CUDA (requires Unix) /Metal0CfID of a previous response for multi-turn conversationsCcPrevious response ID (for multi-turn conversations)CfRegister an alternate model ID that resolves to an \xe2\x80\xa60AiSet the default model ID.AhSet the default model ID0CdConvert the current task state to a ResponseResourceAnSets the CORS allowed origins.DjOverride the search function used when <code>web_search_options</code> \xe2\x80\xa600BoUse a custom callback to gather search results.01BkApplication of transforms to the Self type.AlManager for background tasksCjChat completion request following OpenAI\xe2\x80\x99s specificationBgChat completion streaming chunk choice.0AnCustomizable logits processor.0B`Request to detokenize some text.0CjConfigure a text model with the various parameters for \xe2\x80\xa6ClConfigure an embedding model with the various parameters \xe2\x80\xa6AnAn individual embedding input.CkInclude file search results (not currently supported by \xe2\x80\xa6BdRequest to generate speech from textCkWrapper of <code>GgufModelBuilder</code> for X-LoRA models.CmGPT-OSS style rotary embedding with YARN scaling support. \xe2\x80\xa60BiIn-memory implementation of ResponseCacheBaDefines what is allowed in BranchCeReplace invalid UTF-8 with the replacement character.oRoPE for Llama30AgMCP server capabilitiesCdOpenResponses streamer that emits proper event typesBdReal (for Metal) and Fake (for CUDA)AkResponse delta content itemClBuild a speech model pipeline from a SpeechModelBuilder. \xe2\x80\xa6ClBuild a vision model pipeline from a VisionModelBuilder. \xe2\x80\xa6CdConditional (and unconditional) splices are allowed.DkDetokenize some tokens using a specific model. If <code>model_id</code> \xe2\x80\xa6BlDRY sequence breakers (mistral.rs extension)AfFP8 vector dequantize.CjGet the current tool call being built (if any) Returns \xe2\x80\xa60BiFilepath for general model configuration.0ClGiven the image sizes (h, w) and the minimum and maximum \xe2\x80\xa6ChFile where the content is expected to deserialize to \xe2\x80\xa60CnReturns true if the data is stored in a Fortran contiguous \xe2\x80\xa6DoCheck if a chat template uses <code>&lt;think&gt;...&lt;/think&gt;</code> tags.0CnGet the error message from the matcher or null if there is \xe2\x80\xa6CmCommit a token to the stop-sequence controller. Returns a \xe2\x80\xa6CkPath to local Matryoshka Transformer configuration CSV file00000AoNumber of connected MCP serversBg<strong>mistral.rs server core</strong>AoDetailed output token breakdownBoBlock size (number of tokens per block) for \xe2\x80\xa60ClSynchronous version of reload_model for use in non-async \xe2\x80\xa60CjSame as <code>run_attention</code>, but no flash attention0CnGenerate with the model, returning raw logits of the first \xe2\x80\xa6CaCreate a new text output content with annotationsCjTokenize a given byte sequence. It will interpret text \xe2\x80\xa6CgIf this returns true, this tokenizer always returns \xe2\x80\xa6DlTokenize a string, interpreting <code>&lt;name&gt;</code> as special tokens.ChUtilise this calibration file to collcet an imatrix. \xe2\x80\xa6BmUtilise this calibration_file file during ISQCmSet the default model ID to use when none is specified in \xe2\x80\xa6BhSets whether to run in interactive mode.AdSet presence penaltyBnAn OpenAI compatible chat completion response.0AmA streaming response handler.CgThis is the axum default request body limit for the \xe2\x80\xa6CmThe scheduler method controld how sequences are scheduled \xe2\x80\xa60CjA builder for a loader for a vision (non-quantized) model.000AhImage generation requestC`Normalized input content for internal processingBnResponse responder types for OpenResponses APIAiResponse creation requestA`RoPE for SmolLm30AhSpecific function choiceCmCollection of callbacks with their tool definitions keyed \xe2\x80\xa6CiOffset for the quant type. UQFF always serializes the \xe2\x80\xa6CkConfigure a UQFF text model with the various parameters \xe2\x80\xa6BeWebSocket-based MCP server connectionCfBy convention, all images are added before all audios.CkAuto-detect diffusion loader type from a repo file listing.0EgSame as <code>avg_pool2d</code> but with a <code>stride</code> that can be set to a \xe2\x80\xa6CnReturns the amount of time elapsed from another instant to \xe2\x80\xa6AgFP8 blockwise quantize.DbA serialised <code>tokenizers.Tokenizer</code> HuggingFace object.0CkCheck if the matcher will force EOS token. This returns \xe2\x80\xa6CkCreate a new constraint with specified type Type can be \xe2\x80\xa6EgSame as <code>max_pool2d</code> but with a <code>stride</code> that can be set to a \xe2\x80\xa6CmSanitize error messages to remove internal implementation \xe2\x80\xa6ChSelect which built-in search embedding model to load \xe2\x80\xa6CbBuilt-in search embedding model to load (e.g., \xe2\x80\xa6CmIndex of completion tokens to generate scalings up until. \xe2\x80\xa600000C`Tokenize a byte slice. It will interpret &lt;\xe2\x80\xa6AeSet frequency penaltyAeSet max output tokensAmSets the device layer mappingCnTruncate prompts that exceed the model\xe2\x80\x99s maximum context \xe2\x80\xa6CmControl whether prompts longer than the model context are \xe2\x80\xa6ChRepresents different types of chat completion responses.AjCompletion request choice.0BkBuilder for configuring embedding requests.BjConfig specific to loading a vision model.0DiThis is the <code>SharedMistralRsState</code> that has been extracted \xe2\x80\xa6CkGeneration parameters that can be sent per-message from \xe2\x80\xa6AiSpeech generation requestCkTerminate all sequences on the next scheduling step. Be \xe2\x80\xa60CmAuto-detect speech loader type from a config.json string. \xe2\x80\xa60BmCreates a channel for response communication.CdGet the current sequence number without incrementingF`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa6CeGets the keep-alive interval for SSE streams from \xe2\x80\xa6CmGet the preprocessor config (for the vision models). This \xe2\x80\xa60CnHandles sampling commands (\\temperature, \\topk, \\topp) and \xe2\x80\xa6ChIf this returns true, then the regex will match only \xe2\x80\xa6ChList all models with their status (Loaded, Unloaded, \xe2\x80\xa6BaList all models with their status0CkCreate a new constraint from a given JSON schema Always \xe2\x80\xa6ClCreate a new constraint from a given lark grammar Always \xe2\x80\xa6BeCreate a new stop-sequence controllerBhLoad multi-model configuration from fileCfWhether to execute multiple tool calls in parallel \xe2\x80\xa6EgReapply ISQ to a specific model. If <code>model_id</code> is <code>None</code>, the \xe2\x80\xa6CdSet to true to enable hack that works around the \xe2\x80\xa6AfSet incomplete detailsCkSets the GPU memory allocation for PagedAttention KV cache.BaEnable runner throughput logging.000CjSet the model topology from a path. This preserves the \xe2\x80\xa6000CkA callback function that is executed when the streaming \xe2\x80\xa6CiRepresents different types of image generation responses.BdJSON Schema for structured responsesCiPrepend a vision tag appropriate for the model to the \xe2\x80\xa60BdOpenResponses streaming event formatBkBuild a diffusion model pipeline from a \xe2\x80\xa6BmBuild an embedding model pipeline from an \xe2\x80\xa6AiFP8 blockwise dequantize.BlRetrieve conversation history for a responseEdCheck if <code>small</code> is contained in <code>big</code> with a limit on the \xe2\x80\xa6BaFree the stop-sequence controllerClCompute the set of allowed tokens for the current state. \xe2\x80\xa6C`Check if the grammar can fully accept the input.CkCreate a new constraint from a given regular expression \xe2\x80\xa6CnPercentage of GPU memory to utilize after allocation of KV \xe2\x80\xa60ClIf true, we\xe2\x80\x99ll run any extremely large regexes against \xe2\x80\xa6ClPre-initialize the Harmony encoding. This MUST be called \xe2\x80\xa60BnSets the MCP client configuration if provided.AgSet parallel tool callsCiA callback function that processes streaming response \xe2\x80\xa6AlFunction call arguments doneCaEmitted when function call arguments are completeCeA builder for creating a mistral.rs instance with \xe2\x80\xa6CnOpenResponses format input content types (with input_text, \xe2\x80\xa6BdBuilder for PagedAttention metadata.BcInclude reasoning encrypted contentCjRepresents different types of speech generation responses.CeConfigure a UQFF embedding model with the various \xe2\x80\xa6ChCreate a Model from pipeline components. This is the \xe2\x80\xa6DiGenerate an image using a specific model. If <code>model_id</code> is \xe2\x80\xa6CkGet or create a termination flag for the current engine \xe2\x80\xa60CcCheck if the Harmony encoding has been initialized.0CmClone the stop-sequence controller. The cloned controller \xe2\x80\xa6CnAdvance the matcher by one token. Returns 0 on success and \xe2\x80\xa6CkTokenize the given bytes and return the tokens. Special \xe2\x80\xa6CnReturns the amount of time elapsed from another instant to \xe2\x80\xa6CmSet the sampling parameters for deterministic generation. \xe2\x80\xa6ClName of the slice to use from the Matryoshka Transformer \xe2\x80\xa60CnOrganize ISQ to enable MoQE (Mixture of Quantized Experts, \xe2\x80\xa6CjInclude computer call output image URLs (not currently \xe2\x80\xa6AmFunction call arguments deltaCgEmitted when function call arguments are being streamedD`Include special tokens in the output. They may look like &lt;\xe2\x80\xa6B`OpenResponses API create requestBkIncomplete details for incomplete responsesCjGenerate audio given a (model specific) prompt using a \xe2\x80\xa6AoGet the explicit chat template.0CnCreates a search embedding model configuration for agentic \xe2\x80\xa6CbCreate an image URL content part with detail levelCkStatic LoRA in the style of Phi-4 multimodal. Only when \xe2\x80\xa6CgAdvance the matcher by several tokens. Returns 0 on \xe2\x80\xa6BiStore conversation history for a responseChPath to a Matryoshka Transformer configuration CSV file.0BgSets the block size for PagedAttention.0BhChat completion streaming request chunk.0ChDefault buffer size for the response channel used in \xe2\x80\xa6AdInput tokens detailsBfGet the global background task managerChCheck how many tokens can be consumed from the given \xe2\x80\xa6BoReset termination flags for the current engine.0CaSets the chat template configuration if provided.CaSets the in-situ quantization method if provided.CaConfigures whether to include OpenAPI doc routes.ChSets the embedding model used for web search assistance.CjRegister a callback with an associated Tool definition \xe2\x80\xa600CgRegister a custom callback with its associated Tool \xe2\x80\xa60CkA callback function that is executed when the streaming \xe2\x80\xa6CjA builder for creating a mistral.rs server router with \xe2\x80\xa6AeOutput tokens detailsBgmistral.rs instance for server builder.DhSend a chat request to a specific model. If <code>model_id</code> is \xe2\x80\xa6CfSets an explicit JINJA chat template file if provided.ClSets the total context length for KV cache allocation if \xe2\x80\xa6CiEnable or disable parallel tool execution (default: true)CiA callback function that processes streaming response \xe2\x80\xa6B`Image generation response format0CiConvenience wrapper for generating a single embedding \xe2\x80\xa6CfGet tool callbacks paired with their tool definitions.00ClCompute the fast-forward (forced) tokens for the current \xe2\x80\xa6ClCompute the set of allowed tokens for the current state. \xe2\x80\xa6CdSets the percentage of GPU memory to utilize for \xe2\x80\xa6ClDefault keep-alive interval for Server-Sent Events (SSE) \xe2\x80\xa6CfGenerate embeddings for one or more inputs using a \xe2\x80\xa6BeReturn the size of the mask in bytes.CgReturns the maximum supported sequence length for a \xe2\x80\xa6C`Process non-streaming chat completion responses.BkProcess non-streaming completion responses.CaProcess non-streaming image generation responses.CbProcess non-streaming speech generation responses.DkGenerate with a specific model (streaming). If <code>model_id</code> is \xe2\x80\xa6BkBilinear interpolation using scale factors.BmConfigures PagedAttention based on two flags.Bamistral.rs server router builder.CkParse HF tokenizer.json file and return bytes for every \xe2\x80\xa6BjSets the device layer mapping if provided.CjSet the default values for the ConstraintInit Disables \xe2\x80\xa6B`Send initialization notificationClSends the server a initialization notification to let it \xe2\x80\xa600CkGenerate with a specific model, returning raw logits of \xe2\x80\xa6CnSets the GPU memory allocation for PagedAttention KV cache \xe2\x80\xa6CgCheck if the current engine should terminate sequences.0CkSet to true to not use tokenize_fn and instead tokenize \xe2\x80\xa6CcSets the block size for PagedAttention if provided.CdSets the percentage of GPU memory to utilize for \xe2\x80\xa6")