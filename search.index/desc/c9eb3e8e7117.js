rd_("Ajsee [TensorIndex#method.i]0000ChReturns a tensor that is a transposed version of the \xe2\x80\xa6Ah4-bit float (MX4 format)AjContains the success valueAdUnion of the regexesAfElement-wise equality.ClElement-wise comparison with greater-equal, the returned \xe2\x80\xa6CkElement-wise comparison with greater-than, the returned \xe2\x80\xa6BfThe unique identifier for this tensor.BaUnique identifier for this server000CjElement-wise comparison with lower-equal, the returned \xe2\x80\xa6CiElement-wise comparison with lower-than, the returned \xe2\x80\xa6CaPerforms strict matrix-vector multiplication (\xe2\x80\xa6AjElement-wise non-equality.BmChannel receiver for incoming model responses000AmThe type of tool being calledCkHigh-quality lossy compression, commonly used in mobile \xe2\x80\xa6AkIntersection of the regexesAhContains the error valueAfManual device mapping.0CfWidely compatible, lossy compression, good for web \xe2\x80\xa6CmMatches everything the regex doesn\xe2\x80\x99t match. Can lead to \xe2\x80\xa6ChRaw audio data, requires additional format specificationBjSelect a model for running via auto loader0CjUncompressed, largest file sizes but maximum compatibilityfPanicsCgConcatenates two or more tensors along a particular \xe2\x80\xa6CeElement-wise comparison between two tensors, e.g. \xe2\x80\xa6lUse CPU onlyBgUse CPU only (disable GPU acceleration)CcThe dimension size for a specified dimension index.BkComputes the dot product of two 1D tensors.CnApplies the Exponential Linear Unit (ELU) function on each \xe2\x80\xa6BnOptional environment variables for the process000CdThe difference between 1.0 and the next smallest \xe2\x80\xa60ChReturns a matrix with a diagonal of ones of size n by n.DjReturns the sub-tensor fixing the index at <code>i</code> on the first \xe2\x80\xa6CkGet vector with given unique id. Panics if id is out of \xe2\x80\xa6B`Get the handle if not inhibited.iGQA value00000CjIn-situ quantization to apply. Defaults to Q6K on CPU, \xe2\x80\xa6CkReturn number of elements in the hashcons (also largest \xe2\x80\xa6BkLog all responses and requests to this fileDcSimilar to <code>max_keepdim</code> but the target dimension is \xe2\x80\xa6AlMaximum representable value.0DcSimilar to <code>min_keepdim</code> but the target dimension is \xe2\x80\xa6AlMinimum representable value.0CdCreate a new MCP client with the given configurationB`A few defaults are applied here:0CeCreate a builder for a speculative decoding pipeline.111ClCreate an empty builder. You must add at least one input \xe2\x80\xa62222CjCreates a new tensor on the specified device using the \xe2\x80\xa6ClCreates a new builder with the given pipeline, scheduler \xe2\x80\xa6DjCreate a loader builder for a GGUF model. <code>tok_model_id</code> is \xe2\x80\xa6CdConstruct a state machine for a sequence constraint.AfCreate a new hashcons.8328CbCreates a new HTTP transport for MCP communicationCmCreates a new process transport by spawning an MCP server \xe2\x80\xa6CjCreates a new WebSocket transport connection to an MCP \xe2\x80\xa6CjNote: we only support AFQ and unquantized here because \xe2\x80\xa6DdBinds the listener and then accepts exactly <code>n_nodes</code> \xe2\x80\xa6CeCreates a wrapper around a memory mapped file and \xe2\x80\xa6AlCreate a new in-memory cacheDeCreates a new <code>MistralRsForServerBuilder</code> with default \xe2\x80\xa6DhCreates a new <code>MistralRsServerRouterBuilder</code> with default \xe2\x80\xa6BnReturns an instant corresponding to \xe2\x80\x9cnow\xe2\x80\x9d.CkPad an image of shape (c, h, w) to (c, max_h, max_w) by \xe2\x80\xa6AhPointwise pow operation.CnReturns the amount of time elapsed from another instant to \xe2\x80\xa6ClReturns the sum of all elements in the input tensor. The \xe2\x80\xa6C`Base URL of the MCP server (http:// or https://)0AoWebSocket URL (ws:// or wss://)01010CjReturns the unbiased variance over the selected dimension.BgAutomatic device mapping (recommended).CeAllow automatic selection of any given tool, or none.10ClMatches this byte only. If byte is not in 0..127, it may \xe2\x80\xa6CfThe current state is dead. Should be only true for \xe2\x80\xa60BaThe stream has completed entirelyCjLossless compression, larger file sizes but good audio \xe2\x80\xa6AdSelect a GGML model.0AdSelect a GGUF model.0BnHTTP-based MCP server using JSON-RPC over HTTP000AcLark parser grammarAoA value of type <code>L</code>.0AjSelect a LoRA architecture0BgReal device mapping for a NCCL pipeline0AlDisallow selection of tools.iNo value.01CdGood compression efficiency, ideal for real-time \xe2\x80\xa6BbSome value of type <code>T</code>.0AgFree-form text responseBaSelect the model from a toml file0B`Force selection of a given tool.oTool definition100AnThe architecture of the model.0000000000000B`Arguments to pass to the command000CmCompared to clone, this copies the actual storage but may \xe2\x80\xa6C`The dimension size for this tensor on each axis.Ca.toml file containing the selector configuration.0ClReturns a new tensor with the order of elements reversed \xe2\x80\xa6B`More than one branch is allowed.AoReturns the argument unchanged.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000CnReturns a new tensor with all the elements having the same \xe2\x80\xa6BiIP address to serve on (default: 0.0.0.0)BaCalls <code>U::from(self)</code>.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000CjLazy gen()s take the shortest match. Non-lazy take the \xe2\x80\xa6DeLoads the ring backend config from a path at <code>RING_CONFIG</code>CmReturns the mean of all elements in the input tensor. The \xe2\x80\xa6BcHuman-readable name for this server0BnName of the tool as reported by the MCP server0CjThe name of this grammar, can be used in GenGrammar nodes.2121B`The name of the function to callDmComputes the <strong>Frobenius norm</strong> (L2 norm of all elements) of \xe2\x80\xa6BfCreates a new tensor filled with ones.BbCheck if the connection is healthyBlCheck if the transport connection is healthyCcTests the HTTP connection by sending a ping requestCfTests the process connection by sending a ping requestCfSends a WebSocket ping frame to test connection healthAaPort to serve on.BaPort to listen on (default: 8080)CgRaise the tensor to some float exponent <code>e</code>.ChCreates a new tensor initialized with values sampled \xe2\x80\xa6ClThe number of dimensions for this tensor, 0 for a scalar \xe2\x80\xa6CnThe role of the message sender (\xe2\x80\x9cuser\xe2\x80\x9d, \xe2\x80\x9cassistant\xe2\x80\x9d\xe2\x80\xa6CmRoll the tensor input along the given dimension. Elements \xe2\x80\xa6CeInteger seed to ensure reproducible random number \xe2\x80\xa60BjReturns the total of model execution time.00CnWhen disabled, translation will permit the construction of \xe2\x80\xa6AbGeneral utilities.CcThe different types of elements allowed in tensors.BhDelta in content for streaming response.0CkThe object used to interact with the model. This can be \xe2\x80\xa6B`Multiple possible stop sequencesCfSelect a plain model, without quantization or adapters0ChCompile the regex using the regex_syntax crate. This \xe2\x80\xa6AjRegular expression grammarAoA value of type <code>R</code>.0CdOpenAI compatible (superset) usage during a request.0AmSelect an X-LoRA architecture0CnLogits and sequence context (prompt and generated tokens), \xe2\x80\xa6DkRun the <code>forward</code> method of <code>m</code> on <code>self</code>.1CgIf the loader type is not specified, loader type is \xe2\x80\xa60BjBuilds the configured mistral.rs instance.BbBuilds the configured axum router.ClSplit a tensor into the specified number of chunks, this \xe2\x80\xa6DlClamp the tensor values to be between <code>min</code> and <code>max</code>.AnClose the transport connectionBdCloses the HTTP transport connectionCdTerminates the child process and cleans up resourcesBjGracefully closes the WebSocket connectionCfThe dtype for the elements stored in the input tensor.BoModel data type. Defaults to <code>auto</code>.000000000000000000000000000BbA device mapper to not map device.000CfCreates a new tensor filled with uninitialized memory.AiCreate an empty topology.0Bh\xce\xb3 completions to run of the draft model0BfGet the underlying MistralRs instance.BfThe text content to convert to speech.eModelnModel selector00BjThe TTS model to use for audio generation.ClCreates a wrapper around multiple memory mapped file and \xe2\x80\xa6BdNumber of generations tokens to run.AbOrdering JSON file000000000CmCreates a new tensor initialized with values sampled from \xe2\x80\xa6CdThe tensor shape, i.e. dimension sizes on each axis.ChStacks two or more tensors along a particular dimension.AnUnderlying mistral.rs instance000BiDefault top_k for generation. Default: 40CdDefault top_p for generation (0.0-1.0). Default: 0.9CiReturns a lower triangular matrix of ones of size n by n.CjReturns an upper triangular matrix of ones of size n by n.BlCustom types used in mistral.rs server core.BgCreates a new tensor filled with zeros.AgChat completion choice.0CmThe Client holds its persistent connection inside a Mutex \xe2\x80\xa6AlConcatenation of the regexesAcCpu, Cuda, or MetalCm6-bit float with 2 exponent bits and 3 mantissa bits (MX6 \xe2\x80\xa6Cm6-bit float with 3 exponent bits and 2 mantissa bits (MX6 \xe2\x80\xa6Cd8-bit float with 8 exponent bits and 0 mantissa bitsDdThe <code>Loader</code> trait abstracts the loading process. The \xe2\x80\xa60CeDevice/configurable intelligent matrix multiplication00BgRaw text prompt that will be tokenized.CjRepeat the regex at least min times, at most max times \xe2\x80\xa6BlThe Server maintains persistent connections.AdSingle stop sequenceEhExtension trait adding <code>argsort</code> / <code>sort</code> convenience calls on \xe2\x80\xa6BkSpeech models for text-to-speech generationBdDescribes what to do after sampling.BiThe core struct for manipulating tensors.AdPre-tokenized input.DgThis operation multiplies the input tensor by <code>mul</code> then \xe2\x80\xa6CiCreates a new 1D tensor with values from the interval \xe2\x80\xa6DfSimilar to <code>argmax_keepdim</code> but the target dimension is \xe2\x80\xa6DfSimilar to <code>argmin_keepdim</code> but the target dimension is \xe2\x80\xa6BaString representation for dtypes.DiAll chunks received during streaming (if <code>store_chunks</code> is \xe2\x80\xa6000BkRetrieve some information about this model.BgGet configuration for a specific model.AoGet config for a specific modelCaMulti-model configuration file path (JSON format)10BoApplies a 1D convolution over the input tensor.BoApplies a 2D convolution over the input tensor.CnReturns the cumulative sum of elements of the input tensor \xe2\x80\xa6CiReturns a new tensor detached from the current graph, \xe2\x80\xa6C`The device on which the input tensor is located.ClThis is the primary interface for llguidance \xe2\x80\x93 the one \xe2\x80\xa6AjAn alias for broadcast_as.BjGather values across the target dimension.BoInsert a given vector and return its unique id.CnThe layout of the input tensor, this stores both the shape \xe2\x80\xa6CmThe resource limits for the parser Default values will be \xe2\x80\xa6CnReturns the matrix-multiplication of the input tensor with \xe2\x80\xa6AnCompute matrix-matrix product.00CjReturns a new tensor that is a narrowed version of the \xe2\x80\xa6B`OpenAI compatible functionality.CbRepeat this tensor along the specified dimensions.CmTokenizer partitions for the slicer optimization. This is \xe2\x80\xa6BkTransport-specific connection configuration000CkThe tokens to append to the output if any This is valid \xe2\x80\xa6DiReturns a view of which contains all slices of size <code>size</code> \xe2\x80\xa6CnMatches any byte in the set, expressed as bitset. Can lead \xe2\x80\xa6BoA unique ID of a symbol in the compiled grammarB`Reference previously built regexBoGrammar specification for structured generationCiA measurement of a monotonically nondecreasing clock. \xe2\x80\xa6AhMatches this string onlyCjThis is meant to be used in server-side scenarios. The \xe2\x80\xa6BmRepresents a single message in a conversationB`Matches nothing. Same as Or([]).CeLocal process-based MCP server using stdin/stdout \xe2\x80\xa6000CfA request to the Engine, encapsulating the various \xe2\x80\xa60C`Multiply the pixe values by the provided factor.CjThe stream is actively processing and sending response \xe2\x80\xa6CkAcquire the quantize drop guard to protect the critical \xe2\x80\xa6DkRun the <code>forward</code> method of <code>m</code> on <code>self</code>.CcRegular expression matching the body of generation.BnCreate a new builder for an embedding request.CfCommand to execute (e.g., \xe2\x80\x9cmcp-server-filesystem\xe2\x80\x9d)000AcThe message contentBbImage dimensions will be 720x1280.0CaCreates a new builder with default configuration.0CfReturns the amount of time elapsed since this instant.BgWhether this server should be activated000CkFlattens the input tensor on the dimension indexes from \xe2\x80\xa6F`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa6ClOptional headers to include in requests (e.g., API keys, \xe2\x80\xa60BlOptional headers for the WebSocket handshake01010Ca.imatrix file to enhance GGUF quantizations with.Ce.imatrix file to enhance GGUF quantizations with. \xe2\x80\xa6Ck.cimatrix file to enhance GGUF quantizations with. This \xe2\x80\xa6210mInitial stateAiShould the sequence stop?0CkComputes the max of all the elements in this tensor and \xe2\x80\xa6CkComputes the min of all the elements in this tensor and \xe2\x80\xa6CeOptional callback to execute when streaming completes000CnReturns a tensor with the same data as the input where the \xe2\x80\xa6BhCompute quantized matrix-matrix product.00CkReshape returns a tensor with the target shape provided \xe2\x80\xa6CiThe token that was sampled (after applying the mask), \xe2\x80\xa6BaList of MCP servers to connect to000DjInitializes a <code>VarBuilder</code> that retrieves tensors stored in \xe2\x80\xa6CnDescribes what to do after sampling. If no sampling, there \xe2\x80\xa60CmCreates a new tensor with the specified dimension removed \xe2\x80\xa6DcThe whole generation must match <code>body_rx + stop_rx</code>. \xe2\x80\xa6CkComputes the sum of all the elements in this tensor and \xe2\x80\xa6CjConvert multi channel audio to mono by averaging channels.00B`Extract text from MessageContentBdAn alias for <code>to_scalar</code>.ClReturns the data contained in a 1D tensor as a vector of \xe2\x80\xa6ClReturns the data contained in a 2D tensor as a vector of \xe2\x80\xa6BjReturns the data contained in a 3D tensor.CmThe token ID for the end of sentence token For chat mode, \xe2\x80\xa6CoEnable or disable the Unicode flag (<code>u</code>) by default.AnFunction definition for a tool00AcLogprobs per token.0AnSelect a GGML model with LoRA.0AnSelect a GGUF model with LoRA.0BcAdapter model ordering information.0BoThe response enum contains 3 types of variants:0CmConvert an image to a tensor. This converts the data from \xe2\x80\xa6BlRepresents a tool call made by the assistantlType of tool00Cl\xe2\x80\x9cCollapse\xe2\x80\x9d the stack so that it consists only of its \xe2\x80\xa6CiCreates a new tensor initialized with values from the \xe2\x80\xa6AiThe function call detailsClAutomatically resize and pad images to this maximum edge \xe2\x80\xa6000CaMaximum running sequences at any time. If the \xe2\x80\xa6AmPort to serve MCP protocol onChCreates grids of coordinates specified by the 1D inputs.CjModel ID to load from. May be a HF hub repo or a local \xe2\x80\xa6CiModel ID to load from. This may be a HF hub repo or a \xe2\x80\xa6CkForce a base model ID to load from instead of using the \xe2\x80\xa60111121001111BgModel identifier (used in API requests)AoNumber of prompt tokens to run.CcThe number of tokens in the tokens array (can be 0)CfOptional callback to process each chunk before sending000Ajself |= other &amp; !minusCiAdd an element to the vector being inserted. Requires \xe2\x80\xa6AkQuantize the model into HQQCjWhen set, \xe2\x80\x9c\xe2\x80\xa6\xe2\x80\x9d will not be added around the final \xe2\x80\xa6ClReads a npy file and return the stored multi-dimensional \xe2\x80\xa6CmReads a npz file and returns the stored multi-dimensional \xe2\x80\xa6AjRead a wav file from disk.00CiRound element of the input tensor to the nearest integer.BiIP to serve on. Defaults to \xe2\x80\x9c0.0.0.0\xe2\x80\x9dChCasts the input tensor to the target <code>dtype</code>.A`Associated trie.AoTokenize some text or messages.CiTokenize a string coming from user. It may or may not \xe2\x80\xa6AmPath to a topology YAML file.00000000000000000000000CkReturns true if the computation graph should track this \xe2\x80\xa6BiSets whether to force CPU-only execution.CjUse ISQ of a certain type. If there is an overlap, the \xe2\x80\xa600AoSets the logging configuration.BjOptional working directory for the process000CeRepresents the current state of a streaming response.BhDummy device mapping for a NCCL pipeline0CkSelect an embedding model, without quantization or adapters0ClTransition via any byte leads to a dead state but EOI is \xe2\x80\xa60CkQuote the regex as a JSON string. For example, [A-Z\\n]+ \xe2\x80\xa6CnMatches the regex; should be at the end of the main regex. \xe2\x80\xa6CkMCP client that manages connections to multiple MCP servers000CmThe MistralRs struct handles sending requests to multiple \xe2\x80\xa60AkThe kind of model to build.0BcOptional fields allowed on any NodeCkNormalize the image data based on the mean and standard \xe2\x80\xa6CjWebSocket-based MCP server for real-time bidirectional \xe2\x80\xa6000B`Select a GGML model with X-LoRA.0B`Select a GGUF model with X-LoRA.0BlAdd a new model to the multi-model instance.C`Add a new model engine to the MistralRs instance0CnAdd a model with just an ID and ModelSelected (convenience \xe2\x80\xa6BdIf the quant is backed by a qmatmul.BhConvert the response into a result form.0AhBacktracking is allowed.CjSampling result for the previous iteration. For simple \xe2\x80\xa6ClBacktrack this much before appending this sequence (this \xe2\x80\xa6AjCall a tool on this serverDiReturns a tensor with the values from the <code>self</code> tensor at \xe2\x80\xa6AeEnd of sentence tokenB`Unconditional splice is allowed.BgAppend these tokens after backtracking.BiCreates a new 1D tensor from an iterator.BiCreate a new MessageContent from a stringD`Path to read a <code>.uqff</code> file from. Other necessary \xe2\x80\xa600CnUQFF path to load from. If provided, this takes precedence \xe2\x80\xa600000000000CiCheck if there are any errors to be reported to the user.CbGet error message if recognizer is in error state.B`Get discovered tools information00EgAccumulate element from <code>source</code> at indexes <code>indexes</code> and add \xe2\x80\xa6CnTreat stop_rx as suffix, i.e., do not hide it from the LLM \xe2\x80\xa6CcPointer to memory where the mask should be written.DcThis crate provides an asynchronous API to <code>mistral.rs</code>.AgGemma uses weight + 1.00CkCheck if the there is only one transition out of state. \xe2\x80\xa60BcNormalize audio to prevent clipping00BoEstimate the size of the regex tables in bytes.0BnEstimate number of bytes used by the hashcons.1CjCreates a new tensor filled with ones with same shape, \xe2\x80\xa6C`Polls the stream for the next Server-Sent Event.0Aofor _ in 0..num { stack.pop() }CcOptional resource URI patterns this server provides000BoResponses API functionality and route handlers.BeID of the server this tool comes from000AaGet the server IDB`The default tool choice is auto.EdSet the values on <code>self</code> using values from <code>src</code>. The copy \xe2\x80\xa6AhSSE streaming utilities.CjIf the target device is the same as the tensor device, \xe2\x80\xa6ClRetrieves the single scalar value hold in the tensor. If \xe2\x80\xa6CfThe tokenizer to use, created with llg_new_tokenizer()ChReturns a tensor that is a transposed version of the \xe2\x80\xa6CnCreates a new tensor with a dimension of size one inserted \xe2\x80\xa6CfSets the random seed for deterministic model behavior.CcWrites a multi-dimensional array in the npy format.CjWrites multiple multi-dimensional arrays using the npz \xe2\x80\xa6CjRaw audio input consisting of PCM samples and a sample \xe2\x80\xa600CkAutomatically selects between a normal or vision loader \xe2\x80\xa60BgControl the constraint with llguidance.0CiTransition via any other byte, or EOI leads to a dead \xe2\x80\xa60AjA loader for a GGML model.0AhLoader for a GGUF model.0AcJSON schema grammarBkStructured response following a JSON schemaAbLLGuidance grammarAaMCP content typesAdDType for the model.0Dh<code>ModelPaths</code> abstracts the mechanism to get all necessary \xe2\x80\xa60BoSelect multi-model mode with configuration fileCjA simpler multi-model interface that wraps an existing \xe2\x80\xa61CiMultipleOf(d, s) matches if the input, interpreted as \xe2\x80\xa6BbParser has not emitted stop() yet.Bl<code>NormalLoader</code> for a Phi 2 model.0Bl<code>NormalLoader</code> for a Phi 3 model.0DeTransition via some bytes <em>may be</em> possible. The bytes are \xe2\x80\xa60AfStop sequences or ids.0BgStop token configuration for generationAfTop-n logprobs element0DkTransforms to apply, starting with the <code>input</code> and then with \xe2\x80\xa6AiAdd a single text prompt.BbAdd a single pre-tokenized prompt.BkApply fade in/out to reduce audio artifacts00Ci2D average pooling over an input tensor with multiple \xe2\x80\xa6CnPagedAttention KV cache type (auto or f8e4m3). Defaults to \xe2\x80\xa60CnCheck whether the current parser state forces the sequence \xe2\x80\xa6BcThe constraint to compute mask for.ClReturns a tensor that is in row major order. This is the \xe2\x80\xa6AgDetokenize some tokens.BhCurrent state of the streaming operation000BmThe number of elements stored in this tensor.BfOpenAI-compatible embeddings endpoint.ClCreates a new tensor filled with uninitialized memory of \xe2\x80\xa6DjFlattens the input tensor on the dimension indexes from <code>0</code> \xe2\x80\xa6AlLogs to be sent to the user.C`Decode audio bytes using <code>symphonia</code>.00CgCreate a MultiModel from an existing Model that has \xe2\x80\xa6CiCreates a new tensor initialized with values from the \xe2\x80\xa6BhRetrieve streaming chunks for a responseDhReturns the sub-tensor fixing the index at <code>index</code> on the \xe2\x80\xa6CnGet sender for a specific model. If model_id is None, uses \xe2\x80\xa60C`Initialize connections to all configured servers00CnThis wraps the EmbeddingModelBuilder, so users should take \xe2\x80\xa6CkThis wraps the VisionModelBuilder, so users should take \xe2\x80\xa60CiSpecial nodes can\xe2\x80\x99t be removed in grammar optimizationsBeList available tools from this serverCk2D max pooling over an input tensor with multiple channels.BmDefault max tokens to generate. Default: 2048BbMCP client configuration file pathClNormally, when a sequence of bytes is forced by grammar, \xe2\x80\xa6CmEnable PagedAttention on Metal. Because PagedAttention is \xe2\x80\xa60AfThe function argumentsCfAdd a slice to the vector being inserted. Requires \xe2\x80\xa6CgReceives the broadcasted ID from the persistent stream.C`Directory for storing generated speech wav filesClAn array of the lengths of the token strings (vocab_size \xe2\x80\xa6AkOptional list of tool callsCkGiven a transition (a from-state and a byte) of the DFA \xe2\x80\xa6CaExtend the recognizer with given byte if allowed.CbGemma uses weight + 1.0. Undo for UQFF generation.0BfThe number of tokens in the vocabularyCmReturns a tensor with the same shape as the input tensor, \xe2\x80\xa6BbLoad the model in a certain dtype.0000AjSets the model to be used.DfPath to write a <code>.uqff</code> file to and serialize the other \xe2\x80\xa600AfUQFF path to write to.00000000000CcUpgrades an HTTP request to a WebSocket connection.CkCreates a new tensor filled with zeros with same shape, \xe2\x80\xa6CjMatches this string of bytes only. Can lead to invalid \xe2\x80\xa6BgChat completion streaming chunk choice.0BkDevice/configurable intelligent convolutionBmMatches the empty string. Same as Concat([]).Bl<code>NormalLoader</code> for a Gemma model.0CeRegex string used to extract image URLs from prompts.Cd<code>VisionLoader</code> for an LLaVA Vision model.0Bl<code>NormalLoader</code> for a Llama model.0CcFunction which llg calls when an operation is done.CjDistinguish at runtime which kind of model we have loaded.CfInformation about a tool discovered from an MCP server000CgConfiguration for a single model in a multi-model setupCbModel information metadata about an available modeCkTop-level parser indicates that no more bytes can be added.CcJSON-RPC error codes based on MCPEx.Protocol.ErrorsCc<code>VisionLoader</code> for a Phi 3 Vision model.0BhQuantized method for a quantized matmul.Bm<code>NormalLoader</code> for a Qwen 2 model.0BkA type which can be used as a chat request.CnCompile the regex using the regex_syntax crate, but do not \xe2\x80\xa6CkThe stream has finished processing and is about to send \xe2\x80\xa6AkThe source of the HF token.0ClA hashconsing data structure for vectors of u32. Given a \xe2\x80\xa6ChSelect a vision plain model, without quantization or \xe2\x80\xa60ClAdd a delta weight from LoRA to the weights. This should \xe2\x80\xa6AmAdd a message to the request.BbAdd multiple text prompts at once.CiCreates a new 1D tensor with values from the interval \xe2\x80\xa6CmGets the best device, cpu, cuda if compiled with CUDA, or \xe2\x80\xa6G`Returns <code>Some(t)</code> where <code>t</code> is the time <code>self + duration</code> if <code>t</code> \xe2\x80\xa6G`Returns <code>Some(t)</code> where <code>t</code> is the time <code>self - duration</code> if <code>t</code> \xe2\x80\xa6ChReturn how many tokens and bytes need to chopped off \xe2\x80\xa6BlCompletions functionality and route handler.BoOpenAI-compatible completions endpoint handler.CbNumber of concurrent requests to run. Default is 1CiOptional human-readable description of what the tool does000CiExpands a mask from (bs, seq_len) to (bs, 1, tgt_len, \xe2\x80\xa60Bbinclusive = false, reverse = falseChFlattens the input tensor by reshaping it into a one \xe2\x80\xa6DaGet embedding model <code>modules.json</code> compatible with \xe2\x80\xa60BcApplies a unary custom op in place.CjApplies a unary custom op in place (for the first tensor).CgApplies a ternary custom op in place (for the first \xe2\x80\xa6CmWhether this tensor is a variable or not. A variable is a \xe2\x80\xa6CjThe JSON schema that the grammar should generate. When \xe2\x80\xa6AmList all available model IDs.AlList all available model IDs0BcReturns log(sum(exp(tensor), dim)).ClGathers the maximum value across the selected dimension. \xe2\x80\xa6ClMaximum prompt sequence length to expect for this model. \xe2\x80\xa6000000000000000000000CjMaximum sequence length for context. If not specified, \xe2\x80\xa6ClGathers the minimum value across the selected dimension. \xe2\x80\xa6A`Use no KV cache.AjOpenAPI doc functionality.CmFactor by which the weight size is reduced over the given \xe2\x80\xa600BdNumber of times to repeat each test.CnOne bit per vocab token This is valid until any call to llg\xe2\x80\xa6CiIf None, no sampling is performed. If Some(set), only \xe2\x80\xa60CdDisplay name of the server for logging and debugging000AcGet the server nameBfSet whether to inhibit CUBLASLT usage.ClReturns the sum of all elements in the input tensor. The \xe2\x80\xa6AoTemperature to use for samplingCkOverride temperature for sampling. It may or may not be \xe2\x80\xa60AnOverride sampling temperature.0CjDefault temperature for generation (0.0-2.0). Default: 0.7ClRepeated flag for text-only models (HuggingFace model ID \xe2\x80\xa6CmA pointer to the token strings The length of this the sum \xe2\x80\xa6ClTokenization function, see LlgTokenizeFn docs. It should \xe2\x80\xa6CiOptional prefix to add to all tool names from this server000CkAccepts multipart text file upload and returns the file \xe2\x80\xa6CjReturns the unbiased variance over the selected dimension.CjSet the main device to load this model onto. Automatic \xe2\x80\xa6000CbSets the Candle device to use for model execution.BoEnable searching compatible with the OpenAI \xe2\x80\xa600AmA streaming response handler.CmTemplate for chat models including bos/eos/unk as well as \xe2\x80\xa60BmConfiguration for creating an engine instance0BeTransport layer for MCP communicationAnCollection of available modelsCbA loader for a \xe2\x80\x9cnormal\xe2\x80\x9d (non-quantized) model.0AkPlain text (chat) messages.CiCallback used for custom tool functions. Receives the \xe2\x80\xa6000BfType which can be converted to a DType0BlA loader for a vision (non-quantized) model.0CjDoes the engine support backtracking? (Removing tokens \xe2\x80\xa6CiFused batch matmul + add + Relu/Gelu activation using \xe2\x80\xa6BhOptional Bearer token for authentication000ChBroadcast the input tensor to the target shape. This \xe2\x80\xa6ChBroadcasts the given ID over all persistent connections.Ckcheck if stack.top() transitions via byte to a viable stateClcommit_token() is a top-level method in this file and is \xe2\x80\xa6CmThis is a top-level method in this file.  It is called by \xe2\x80\xa6CjCompute which tokens can be consumed in the current state.ClThis computes token sampling mask. It typically takes up \xe2\x80\xa6AoRegex is empty iff self \xe2\x8a\x86 bigCgDAC Model ID to load from. If not provided, this is \xe2\x80\xa60CkDoes the engine support fast-forward tokens? (Appending \xe2\x80\xa6CkFlattens the input tensor on the dimension indexes from \xe2\x80\xa6CnThis can be called before the first compute_mask() to walk \xe2\x80\xa6CmCreates a fresh tensor structure based on a storage and a \xe2\x80\xa6B`Retrieve a response object by IDAkGet response by ID endpointCnGet current settings (default generation params and search \xe2\x80\xa6CbHandle route / generation errors and logging them.0CiHelper function to handle image generation errors and \xe2\x80\xa6CjHelper function to handle speech generation errors and \xe2\x80\xa6B`Core functionality for handlers.ClSelect values for the input tensor at the target indexes \xe2\x80\xa6CdJSON schema describing the tool\xe2\x80\x99s input parameters000CnCan the grammar be finished in the current state? In other \xe2\x80\xa6CkThe Lark grammar that the grammar should generate. When \xe2\x80\xa6CmReturns the mean of all elements in the input tensor. The \xe2\x80\xa6AdGemma 3n uses weight0ClISQ organization: <code>default</code> or <code>moqe</code>.EeISQ organization: <code>default</code> or <code>moqe</code> (Mixture of Quantized \xe2\x80\xa610CkPrefix for inclusion in messages (may do nothing if the \xe2\x80\xa600000CkReapply ISQ to the model. This will be done on whatever \xe2\x80\xa6AeRemove a model by ID.CaRemove a model engine from the MistralRs instance0CmDispatch a request to the appropriate engine based on the \xe2\x80\xa60BnSend a JSON-RPC request and receive a responseCgSends an MCP request over HTTP and returns the responseCmSends an MCP request to the child process and returns the \xe2\x80\xa6CiSends an MCP request over WebSocket and waits for the \xe2\x80\xa6CaSends a request to the model processing pipeline.BeSet the sampling parameters as given.EhReturns a copy of <code>self</code> where the values within <code>ranges</code> have \xe2\x80\xa6CeReturn all the nodes that lead to this value in a \xe2\x80\xa6CcStart insertion process for a vector. Panics if \xe2\x80\xa6BeStore streaming chunks for a responseCcWhether to store chunks for the completion callback000CmOptional timeout in seconds for HTTP requests Defaults to \xe2\x80\xa60ClOptional timeout in seconds for connection establishment \xe2\x80\xa601010Dk<code>tok_model_id</code> is the local or remote model ID where you can \xe2\x80\xa600CmModel ID to load the tokenizer from. This may be a HF hub \xe2\x80\xa600111000CiSource of the token for authentication. Can be in the \xe2\x80\xa6BnCalled when iteration over the trie is startedDkAccepts multipart audio upload, stores under <code>cache/uploads/</code>\xe2\x80\xa6CcAccepts multipart image upload, stores it under \xe2\x80\xa6CiIf one of the tokens in when_sampled is sampled, this \xe2\x80\xa6CkUtilise this imatrix file during ISQ. Incompatible with \xe2\x80\xa6oEnable logging.00000CkTop-level parser allowed EOS (as it was in an accepting \xe2\x80\xa6AhHTTP-based MCP transportCcSomething went wrong with creating a nested parser.CbTokenization function Will not write more than \xe2\x80\xa6C`A builder for a loader using the selected model.0AiMCP initialization resultAfMCP server informationAdMCP tool call resultBkOpenAI-compatible tool schema for MCP toolsCkCategory of the model. This can also be used to extract \xe2\x80\xa60CgA normal request request to the <code>MistralRs</code>.0AkTrait for caching responsesBkCollection of callbacks keyed by tool name.00CfContext for managing vision messages and image buffer.AjApplies a unary custom op.AkApplies a binary custom op.AlApplies a ternary custom op.BiBroadcasting version of <code>pow</code>.CaJinja format chat templating for chat completion.0DdChat template file with a JINJA file with <code>messages</code>, \xe2\x80\xa6AlModel-specific chat template0CjPart of the interface for \xe2\x80\x9csubsumption\xe2\x80\x9d, a feature \xe2\x80\xa6ClExtend the current state of the parser with given token. \xe2\x80\xa6BmThis sets up the parameters so that there is:0BoEnable searching compatible with the OpenAI \xe2\x80\xa6CaEnable web search tool (requires embedding model)DkReturns the tensor\xe2\x80\x99s values (ascending) sorted along <code>axis</code>\xe2\x80\xa6CmFinish insertion process for a vector. Returns the unique \xe2\x80\xa6ClAutomatically resize and pad images to this maximum edge \xe2\x80\xa6CkDetermine the base cache directory for the application. \xe2\x80\xa6AdPer-connection task.CfCache path for Hugging Face models downloaded locally.CeCache path for Hugging Face models downloaded locally0000100000AnIn-situ quantization to apply.0BcModel-specific in-situ quantization0CiCreate and return the initial state of a DFA for this \xe2\x80\xa6DfInterpolate the input tensor to the <code>target_size</code> size, \xe2\x80\xa6DiInterpolate the input tensor to the <code>(target_h, target_w)</code> \xe2\x80\xa6CgConverts the chat completion responder into an HTTP \xe2\x80\xa6ChConverts the completion responder into an HTTP response.ChConverts the image generation responder into an HTTP \xe2\x80\xa6CiConverts the speech generation responder into an HTTP \xe2\x80\xa6CmReturns true if the data is stored in a C contiguous (aka \xe2\x80\xa6CnGet the error message from the constraint or null if there \xe2\x80\xa6EhIf <code>training == true</code>, <code>loss_csv_path</code> will not save anything. \xe2\x80\xa60CjThe length of the mask_dest array in bytes (not elements).CbModel Context Protocol (MCP) Client ImplementationCmDisable PagedAttention on CUDA. Because PagedAttention is \xe2\x80\xa60DjPad the input tensor using same values along dimension <code>dim</code>\xe2\x80\xa6BoParses and validates a chat completion request.BjParses and validates a completion request.C`Parses and validates a image generation request.CaParses and validates a speech generation request.B`Read a resource from this serverCjMake sure given regex can be used inside /\xe2\x80\xa6/ in Lark \xe2\x80\xa6BaComputes softmax(QK^T*sqrt(d_k))V0DiThe size used by each element in bytes, i.e. 1 for <code>U8</code>, 4 \xe2\x80\xa6EfEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor \xe2\x80\xa6CjSorts the tensor along the last dimension, returns the \xe2\x80\xa6ClRepeated flag for speech models (HuggingFace model ID or \xe2\x80\xa6ClReturns an iterator over position of the elements in the \xe2\x80\xa6BcDefault system prompt for all chatsCmCalled when iteration over the trie is finished Stack has \xe2\x80\xa6EhThis combines <code>push_byte</code> and <code>byte_allowed</code> into one function \xe2\x80\xa6Ck<code>ToString::to_string</code>, but without panic on OOM.000000000000000000000000000000000000000ClRepeated flag for vision models (HuggingFace model ID or \xe2\x80\xa6C`Sets the maximum number of concurrent sequences.CnSet the model topology for use during loading. If there is \xe2\x80\xa6000BmConfiguration for adding a model to MistralRs0BgCalled function with name and arguments00CjSelect a diffusion model, without quantization or adapters0C`Represents a function call made by the assistantCh<code>VisionLoader</code> for an Idefics 2 Vision model.0CiA transform over an image. The input may vary but the \xe2\x80\xa6Ckmax_tokens limit on the total number of tokens has been \xe2\x80\xa6CmMessage content that can be either simple text or complex \xe2\x80\xa6CkOnly quantize MoE experts, if applicable. The enables MoQE.0CkA callback function that is executed when the streaming \xe2\x80\xa6AnGlobal response cache instanceBoA way to add messages with finer control given.BoMessage or messages for a <code>Request</code>.0B`Response format for model outputAhResponse streaming chunkAlResponse delta for streamingnResponse errorAjResponse usage informationBmSampling params are used to control sampling.0CnCallback used to override how search results are gathered. \xe2\x80\xa600CgConvert an image to a tensor without normalizing to \xe2\x80\xa6BoText (chat) messages with images and/or audios.CmAppends a partial assistant response (or any role) to the \xe2\x80\xa6CkReturns a new tensor duplicating data from the original \xe2\x80\xa6CnAdvance the parser by one token. Also checks if the parser \xe2\x80\xa6BfDefault generation parameters from CLICnReturns the amount of time elapsed from another instant to \xe2\x80\xa6CnThis returns parser outputs to be passed back to the user. \xe2\x80\xa6CgCreate a MultiModel directly from a MistralRs instance.F`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa600CfCheck if MCP client is configured for a specific model0CmExplicit JINJA chat template file (.jinja) to be used. If \xe2\x80\xa6AmModel-specific JINJA template0CmIndex of lowest matching regex if any. Lazy regexes match \xe2\x80\xa6BiList available resources from this serverCmGet the logs from the constraint, since last call to this \xe2\x80\xa6CfCheck if constraint is stopped (cannot be extended \xe2\x80\xa6ClMaximum prompt batch size to expect for this model. This \xe2\x80\xa6000000000000000000000CmMaximum prompt number of images to expect for this model. \xe2\x80\xa6000CnNormalize a \xe2\x80\x98relative\xe2\x80\x99 axis value: positive values are \xe2\x80\xa6DhPad the input tensor using 0s along dimension <code>dim</code>. This \xe2\x80\xa6ClTotal context length to allocate the KV cache for (total \xe2\x80\xa60ChNumber of prefix caches to hold on the device. Other \xe2\x80\xa6CjYou can call this first with the prompt from the user, \xe2\x80\xa6BhCompute quantized matrix-matrix product.00AmWhether web search is enabledAdSets PagedAttention.EfEmbeds the values of the <code>src</code> tensor into the <code>self</code> tensor \xe2\x80\xa6ClNumber of Earley items created for the whole token mask. \xe2\x80\xa6BiStore a response object with the given IDDiSimilar to <code>strided_index</code> but returns the position of the \xe2\x80\xa6CeTokenize a given byte sequence. It may or may not \xe2\x80\xa6CnInstead of passing token_lens and token_bytes, this can be \xe2\x80\xa6CjPath to local tokenizer.json file. If specified, it is \xe2\x80\xa6CnPath to local tokenizer.json file. If this is specified it \xe2\x80\xa60000000100000000CkIf true, include parser state (including tokens so far) \xe2\x80\xa6BgSets a base path prefix for all routes.ClForce usage of the CPU device. Do not use PagedAttention \xe2\x80\xa600000BcSets the shared mistral.rs instanceCnModel ID to load X-LoRA from. This may be a HF hub repo or \xe2\x80\xa600000BkApplication of transforms to the Self type.BlA loader for a vision (non-quantized) model.000ClDo not include special tokens, and keep invalid UTF-8 as \xe2\x80\xa6Ch<code>VisionLoader</code> for an LLaVANext Vision model.0AhThe lexer is too complexBiRepresents result from llg_commit_token()CgAll local paths and metadata necessary to load a model.0Cmmax_tokens limit on the number of tokens in the top-level \xe2\x80\xa6BhConfiguration for MCP client integration000BjConfiguration for an individual MCP server000BfSupported MCP server transport sources000ChTop-level parser indicates that no more bytes can be \xe2\x80\xa6CiA callback function that processes streaming response \xe2\x80\xa6CmApplies 2D reflection padding to a tensor of shape (N, C, \xe2\x80\xa60BaThis layer has no parallelizationBoA logprob with the top logprobs for this token.0BaChat completion response message.0oResponse objectAdResponse output itemAdA chat message role.CkThis represents a collection of grammars, with a designatedChWhich escapes to allow (after ). Represents a set of \xe2\x80\xa6CiFused batch matmul + add + Relu/Gelu activation using \xe2\x80\xa6CaChat Completions functionality and route handler.CdOpenAI-compatible chat completions endpoint handler.j(cos, sin)00000AhCreate response endpointCmCreates a SSE streamer for chat completions with optional \xe2\x80\xa60AnDelete a response object by IDAnDelete response by ID endpointChEnable thinking for interactive mode and models that \xe2\x80\xa6CmEnd tracking stats into an ImatrixLayerStats. Returns the \xe2\x80\xa6CbThe data is of different format, depending on tag:BoGenerate audio given a (model specific) prompt.CiEndpoint to generate speech (.wav) for a given prompt \xe2\x80\xa6CmThis is used to generate the OpenAPI docs. The mistral.rs \xe2\x80\xa6CjGet the number of tools available for a specific model \xe2\x80\xa60CmCreate a new matcher from the given ConstraintInit Always \xe2\x80\xa6ChLoad MCP configuration from file path or environment \xe2\x80\xa6CeGenerate pixel mask of shape (c, max_h, max_w). 1 \xe2\x80\xa6ClMatches and processes different types of model responses \xe2\x80\xa6000BjNumber of tools available from MCP serversBlAudio utilities for <code>mistral.rs</code>.CmResize the images to the maximum edge length - preserving \xe2\x80\xa6ClParses and loads an audio file from a URL, file path, or \xe2\x80\xa6ClParses and loads an image from a URL, file path, or data \xe2\x80\xa6A`Parse ISQ value.0CbThe desired audio format for the generated speech.CkMaximum lexer fuel for computation of the whole token mask.CmSimilar to <code>substring_chunks: s.split(&#39;&#39;)</code>ChSimilar to <code>substring_chunks: s.split(/\\s+/)</code>CiWhether tools are available through MCP or tool callbacksBgReturns how many tokens can be applied.CkConfigure MCP client to connect to external MCP servers \xe2\x80\xa6ChConfigure MCP client to connect to external MCP servers.0BbSets the MCP client configuration.CjEnable PagedAttention. Configure PagedAttention with a \xe2\x80\xa600AjCompletion request choice.0CaA validated embedding request constructed via \xe2\x80\xa6AmDefault generation parametersCkConfigure a text GGUF model with the various parameters \xe2\x80\xa6CiWrapper of <code>TextModelBuilder</code> for LoRA models.CkThe MistralRsBuilder takes the pipeline and a scheduler \xe2\x80\xa60BmThe architecture to load the normal model as.0AiThe parser is too complexCbProcess-based MCP transport using stdin/stdout \xe2\x80\xa6AeResponse content itemCjThis layer has a weight that is parallelized along the \xe2\x80\xa6Ca<code>NormalLoader</code> for a Starcoder2 model.0DbTransforms, with each of <code>inner_transforms</code> applied \xe2\x80\xa6CjConfigure a text model with the various parameters for \xe2\x80\xa6BmThe architecture to load the vision model as.0CnModel ID to load LoRA from. This may be a HF hub repo or a \xe2\x80\xa60AjAdd a model configuration.BcAdd multiple pre-tokenized prompts.BmAdd a message with the output of a tool call.BoApply generation parameters to a RequestBuilderCbApplies a unary custom op without backward supportCcApplies a binary custom op without backward supportCdApplies a ternary custom op without backward supportC`Matrix-multiplication with broadcasting support.CeResponse caching functionality for the Responses API.CcGenerate and utilize an imatrix to enhance GGUF \xe2\x80\xa600000CgEnable or disable the case insensitive flag by default.CjReturn any bytes that are forced by the current parser \xe2\x80\xa6CjCompute the appropriate KV shard. This handles KV head \xe2\x80\xa6BoApplies a 1D convolution over the input tensor.CjApplies a 1D transposed convolution over the input tensor.CjApplies a 2D transposed convolution over the input tensor.CiDefault model ID to use when no model is specified in \xe2\x80\xa60AgWeight dtype and deviceCnReturns the indices that would (ascending) sort the tensor \xe2\x80\xa6ClReturns a tensor that is in row major order. This always \xe2\x80\xa6F`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa6B`Amount of total memory in bytes.0BiCheck if the lexeme always matches bytes.CaImage generation functionality and route handler.BbImage generation endpoint handler.ChEnter interactive mode instead of serving a chat server.ChCommit the token sampled with the mask returned from \xe2\x80\xa6CiCompute mask for the next token sampling It typically \xe2\x80\xa6AaFree the matcher.CkThe log level for the buffer that is kept inside of the \xe2\x80\xa6BcThe log level for writing to stderrChMaximum size of the grammar (symbols in productions) \xe2\x80\xa6CmMaximum expected image size will have this edge length on \xe2\x80\xa6000CfFor non-ambiguous grammars, this is the maximum \xe2\x80\x9c\xe2\x80\xa6CbMaximum number of lexer states. Affects memory \xe2\x80\xa6CgThis crate provides vision utilities for mistral.rs \xe2\x80\xa6CmReads a npz file and returns the stored multi-dimensional \xe2\x80\xa6BjRemove DC offset (audio centered around 0)00ClThe lexeme should accept any (possibly empty) contiguous \xe2\x80\xa6Ce\xe2\x80\x9cSubsumption\xe2\x80\x9d is a feature implementing regex \xe2\x80\xa6DdTokenize a string. It will interpret &lt;|special_tokens|&gt; as \xe2\x80\xa6Em<code>true</code> if built with the <code>flash-attn</code> or <code>flash-attn-v3</code> \xe2\x80\xa60ChSet the revision to use for a Hugging Face remote model.00000CjManually set the model loader type. Otherwise, it will \xe2\x80\xa600CeDisable KV cache. Trade performance for memory usage.0BlSets whether to disable the key-value cache.C`Legacy OpenAI compatible text completion requestBiMetadata to initialize the device mapper.0AlA builder for a GGML loader.0AlA builder for a GGUF loader.0B`HTTP-based MCP server connectionBkResize the image via nearest interpolation.CnIn lark syntax, this can be specified as JSON object after \xe2\x80\xa6CkA device mapper which does device mapping per hidden layer.0AmFunction schema for MCP toolsCkConfiguration for a single model in a multi-model setup \xe2\x80\xa6CfUsed to gate access to quantizing onto the host deviceChHelper type for messages field in ResponsesCreateRequestBgResponse streamer for the Responses APIBcMetadata for a speculative pipeline0DaA loader for a speculative pipeline using 2 <code>Loader</code>s.0CkWrapper of <code>TextModelBuilder</code> for X-LoRA models.CnModel ID to load LoRA from. This may be a HF hub repo or a \xe2\x80\xa6000CkReturns the indices that sort the tensor along the last \xe2\x80\xa6BnBegin tracking stats into an ImatrixLayerStatsChCreate a BlockwiseFP8Linear for MoE with 3D weights [\xe2\x80\xa6AlBuild a multi-model instanceChThis will always return [] for non-canonical tokenizers.ClCheck if there are any tokens to fast-forward, forced by \xe2\x80\xa6BmCompute and then consume fast-forward tokens.AbGet adapter paths.0CcGet the LoRA adapters for the current engine threadBnEnable verbose mode in the regular expression.AbClone the matcher.ClReturn a string representation of the tokens, useful for \xe2\x80\xa6CiResets the matcher to the initial state. A matcher in \xe2\x80\xa6CfConstruct a new tokenizer from the given TokenizerInitCmCompute matrix-matrix product. The result will be divided \xe2\x80\xa600000CnNOTE: This can be omitted to use automatic device mapping! \xe2\x80\xa60AlModel-specific device layers0CaPush a LoRA adapter for the current engine threadCmSave the error string to the given pointer. The string is \xe2\x80\xa6AhGenerate with the model.CbSpeech generation functionality and route handler.BcSpeech generation endpoint handler.DjWhen set, the string matching <code>stop_rx</code> will be output as a \xe2\x80\xa6C`Timeout for individual tool execution in seconds000CgDAC Model ID to load from. If not provided, this is \xe2\x80\xa6BkSets the logging configuration if provided.CkSet the maximum number of sequences which can be run at \xe2\x80\xa600000BmAdd a model to the multi-model configuration.CmSource the tokenizer and chat template from this model ID \xe2\x80\xa6BaSource of the Hugging Face token.00000BiSets the token source for authentication.BiAn OpenAI compatible completion response.0AmA streaming response handler.CgController for the CUBLASLT handle and inhibition flag.ocbindgen:ignoreCcRepresents different types of embeddings responses.AiConfig for a GGML loader.0AiConfig for a GGUF loader.0AhRoPE supporting LongRope0AhResponse responder typesCjConfigure a text model with the various parameters for \xe2\x80\xa6ClConfigure a vision model with the various parameters for \xe2\x80\xa6AmWebSocket-based MCP transportClIf set, the grammar will allow the %ignore lexeme at the \xe2\x80\xa6CkIf set, the grammar will allow invalid utf8 byte sequences.CnGenerate the appropriate MIME content type string for this \xe2\x80\xa6AgThe cross-entropy loss.CeCache path for Hugging Face models downloaded locallyCfConvenience wrapper for generating a single embedding.CkGet model category for a specific model. If model_id is \xe2\x80\xa60C`Helper function to get the global cache instanceCnGet tool callbacks that can be used with the existing tool \xe2\x80\xa600ChHow much \xe2\x80\x9cfuel\xe2\x80\x9d are we willing to spend to build \xe2\x80\xa6CjThis should be called to initialize the debug flag and \xe2\x80\xa60DeFree the tokenizer. Should <em>NOT</em> be called while there are \xe2\x80\xa6CmCreate a new constraint from a grammar JSON string Always \xe2\x80\xa6CjTokenize the given bytes and return the tokens. Always \xe2\x80\xa6FeIf <code>revision</code> is None, then it defaults to <code>main</code>. If <code>dtype</code> is \xe2\x80\xa60CnGPU memory to allocate for KV cache with PagedAttention in \xe2\x80\xa60CcIf a quantized method, return the activation dtype.CmQuantized filename(s). May be a single filename, or use a \xe2\x80\xa600AcQuantized filename.00111000DhQuantized model ID to find the <code>quantized_filename</code>. This \xe2\x80\xa600000000000CkDefault repetition penalty (1.0 = no penalty). Default: 1.1BmOverride the description for the search tool.0CmThe storage used by this tensor, together with the layout \xe2\x80\xa6CkGeneric helper to stream tokens and forward them to the \xe2\x80\xa6BdUser data to pass to the tokenize_fnCnTries to advance the parser by consuming the given tokens. \xe2\x80\xa6BeAlias for <code>interpolate1d</code>.BeAlias for <code>interpolate2d</code>.DiLiteral Jinja chat template OR Path (ending in <code>.json</code>) to \xe2\x80\xa600BeSets the chat template configuration.C`Sets whether to enable web search functionality.BeSets the in-situ quantization method.CeAdd multiple models to the multi-model configuration.BaSets the random seed if provided.BmRegister a callback for a specific tool name.CgRegister a custom callback for the specified tool name.0CeAudio format options for speech generation responses.CjThis layer has a weight that is parallelized along the \xe2\x80\xa6CcRepresents different types of completion responses.BmThe architecture to load the vision model as.0BoEngine instructions, per Engine (MistralRs) ID.0C`The architecture to load the embedding model as.0CeAll local paths and metadata necessary to load an \xe2\x80\xa60B`Trait for MCP server connectionsCmInner content structure for messages that can be either a \xe2\x80\xa6CmA builder for a loader for a \xe2\x80\x9cnormal\xe2\x80\x9d (non-quantized) \xe2\x80\xa60AcResponse annotationBbSpeculative decoding pipeline: \xe2\x80\xa60BoRequest to tokenize some messages or some text.0CjA builder for a loader for a vision (non-quantized) model.0CaAppend a chat message to the specified chat file.CkWhether to automatically register discovered tools with \xe2\x80\xa6000DkReturn <code>BF16</code> for devices that support it, otherwise default \xe2\x80\xa6CeClear all LoRA adapters for the current engine threadCnCompute which tokens can be consumed in the current state. \xe2\x80\xa6CkCompute the number of KV groups, taking into account KV \xe2\x80\xa6CaOverride the description for the extraction tool.0AdFP8 vector quantize.CeCache path for Hugging Face models downloaded locally0CmGenerate embeddings for one or more inputs configured via \xe2\x80\xa6C`Retrieve the <code>PretrainedConfig</code> file.0ClClone a tokenizer. This increments a reference count and \xe2\x80\xa6AcFree the constraintChGet the current temperature of the constraint. It is \xe2\x80\xa6CjReturns the maximum supported sequence length for this \xe2\x80\xa6CiReturns the maximum supported sequence length for the \xe2\x80\xa6CmGet the maximum supported sequence length for a model, if \xe2\x80\xa60CjParse a special token of the form \\xFF [ 1 2 3 4 ] The \xe2\x80\xa6AnNOT meant for external callingAhGenerate with the model.ClValidate that a file path is safe and within the uploads \xe2\x80\xa6BlValidate MCP configuration for common issuesCiValidates that the requested model matches one of the \xe2\x80\xa6ClReturns how many of the passed tokens can be accepted by \xe2\x80\xa6CaProvide metadata to initialize the device mapper.000CmExplicit JINJA chat template file (.jinja) to be used. If \xe2\x80\xa600BjSets an explicit JINJA chat template file.BiSets the axum default request body limit.CfSets the total context length for KV cache allocation.ClSet the number of sequences to hold in the prefix cache. \xe2\x80\xa600CgSets the number of prefix caches to hold on the device.CdPath to a discrete <code>tokenizer.json</code> file.000CiRequest to append a (partial) assistant message to a chatCiWrapper of <code>GgufModelBuilder</code> for LoRA models.AlReference to an MCP resourceBjConfig specific to loading a normal model.0CfAll memory counts in MB. Default for block size is 32.0BcProcess-based MCP server connectionAjResponse delta output itemCgEmbedding model used for ranking web search results \xe2\x80\xa60BnThis is the underlying instance of mistral.rs.CdA tool callback with its associated Tool definition.00CkConfigure a UQFF text model with the various parameters \xe2\x80\xa6BjConfig specific to loading a vision model.0BfApply Rotary position encoding inplaceCnEnable or disable the \xe2\x80\x9cdot matches any character\xe2\x80\x9d flag \xe2\x80\xa6AiGet the default model ID.B`Get the current default model ID0BdAmount of available memory in bytes.0CmGet the processor config (for the vision models). This is \xe2\x80\xa60BoModel weights files (multiple files supported).0CcCheck if add_bias() would have returned any tokens.AdClone the constraintBjReturn pointer to the mask computed by \xe2\x80\xa6BjCheck if the matcher is in an error state.CmBacktracks the matcher states by num_tokens. Returns 0 on \xe2\x80\xa6CaCompute mask for several constraints in parallel.ClReturn a string representation of the tokens, useful for \xe2\x80\xa6CmCheck if given grammar is valid. This about twice as fast \xe2\x80\xa6CkLoad a model from the specified paths. Also initializes \xe2\x80\xa60BoName of the Matryoshka Transformer slice to use00000CjMaximum number of concurrent tool calls across all MCP \xe2\x80\xa6000Ck<code>true</code> if built with CUDA (requires Unix) /Metal0AiSet the default model ID.AhSet the default model ID0AnSets the CORS allowed origins.DjOverride the search function used when <code>web_search_options</code> \xe2\x80\xa600BoUse a custom callback to gather search results.01BkApplication of transforms to the Self type.CjChat completion request following OpenAI\xe2\x80\x99s specificationBgChat completion streaming chunk choice.0AnCustomizable logits processor.0B`Request to detokenize some text.0CjConfigure a text model with the various parameters for \xe2\x80\xa6ClConfigure an embedding model with the various parameters \xe2\x80\xa6AnAn individual embedding input.BdRequest to generate speech from textCkWrapper of <code>GgufModelBuilder</code> for X-LoRA models.BiIn-memory implementation of ResponseCacheBaDefines what is allowed in BranchCeReplace invalid UTF-8 with the replacement character.oRoPE for Llama30AgMCP server capabilitiesBdReal (for Metal) and Fake (for CUDA)AkResponse delta content itemCdConditional (and unconditional) splices are allowed.AfFP8 vector dequantize.BiFilepath for general model configuration.0ClGiven the image sizes (h, w) and the minimum and maximum \xe2\x80\xa6ChFile where the content is expected to deserialize to \xe2\x80\xa60CnReturns true if the data is stored in a Fortran contiguous \xe2\x80\xa6CnGet the error message from the matcher or null if there is \xe2\x80\xa6CmCommit a token to the stop-sequence controller. Returns a \xe2\x80\xa6CkPath to local Matryoshka Transformer configuration CSV file00000AoNumber of connected MCP serversBg<strong>mistral.rs server core</strong>BoBlock size (number of tokens per block) for \xe2\x80\xa60CjSame as <code>run_attention</code>, but no flash attention0CnGenerate with the model, returning raw logits of the first \xe2\x80\xa6CjTokenize a given byte sequence. It will interpret text \xe2\x80\xa6CgIf this returns true, this tokenizer always returns \xe2\x80\xa6DlTokenize a string, interpreting <code>&lt;name&gt;</code> as special tokens.ChUtilise this calibration file to collcet an imatrix. \xe2\x80\xa6BmUtilise this calibration_file file during ISQCmSet the default model ID to use when none is specified in \xe2\x80\xa6BhSets whether to run in interactive mode.BnAn OpenAI compatible chat completion response.0AmA streaming response handler.CgThis is the axum default request body limit for the \xe2\x80\xa6CmThe scheduler method controld how sequences are scheduled \xe2\x80\xa60CjA builder for a loader for a vision (non-quantized) model.000AhImage generation requestAiResponse creation requestA`RoPE for SmolLm30CmCollection of callbacks with their tool definitions keyed \xe2\x80\xa6CiOffset for the quant type. UQFF always serializes the \xe2\x80\xa6CkConfigure a UQFF text model with the various parameters \xe2\x80\xa6BeWebSocket-based MCP server connectionCfBy convention, all images are added before all audios.EgSame as <code>avg_pool2d</code> but with a <code>stride</code> that can be set to a \xe2\x80\xa6CnReturns the amount of time elapsed from another instant to \xe2\x80\xa6AgFP8 blockwise quantize.DbA serialised <code>tokenizers.Tokenizer</code> HuggingFace object.0CkCheck if the matcher will force EOS token. This returns \xe2\x80\xa6CkCreate a new constraint with specified type Type can be \xe2\x80\xa6EgSame as <code>max_pool2d</code> but with a <code>stride</code> that can be set to a \xe2\x80\xa6CmSanitize error messages to remove internal implementation \xe2\x80\xa6ChSelect which built-in search embedding model to load \xe2\x80\xa6CbBuilt-in search embedding model to load (e.g., \xe2\x80\xa6CmIndex of completion tokens to generate scalings up until. \xe2\x80\xa600000C`Tokenize a byte slice. It will interpret &lt;\xe2\x80\xa6AmSets the device layer mappingCnTruncate prompts that exceed the model\xe2\x80\x99s maximum context \xe2\x80\xa6CmControl whether prompts longer than the model context are \xe2\x80\xa6ChRepresents different types of chat completion responses.AjCompletion request choice.0BkBuilder for configuring embedding requests.BjConfig specific to loading a vision model.0DiThis is the <code>SharedMistralRsState</code> that has been extracted \xe2\x80\xa6CkGeneration parameters that can be sent per-message from \xe2\x80\xa6AiSpeech generation requestCkTerminate all sequences on the next scheduling step. Be \xe2\x80\xa60BmCreates a channel for response communication.F`Compute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the \xe2\x80\xa6CeGets the keep-alive interval for SSE streams from \xe2\x80\xa6CmGet the preprocessor config (for the vision models). This \xe2\x80\xa60CnHandles sampling commands (\\temperature, \\topk, \\topp) and \xe2\x80\xa6ChIf this returns true, then the regex will match only \xe2\x80\xa6CkCreate a new constraint from a given JSON schema Always \xe2\x80\xa6ClCreate a new constraint from a given lark grammar Always \xe2\x80\xa6BeCreate a new stop-sequence controllerBhLoad multi-model configuration from fileCdSet to true to enable hack that works around the \xe2\x80\xa6CkSets the GPU memory allocation for PagedAttention KV cache.BaEnable runner throughput logging.000CkA callback function that is executed when the streaming \xe2\x80\xa6CiRepresents different types of image generation responses.BdJSON Schema for structured responsesCiPrepend a vision tag appropriate for the model to the \xe2\x80\xa60AiFP8 blockwise dequantize.BlRetrieve conversation history for a responseEdCheck if <code>small</code> is contained in <code>big</code> with a limit on the \xe2\x80\xa6BaFree the stop-sequence controllerClCompute the set of allowed tokens for the current state. \xe2\x80\xa6C`Check if the grammar can fully accept the input.CkCreate a new constraint from a given regular expression \xe2\x80\xa6CnPercentage of GPU memory to utilize after allocation of KV \xe2\x80\xa60ClIf true, we\xe2\x80\x99ll run any extremely large regexes against \xe2\x80\xa6BnSets the MCP client configuration if provided.CiA callback function that processes streaming response \xe2\x80\xa6CeA builder for creating a mistral.rs instance with \xe2\x80\xa6BdBuilder for PagedAttention metadata.CjRepresents different types of speech generation responses.CeConfigure a UQFF embedding model with the various \xe2\x80\xa6CkGet or create a termination flag for the current engine \xe2\x80\xa60CmClone the stop-sequence controller. The cloned controller \xe2\x80\xa6CnAdvance the matcher by one token. Returns 0 on success and \xe2\x80\xa6CkTokenize the given bytes and return the tokens. Special \xe2\x80\xa6CnReturns the amount of time elapsed from another instant to \xe2\x80\xa6CmSet the sampling parameters for deterministic generation. \xe2\x80\xa6ClName of the slice to use from the Matryoshka Transformer \xe2\x80\xa60CnOrganize ISQ to enable MoQE (Mixture of Quantized Experts, \xe2\x80\xa6D`Include special tokens in the output. They may look like &lt;\xe2\x80\xa6BkIncomplete details for incomplete responsesAoGet the explicit chat template.0CnCreates a search embedding model configuration for agentic \xe2\x80\xa6CkStatic LoRA in the style of Phi-4 multimodal. Only when \xe2\x80\xa6CgAdvance the matcher by several tokens. Returns 0 on \xe2\x80\xa6BhSend a chat request to a specific model.BiStore conversation history for a responseChPath to a Matryoshka Transformer configuration CSV file.0BgSets the block size for PagedAttention.0BhChat completion streaming request chunk.0ChDefault buffer size for the response channel used in \xe2\x80\xa6AdInput tokens detailsChCheck how many tokens can be consumed from the given \xe2\x80\xa6BoReset termination flags for the current engine.0CaSets the chat template configuration if provided.CaSets the in-situ quantization method if provided.CaConfigures whether to include OpenAPI doc routes.ChSets the embedding model used for web search assistance.CjRegister a callback with an associated Tool definition \xe2\x80\xa600CgRegister a custom callback with its associated Tool \xe2\x80\xa60CkA callback function that is executed when the streaming \xe2\x80\xa6CjA builder for creating a mistral.rs server router with \xe2\x80\xa6AeOutput tokens detailsBgmistral.rs instance for server builder.CfSets an explicit JINJA chat template file if provided.ClSets the total context length for KV cache allocation if \xe2\x80\xa6CiA callback function that processes streaming response \xe2\x80\xa6B`Image generation response format0CiGet tool callbacks with their associated Tool definitions00ClCompute the fast-forward (forced) tokens for the current \xe2\x80\xa6ClCompute the set of allowed tokens for the current state. \xe2\x80\xa6CdSets the percentage of GPU memory to utilize for \xe2\x80\xa6ClDefault keep-alive interval for Server-Sent Events (SSE) \xe2\x80\xa6BeReturn the size of the mask in bytes.C`Process non-streaming chat completion responses.BkProcess non-streaming completion responses.CaProcess non-streaming image generation responses.CbProcess non-streaming speech generation responses.BmConfigures PagedAttention based on two flags.Bamistral.rs server router builder.CkParse HF tokenizer.json file and return bytes for every \xe2\x80\xa6BjSets the device layer mapping if provided.CjSet the default values for the ConstraintInit Disables \xe2\x80\xa6B`Send initialization notificationClSends the server a initialization notification to let it \xe2\x80\xa600CnSets the GPU memory allocation for PagedAttention KV cache \xe2\x80\xa6CgCheck if the current engine should terminate sequences.0CkSet to true to not use tokenize_fn and instead tokenize \xe2\x80\xa6CcSets the block size for PagedAttention if provided.CdSets the percentage of GPU memory to utilize for \xe2\x80\xa6")