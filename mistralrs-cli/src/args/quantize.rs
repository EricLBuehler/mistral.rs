//! Quantize command argument structs for UQFF generation

use clap::{Args, Subcommand};
use mistralrs_core::{AutoDeviceMapParams, IsqOrganization, ModelDType, NormalLoaderType};
use std::path::PathBuf;

/// Quantize model type selection (base models only, no adapter support)
#[derive(Subcommand, Clone)]
pub enum QuantizeModelType {
    /// Auto-detect model type (recommended)
    Auto {
        #[command(flatten)]
        model: QuantizeModelSourceOptions,

        #[command(flatten)]
        quantization: QuantizeQuantizationOptions,

        #[command(flatten)]
        device: QuantizeDeviceOptions,

        #[command(flatten)]
        output: QuantizeOutputOptions,

        #[command(flatten)]
        vision: QuantizeVisionOptions,
    },

    /// Text generation model with explicit architecture
    Text {
        #[command(flatten)]
        model: QuantizeModelSourceOptions,

        /// Model architecture (required for text models)
        #[arg(short = 'a', long, value_parser = parse_arch)]
        arch: Option<NormalLoaderType>,

        #[command(flatten)]
        quantization: QuantizeQuantizationOptions,

        #[command(flatten)]
        device: QuantizeDeviceOptions,

        #[command(flatten)]
        output: QuantizeOutputOptions,
    },

    /// Vision-language model
    Vision {
        #[command(flatten)]
        model: QuantizeModelSourceOptions,

        #[command(flatten)]
        quantization: QuantizeQuantizationOptions,

        #[command(flatten)]
        device: QuantizeDeviceOptions,

        #[command(flatten)]
        output: QuantizeOutputOptions,

        #[command(flatten)]
        vision: QuantizeVisionOptions,
    },

    /// Embedding model
    Embedding {
        #[command(flatten)]
        model: QuantizeModelSourceOptions,

        #[command(flatten)]
        quantization: QuantizeQuantizationOptions,

        #[command(flatten)]
        device: QuantizeDeviceOptions,

        #[command(flatten)]
        output: QuantizeOutputOptions,
    },
}

/// Model source options for quantization
#[derive(Args, Clone)]
pub struct QuantizeModelSourceOptions {
    /// Model ID to load (HuggingFace repo or local path)
    #[arg(short = 'm', long)]
    pub model_id: String,

    /// Path to local tokenizer.json file
    #[arg(short = 't', long)]
    pub tokenizer: Option<PathBuf>,

    /// Model data type
    #[arg(long, default_value = "auto", value_parser = parse_dtype)]
    pub dtype: ModelDType,
}

/// Quantization options for UQFF generation (ISQ-related only, no from_uqff)
#[derive(Args, Clone)]
pub struct QuantizeQuantizationOptions {
    /// In-situ quantization level(s). Multiple values can be comma-separated or specified
    /// via repeated --isq flags (e.g., "--isq q4k,q8_0" or "--isq q4k --isq q8_0").
    #[arg(long = "isq", required = true, value_delimiter = ',')]
    pub in_situ_quant: Vec<String>,

    /// ISQ organization strategy: default or moqe
    #[arg(long)]
    pub isq_organization: Option<IsqOrganization>,

    /// imatrix file for enhanced quantization
    #[arg(long)]
    pub imatrix: Option<PathBuf>,

    /// Calibration file for imatrix generation
    #[arg(long, conflicts_with = "imatrix")]
    pub calibration_file: Option<PathBuf>,
}

/// Device options for quantization
#[derive(Args, Clone)]
pub struct QuantizeDeviceOptions {
    /// Force CPU-only execution
    #[arg(long)]
    pub cpu: bool,

    /// Device layer mapping (format: ORD:NUM;... e.g., "0:10;1:20")
    #[arg(short = 'n', long, value_delimiter = ';')]
    pub device_layers: Option<Vec<String>>,

    /// Topology YAML file for device mapping
    #[arg(long)]
    pub topology: Option<PathBuf>,

    /// Custom HuggingFace cache directory
    #[arg(long)]
    pub hf_cache: Option<PathBuf>,

    /// Max sequence length for automatic device mapping
    #[arg(long, default_value_t = AutoDeviceMapParams::DEFAULT_MAX_SEQ_LEN)]
    pub max_seq_len: usize,

    /// Max batch size for automatic device mapping
    #[arg(long, default_value_t = AutoDeviceMapParams::DEFAULT_MAX_BATCH_SIZE)]
    pub max_batch_size: usize,
}

/// Output options for UQFF generation
#[derive(Args, Clone)]
pub struct QuantizeOutputOptions {
    /// Output path: a `.uqff` file path (single ISQ) or a directory (auto-names files per ISQ type).
    /// Examples: `-o model/model-q4k.uqff` or `-o output/`
    #[arg(short = 'o', long = "output", required = true)]
    pub output_path: PathBuf,

    /// Skip README.md model card generation (generated by default in directory mode)
    #[arg(long)]
    pub no_readme: bool,
}

/// Vision model options for quantization
#[derive(Args, Clone, Default)]
pub struct QuantizeVisionOptions {
    /// Maximum edge length for image resizing (aspect ratio preserved)
    #[arg(long)]
    pub max_edge: Option<u32>,

    /// Maximum number of images per request
    #[arg(long)]
    pub max_num_images: Option<usize>,

    /// Maximum image dimension for device mapping
    #[arg(long)]
    pub max_image_length: Option<usize>,
}

/// Default options for quantize command when no model type subcommand is specified.
/// These mirror the Auto variant's options and are used to construct QuantizeModelType::Auto.
#[derive(clap::Args, Clone)]
pub struct QuantizeDefaultOptions {
    /// HuggingFace model ID or local path to model directory
    #[arg(short = 'm', long)]
    pub model_id: Option<String>,

    /// Path to local tokenizer.json file
    #[arg(short = 't', long)]
    pub tokenizer: Option<PathBuf>,

    /// Model data type
    #[arg(long, default_value = "auto", value_parser = parse_dtype)]
    pub dtype: ModelDType,

    /// In-situ quantization level(s). Multiple values can be comma-separated or specified
    /// via repeated --isq flags (e.g., "--isq q4k,q8_0" or "--isq q4k --isq q8_0").
    #[arg(long = "isq", value_delimiter = ',')]
    pub in_situ_quant: Vec<String>,

    /// ISQ organization strategy: default or moqe
    #[arg(long)]
    pub isq_organization: Option<IsqOrganization>,

    /// imatrix file for enhanced quantization
    #[arg(long)]
    pub imatrix: Option<PathBuf>,

    /// Calibration file for imatrix generation
    #[arg(long, conflicts_with = "imatrix")]
    pub calibration_file: Option<PathBuf>,

    /// Force CPU-only execution
    #[arg(long)]
    pub cpu: bool,

    /// Device layer mapping (format: ORD:NUM;... e.g., "0:10;1:20")
    #[arg(short = 'n', long, value_delimiter = ';')]
    pub device_layers: Option<Vec<String>>,

    /// Topology YAML file for device mapping
    #[arg(long)]
    pub topology: Option<PathBuf>,

    /// Custom HuggingFace cache directory
    #[arg(long)]
    pub hf_cache: Option<PathBuf>,

    /// Max sequence length for automatic device mapping
    #[arg(long, default_value_t = AutoDeviceMapParams::DEFAULT_MAX_SEQ_LEN)]
    pub max_seq_len: usize,

    /// Max batch size for automatic device mapping
    #[arg(long, default_value_t = AutoDeviceMapParams::DEFAULT_MAX_BATCH_SIZE)]
    pub max_batch_size: usize,

    /// Output path: a `.uqff` file path (single ISQ) or a directory (auto-names files per ISQ type).
    #[arg(short = 'o', long = "output")]
    pub output_path: Option<PathBuf>,

    /// Skip README.md model card generation (generated by default in directory mode)
    #[arg(long)]
    pub no_readme: bool,

    /// Maximum edge length for image resizing (aspect ratio preserved)
    #[arg(long)]
    pub max_edge: Option<u32>,

    /// Maximum number of images per request
    #[arg(long)]
    pub max_num_images: Option<usize>,

    /// Maximum image dimension for device mapping
    #[arg(long)]
    pub max_image_length: Option<usize>,
}

impl QuantizeDefaultOptions {
    /// Convert default options into a QuantizeModelType::Auto variant.
    /// Returns an error if required fields are missing.
    pub fn into_quantize_model_type(self) -> anyhow::Result<QuantizeModelType> {
        let model_id = self
            .model_id
            .ok_or_else(|| anyhow::anyhow!("--model-id (-m) is required"))?;
        if self.in_situ_quant.is_empty() {
            return Err(anyhow::anyhow!("--isq is required"));
        }
        let output_path = self
            .output_path
            .ok_or_else(|| anyhow::anyhow!("--output (-o) is required"))?;

        Ok(QuantizeModelType::Auto {
            model: QuantizeModelSourceOptions {
                model_id,
                tokenizer: self.tokenizer,
                dtype: self.dtype,
            },
            quantization: QuantizeQuantizationOptions {
                in_situ_quant: self.in_situ_quant,
                isq_organization: self.isq_organization,
                imatrix: self.imatrix,
                calibration_file: self.calibration_file,
            },
            device: QuantizeDeviceOptions {
                cpu: self.cpu,
                device_layers: self.device_layers,
                topology: self.topology,
                hf_cache: self.hf_cache,
                max_seq_len: self.max_seq_len,
                max_batch_size: self.max_batch_size,
            },
            output: QuantizeOutputOptions {
                output_path,
                no_readme: self.no_readme,
            },
            vision: QuantizeVisionOptions {
                max_edge: self.max_edge,
                max_num_images: self.max_num_images,
                max_image_length: self.max_image_length,
            },
        })
    }
}

/// Get the effective QuantizeModelType, using default options if no subcommand was provided.
/// Returns an error if no subcommand is provided and required fields are missing.
pub fn resolve_quantize_model_type(
    model_type: Option<QuantizeModelType>,
    default_options: QuantizeDefaultOptions,
) -> anyhow::Result<QuantizeModelType> {
    match model_type {
        Some(mt) => Ok(mt),
        None => default_options.into_quantize_model_type(),
    }
}

fn parse_arch(s: &str) -> Result<NormalLoaderType, String> {
    s.parse()
}

fn parse_dtype(s: &str) -> Result<ModelDType, String> {
    s.parse()
}
