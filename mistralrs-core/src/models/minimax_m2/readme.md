# attention

minimax uses interleaved attention, 
layer:
0: full attention
1: lightning / linear
2: full attention
3: lightning / linear