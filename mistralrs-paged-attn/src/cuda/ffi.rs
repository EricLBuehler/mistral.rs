use core::ffi::{c_int, c_long, c_uint, c_void};

use candle_core::cuda::cudarc::driver::sys::CUstream;

extern "C" {
    pub fn reshape_and_cache(
        key: *const c_void,
        value: *const c_void,
        key_cache: *const c_void,
        value_cache: *const c_void,
        slot_mapping: *const c_long,

        num_tokens: c_int,
        num_heads: c_int,
        head_size: c_int,
        block_size: c_int,
        x: c_int,
        key_stride: c_int,
        value_stride: c_int,
        stream: CUstream,

        dtype: u32,
        cache_dtype: u32,
        k_scale: *const f32,
        v_scale: *const f32,
    );

    pub fn concat_and_cache_mla(
        ckv: *const c_void,
        k_pe: *const c_void,
        ckv_cache: *const c_void,
        kpe_cache: *const c_void,
        slot_mapping: *const c_long,
        num_tokens: c_int,
        kv_lora_rank: c_int,
        kpe_head_dim: c_int,
        block_size: c_int,
        ckv_stride: c_int,
        kpe_stride: c_int,
        stream: CUstream,
        dtype: u32,
    );

    pub fn flashinfer_mla_decode(
        q_nope: *const c_void,
        q_pe: *const c_void,
        ckv_cache: *const c_void,
        kpe_cache: *const c_void,
        kv_indptr: *const c_int,
        kv_indices: *const c_int,
        kv_last_page_len: *const c_int,
        out: *const c_void,
        batch_size: c_int,
        num_qo_heads: c_int,
        page_size: c_int,
        sm_scale: f32,
        window_left: c_int,
        logits_soft_cap: f32,
        rope_scale: f32,
        rope_theta: f32,
        request_indices: *const c_int,
        kv_tile_indices: *const c_int,
        o_indptr: *const c_int,
        kv_chunk_size_ptr: *const c_int,
        dtype: u32,
        stream: CUstream,
    );

    pub fn gather_mla_cache(
        ckv_cache: *const c_void,
        kpe_cache: *const c_void,
        ckv_out: *const c_void,
        kpe_out: *const c_void,
        block_table: *const c_int,
        cu_seq_lens: *const c_int,
        token_to_seq: *const c_int,
        num_tokens: c_int,
        block_size: c_int,
        block_table_stride: c_int,
        kv_lora_rank: c_int,
        kpe_head_dim: c_int,
        stream: CUstream,
        dtype: u32,
    );

    pub fn gather_kv_cache(
        key_cache: *const c_void,
        value_cache: *const c_void,
        k_out: *const c_void,
        v_out: *const c_void,
        k_scale: *const f32,
        v_scale: *const f32,
        block_table: *const c_int,
        cu_seq_lens: *const c_int,
        num_tokens: c_int,
        num_seqs: c_int,
        block_size: c_int,
        block_table_stride: c_int,
        num_kv_heads: c_int,
        head_size: c_int,
        x: c_int,
        stream: CUstream,
        out_dtype: u32,
        cache_dtype: u32,
    );

    pub fn paged_attention_v1_f16(
        out: *const c_void,
        query: *const c_void,
        key_cache: *const c_void,
        value_cache: *const c_void,
        alibi_slopes: *const c_void,
        num_kv_heads: c_int,
        scale: f32,
        softcapping: f32,
        block_tables: *const c_int,
        context_lens: *const c_int,
        block_size: c_int,
        max_context_len: c_int,
        num_seqs: c_int,
        num_heads: c_int,
        head_size: c_int,
        max_num_blocks_per_seq: c_int,
        q_stride: c_int,
        kv_block_stride: c_int,
        kv_head_stride: c_int,
        stream: CUstream,
        cache_dtype: u32,
        k_scale: *const f32,
        v_scale: *const f32,
        sinks: *const f32,
    );

    pub fn paged_attention_v1_bf16(
        out: *const c_void,
        query: *const c_void,
        key_cache: *const c_void,
        value_cache: *const c_void,
        alibi_slopes: *const c_void,
        num_kv_heads: c_int,
        scale: f32,
        softcapping: f32,
        block_tables: *const c_int,
        context_lens: *const c_int,
        block_size: c_int,
        max_context_len: c_int,
        num_seqs: c_int,
        num_heads: c_int,
        head_size: c_int,
        max_num_blocks_per_seq: c_int,
        q_stride: c_int,
        kv_block_stride: c_int,
        kv_head_stride: c_int,
        stream: CUstream,
        cache_dtype: u32,
        k_scale: *const f32,
        v_scale: *const f32,
        sinks: *const f32,
    );

    pub fn paged_attention_v1_f32(
        out: *const c_void,
        query: *const c_void,
        key_cache: *const c_void,
        value_cache: *const c_void,
        alibi_slopes: *const c_void,
        num_kv_heads: c_int,
        scale: f32,
        softcapping: f32,
        block_tables: *const c_int,
        context_lens: *const c_int,
        block_size: c_int,
        max_context_len: c_int,
        num_seqs: c_int,
        num_heads: c_int,
        head_size: c_int,
        max_num_blocks_per_seq: c_int,
        q_stride: c_int,
        kv_block_stride: c_int,
        kv_head_stride: c_int,
        stream: CUstream,
        cache_dtype: u32,
        k_scale: *const f32,
        v_scale: *const f32,
        sinks: *const f32,
    );

    pub fn paged_attention_v2_f16(
        out: *const c_void,
        exp_sums: *const f32,
        max_logits: *const f32,
        tmp_out: *const c_void,
        query: *const c_void,
        key_cache: *const c_void,
        value_cache: *const c_void,
        alibi_slopes: *const c_void,
        num_kv_heads: c_int,
        scale: f32,
        softcapping: f32,
        block_tables: *const c_int,
        context_lens: *const c_int,
        block_size: c_int,
        max_context_len: c_int,
        num_seqs: c_int,
        num_heads: c_int,
        head_size: c_int,
        max_num_blocks_per_seq: c_int,
        q_stride: c_int,
        kv_block_stride: c_int,
        kv_head_stride: c_int,
        stream: CUstream,
        cache_dtype: u32,
        k_scale: *const f32,
        v_scale: *const f32,
        sinks: *const f32,
    );

    pub fn paged_attention_v2_bf16(
        out: *const c_void,
        exp_sums: *const f32,
        max_logits: *const f32,
        tmp_out: *const c_void,
        query: *const c_void,
        key_cache: *const c_void,
        value_cache: *const c_void,
        alibi_slopes: *const c_void,
        num_kv_heads: c_int,
        scale: f32,
        softcapping: f32,
        block_tables: *const c_int,
        context_lens: *const c_int,
        block_size: c_int,
        max_context_len: c_int,
        num_seqs: c_int,
        num_heads: c_int,
        head_size: c_int,
        max_num_blocks_per_seq: c_int,
        q_stride: c_int,
        kv_block_stride: c_int,
        kv_head_stride: c_int,
        stream: CUstream,
        cache_dtype: u32,
        k_scale: *const f32,
        v_scale: *const f32,
        sinks: *const f32,
    );

    pub fn paged_attention_v2_f32(
        out: *const c_void,
        exp_sums: *const f32,
        max_logits: *const f32,
        tmp_out: *const c_void,
        query: *const c_void,
        key_cache: *const c_void,
        value_cache: *const c_void,
        alibi_slopes: *const c_void,
        num_kv_heads: c_int,
        scale: f32,
        softcapping: f32,
        block_tables: *const c_int,
        context_lens: *const c_int,
        block_size: c_int,
        max_context_len: c_int,
        num_seqs: c_int,
        num_heads: c_int,
        head_size: c_int,
        max_num_blocks_per_seq: c_int,
        q_stride: c_int,
        kv_block_stride: c_int,
        kv_head_stride: c_int,
        stream: CUstream,
        cache_dtype: u32,
        k_scale: *const f32,
        v_scale: *const f32,
        sinks: *const f32,
    );

    pub fn copy_blocks_bf16(
        key_cache_ptrs: *mut c_void,
        value_cache_ptrs: *mut c_void,
        block_mapping: *const c_void,
        num_layers: i32,
        num_pairs: i32,
        numel_per_block_key: i32,
        numel_per_block_value: i32,
        stream: i64,
    );

    pub fn copy_blocks_f16(
        key_cache_ptrs: *mut c_void,
        value_cache_ptrs: *mut c_void,
        block_mapping: *const c_void,
        num_layers: i32,
        num_pairs: i32,
        numel_per_block_key: i32,
        numel_per_block_value: i32,
        stream: i64,
    );

    pub fn copy_blocks_f32(
        key_cache_ptrs: *mut c_void,
        value_cache_ptrs: *mut c_void,
        block_mapping: *const c_void,
        num_layers: i32,
        num_pairs: i32,
        numel_per_block_key: i32,
        numel_per_block_value: i32,
        stream: i64,
    );

    pub fn update_kv_scales_f32(
        k: *const c_void,
        v: *const c_void,
        elements: c_long,
        k_scales: *const f32,
        v_scales: *const f32,
        stream: i64,
    );

    pub fn update_kv_scales_f16(
        k: *const c_void,
        v: *const c_void,
        elements: c_long,
        k_scales: *const f32,
        v_scales: *const f32,
        stream: i64,
    );

    pub fn update_kv_scales_bf16(
        k: *const c_void,
        v: *const c_void,
        elements: c_long,
        k_scales: *const f32,
        v_scales: *const f32,
        stream: i64,
    );

    pub fn flash_attn_sinks_f16(
        q: *const c_void,
        k: *const c_void,
        v: *const c_void,
        out: *mut c_void,
        sinks: *const f32,
        scale: f32,
        batch_size: c_int,
        q_len: c_int,
        kv_len: c_int,
        num_heads: c_int,
        num_kv_heads: c_int,
        head_dim: c_int,
        window_size: c_int,
        stream: CUstream,
    );

    pub fn flash_attn_sinks_bf16(
        q: *const c_void,
        k: *const c_void,
        v: *const c_void,
        out: *mut c_void,
        sinks: *const f32,
        scale: f32,
        batch_size: c_int,
        q_len: c_int,
        kv_len: c_int,
        num_heads: c_int,
        num_kv_heads: c_int,
        head_dim: c_int,
        window_size: c_int,
        stream: CUstream,
    );

    pub fn flash_attn_sinks_f32(
        q: *const c_void,
        k: *const c_void,
        v: *const c_void,
        out: *mut c_void,
        sinks: *const f32,
        scale: f32,
        batch_size: c_int,
        q_len: c_int,
        kv_len: c_int,
        num_heads: c_int,
        num_kv_heads: c_int,
        head_dim: c_int,
        window_size: c_int,
        stream: CUstream,
    );

    pub fn flash_attn_sinks_varlen_f16(
        q: *const c_void,
        k: *const c_void,
        v: *const c_void,
        out: *mut c_void,
        sinks: *const f32,
        cu_seqlens_q: *const c_uint,
        cu_seqlens_k: *const c_uint,
        scale: f32,
        batch_size: c_int,
        max_q_len: c_int,
        num_heads: c_int,
        num_kv_heads: c_int,
        head_dim: c_int,
        window_size: c_int,
        stream: CUstream,
    );

    pub fn flash_attn_sinks_varlen_bf16(
        q: *const c_void,
        k: *const c_void,
        v: *const c_void,
        out: *mut c_void,
        sinks: *const f32,
        cu_seqlens_q: *const c_uint,
        cu_seqlens_k: *const c_uint,
        scale: f32,
        batch_size: c_int,
        max_q_len: c_int,
        num_heads: c_int,
        num_kv_heads: c_int,
        head_dim: c_int,
        window_size: c_int,
        stream: CUstream,
    );

    pub fn flash_attn_sinks_varlen_f32(
        q: *const c_void,
        k: *const c_void,
        v: *const c_void,
        out: *mut c_void,
        sinks: *const f32,
        cu_seqlens_q: *const c_uint,
        cu_seqlens_k: *const c_uint,
        scale: f32,
        batch_size: c_int,
        max_q_len: c_int,
        num_heads: c_int,
        num_kv_heads: c_int,
        head_dim: c_int,
        window_size: c_int,
        stream: CUstream,
    );
}
