<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>mistral.rs Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="Fast, flexible LLM inference.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
            window.path_to_searchindex_js = "searchindex-601bc465.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-84731ca8.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">mistral.rs Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/EricLBuehler/mistral.rs" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="introduction"><a href="#introduction" class="header">Introduction</a></h1>
<p><img src="banner.png" alt="mistral.rs"></p>
<h2 id="quick-links"><a class="header" href="#quick-links">Quick Links</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>I want to…</th><th>Go to…</th></tr>
</thead>
<tbody>
<tr><td>Install mistral.rs</td><td><a href="#installation-guide">Installation Guide</a></td></tr>
<tr><td>Understand cargo features</td><td><a href="#cargo-features-reference">Cargo Features</a></td></tr>
<tr><td>Run a model</td><td><a href="#mistralrs-cli-reference">CLI Reference</a></td></tr>
<tr><td>Use the HTTP API</td><td><a href="#http-server">HTTP Server</a></td></tr>
<tr><td>Fix an error</td><td><a href="#troubleshooting-1">Troubleshooting</a></td></tr>
<tr><td>Configure environment</td><td><a href="#configuration-reference">Configuration</a></td></tr>
<tr><td>Check model support</td><td><a href="#supported-models">Supported Models</a></td></tr>
</tbody>
</table>
</div>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<ul>
<li><a href="#installation-guide">Installation Guide</a> - Install mistral.rs on your system</li>
<li><a href="#cargo-features-reference">Cargo Features</a> - Complete cargo features reference</li>
<li><a href="#mistralrs-cli-reference">CLI Reference</a> - Complete CLI command reference</li>
<li><a href="#mistralrs-cli-toml-config">CLI TOML Configuration</a> - Configure via TOML files</li>
<li><a href="#troubleshooting-1">Troubleshooting</a> - Common issues and solutions</li>
</ul>
<h2 id="sdks--apis"><a class="header" href="#sdks--apis">SDKs &amp; APIs</a></h2>
<ul>
<li><a href="#mistralrs-python-sdk">Python SDK</a> - Python package documentation</li>
<li><a href="#python-sdk-installation">Python Installation</a> - Python SDK installation guide</li>
<li><a href="https://docs.rs/mistralrs/">Rust SDK</a> - Rust crate documentation</li>
<li><a href="#http-server">HTTP Server</a> - OpenAI-compatible HTTP API</li>
<li><a href="#openresponses-api">OpenResponses API</a> - Stateful conversation API</li>
</ul>
<h2 id="models"><a class="header" href="#models">Models</a></h2>
<h3 id="by-category"><a class="header" href="#by-category">By Category</a></h3>
<ul>
<li><a href="#supported-models">Supported Models</a> - Complete model list and compatibility</li>
<li><a href="#vision-model-support-in-mistralrs">Vision Models</a> - Vision model overview</li>
<li><a href="#image-generation-model-support-in-mistralrs">Image Generation</a> - Diffusion models</li>
<li><a href="#embeddings-overview">Embeddings</a> - Embedding model overview</li>
</ul>
<h3 id="model-specific-guides"><a class="header" href="#model-specific-guides">Model-Specific Guides</a></h3>
<details>
<summary>Click to expand model guides</summary>
<p><strong>Text Models:</strong></p>
<ul>
<li><a href="#deepseek-v2-deepseek-aideepseek-v2-lite">DeepSeek V2</a> | <a href="#deepseek-v3-deepseek-aideepseek-v3-deepseek-aideepseek-r1">DeepSeek V3</a></li>
<li><a href="#gemma-2-model">Gemma 2</a> | <a href="#gemma-3-model-googlegemma-3-4b-it">Gemma 3</a> | <a href="#gemma-3n-model-googlegemma-3n-e4b-it">Gemma 3n</a></li>
<li><a href="#glm4-model">GLM4</a> | <a href="#glm-47-flash-moe-zai-orgglm-47-flash">GLM-4.7-Flash</a> | <a href="#glm-47-moe-zai-orgglm-47">GLM-4.7</a></li>
<li><a href="#qwen-3-collection">Qwen 3</a> | <a href="#smollm3-huggingfacetbsmollm3-3b">SmolLM3</a> | <a href="#gpt-oss">GPT-OSS</a></li>
</ul>
<p><strong>Vision Models:</strong></p>
<ul>
<li><a href="#idefics-2-model-huggingfacem4idefics2-8b-chatty">Idefics 2</a> | <a href="#idefics-3-vision-huggingfacem4idefics3-8b-llama3">Idefics 3</a></li>
<li><a href="#llava-and-llavanext-model-llava-hf-model-family">LLaVA</a> | <a href="#llama-32-vision-model-meta-llamallama-32-11b-vision-instruct">Llama 3.2 Vision</a> | <a href="#llama-4-series-meta-llamallama-4-scout-17b-16e-instruct">Llama 4</a></li>
<li><a href="#minicpm-o-26-model-openbmbminicpm-o-2_6">MiniCPM-O 2.6</a> | <a href="#mistral-small-31-model-mistralaimistral-small-31-24b-instruct-2503">Mistral 3</a></li>
<li><a href="#phi-35-model-microsoftphi-35-moe-instruct">Phi 3.5 MoE</a> | <a href="#phi-3-vision-model-microsoftphi-35-vision-instruct">Phi 3.5 Vision</a> | <a href="#phi-4-multimodal-model-microsoftphi-4-multimodal-instruct">Phi 4 Multimodal</a></li>
<li><a href="#qwen-2-vision-model-qwen2-vl-collection">Qwen 2-VL</a> | <a href="#qwen-3-vision-model-qwen3-vl-collection">Qwen 3 VL</a></li>
</ul>
<p><strong>Other Models:</strong></p>
<ul>
<li><a href="#flux1-model-black-forest-labsflux1-schnell">FLUX (Diffusion)</a> | <a href="#dia-16b-model-nari-labsdia-16b">Dia (Speech)</a></li>
<li><a href="#embeddinggemma">EmbeddingGemma</a> | <a href="#qwen3-embedding">Qwen3 Embedding</a></li>
</ul>
</details>
<h2 id="quantization--optimization"><a class="header" href="#quantization--optimization">Quantization &amp; Optimization</a></h2>
<ul>
<li><a href="#quantization-in-mistralrs">Quantization Overview</a> - All supported quantization methods</li>
<li><a href="#in-situ-quantization-isq">ISQ (In-Situ Quantization)</a> - Quantize models at load time</li>
<li><a href="#universal-quantized-file-format-uqff">UQFF Format</a> - Pre-quantized model format | <a href="#uqff-internal-structure">Layout</a></li>
<li><a href="#model-topology-configuration">Topology</a> - Per-layer quantization and device mapping</li>
<li><a href="#enhancing-isq-with-an-imatrix">Importance Matrix</a> - Improve ISQ accuracy</li>
</ul>
<h2 id="adapters--model-customization"><a class="header" href="#adapters--model-customization">Adapters &amp; Model Customization</a></h2>
<ul>
<li><a href="#adapter-model-support">Adapter Models</a> - LoRA and X-LoRA support</li>
<li><a href="#examples-of-lora-and-x-lora-models">LoRA/X-LoRA Examples</a></li>
<li><a href="#x-lora-non-granular-scalings">Non-Granular Scalings</a> - X-LoRA optimization</li>
<li><a href="#build-a-memory-efficient-moe-model-from-anything-in-seconds">AnyMoE</a> - Create MoE models from dense models</li>
<li><a href="#matformer-matryoshka-transformer-support">MatFormer</a> - Dynamic model sizing</li>
</ul>
<h2 id="performance--hardware"><a class="header" href="#performance--hardware">Performance &amp; Hardware</a></h2>
<ul>
<li><a href="#device-mapping-1">Device Mapping</a> - Multi-GPU and CPU offloading</li>
<li><a href="#pagedattention-in-mistralrs">PagedAttention</a> - Efficient KV cache management</li>
<li><a href="#speculative-decoding">Speculative Decoding</a> - Accelerate generation with draft models</li>
<li><a href="#flashattention-in-mistralrs">Flash Attention</a> - Accelerated attention</li>
<li><a href="#multi-head-latent-attention-mla-in-mistralrs">MLA</a> - Multi-head Latent Attention</li>
<li><a href="#distributed-inference-in-mistralrs">Distributed Inference</a>
<ul>
<li><a href="#nccl-in-mistralrs">NCCL Backend</a></li>
<li><a href="#ring-backend-in-mistralrs">Ring Backend</a></li>
</ul>
</li>
</ul>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<ul>
<li><a href="#tool-calling-1">Tool Calling</a> - Function calling support</li>
<li><a href="#web-search-tool-in-mistralrs">Web Search</a> - Integrated web search</li>
<li><a href="#chat-templates-and-tokenizer-customization">Chat Templates</a> - Template customization</li>
<li><a href="#sampling-and-penalty-techniques-in-mistralrs">Sampling Options</a> - Generation parameters</li>
<li><a href="#structured-model-loading-with-toml-files">TOML Selector</a> - Model selection syntax</li>
<li><a href="#multi-model-support-1">Multi-Model Support</a> - Load multiple models</li>
</ul>
<h2 id="mcp-model-context-protocol"><a class="header" href="#mcp-model-context-protocol">MCP (Model Context Protocol)</a></h2>
<ul>
<li><a href="#mcp-model-context-protocol-client">MCP Client</a> - Connect to external tools</li>
<li><a href="#mcp-protocol-support">MCP Server</a> - Serve models over MCP</li>
<li><a href="#mcp-configuration-reference">MCP Configuration</a></li>
<li><a href="#mcp-transport-types-1">MCP Transports</a></li>
<li><a href="#advanced-mcp-usage">MCP Advanced Usage</a></li>
</ul>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<ul>
<li><a href="#configuration-reference">Configuration</a> - Environment variables and server defaults</li>
<li><a href="#engine-internals">Engine Internals</a> - Engine behaviors and recovery</li>
<li><a href="#supported-models">Supported Models</a> - Complete compatibility tables</li>
</ul>
<hr>
<h2 id="contributing"><a class="header" href="#contributing">Contributing</a></h2>
<p>See the main <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/README.md#contributing">README</a> for contribution guidelines.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="installation-guide"><a class="header" href="#installation-guide">Installation Guide</a></h1>
<h2 id="quick-install-recommended"><a class="header" href="#quick-install-recommended">Quick Install (Recommended)</a></h2>
<p>The install script automatically detects your hardware (CUDA, Metal, MKL) and builds with optimal features.</p>
<p><strong>Linux/macOS:</strong></p>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/install.sh | sh
</code></pre>
<p><strong>Windows (PowerShell):</strong></p>
<pre><code class="language-powershell">irm https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/install.ps1 | iex
</code></pre>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ol>
<li>
<p>Install required packages:</p>
<ul>
<li>OpenSSL: <code>sudo apt install libssl-dev</code> (Ubuntu)</li>
<li>pkg-config (Linux only): <code>sudo apt install pkg-config</code></li>
</ul>
</li>
<li>
<p>Install Rust from https://rustup.rs/</p>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
</code></pre>
</li>
<li>
<p>(Optional) Set up HuggingFace authentication:</p>
<pre><code class="language-bash">mistralrs login
</code></pre>
<p>Or use <code>huggingface-cli login</code> as documented <a href="https://huggingface.co/docs/huggingface_hub/en/installation">here</a>.</p>
</li>
</ol>
<h2 id="supported-accelerators"><a class="header" href="#supported-accelerators">Supported Accelerators</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Accelerator</th><th>Feature Flag</th><th>Additional Flags</th></tr>
</thead>
<tbody>
<tr><td>NVIDIA GPUs (CUDA)</td><td><code>cuda</code></td><td><code>flash-attn</code>, <code>flash-attn-v3</code>, <code>cudnn</code></td></tr>
<tr><td>Apple Silicon GPU (Metal)</td><td><code>metal</code></td><td></td></tr>
<tr><td>CPU (Intel)</td><td><code>mkl</code></td><td></td></tr>
<tr><td>CPU (Apple Accelerate)</td><td><code>accelerate</code></td><td></td></tr>
<tr><td>Generic CPU (ARM/AVX)</td><td><em>none</em></td><td>ARM NEON / AVX enabled by default</td></tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>Note for Linux users:</strong> The <code>metal</code> feature is macOS-only. Use <code>--features "cuda flash-attn cudnn"</code> for NVIDIA GPUs or <code>--features mkl</code> for Intel CPUs instead of <code>--all-features</code>.</p>
</blockquote>
<h2 id="feature-detection"><a class="header" href="#feature-detection">Feature Detection</a></h2>
<p>Determine which features to enable based on your hardware:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Hardware</th><th>Features</th></tr>
</thead>
<tbody>
<tr><td>NVIDIA GPU (Ampere+, compute &gt;=80)</td><td><code>cuda cudnn flash-attn</code></td></tr>
<tr><td>NVIDIA GPU (Hopper, compute 90)</td><td><code>cuda cudnn flash-attn flash-attn-v3</code></td></tr>
<tr><td>NVIDIA GPU (older)</td><td><code>cuda cudnn</code></td></tr>
<tr><td>Apple Silicon (macOS)</td><td><code>metal accelerate</code></td></tr>
<tr><td>Intel CPU with MKL</td><td><code>mkl</code></td></tr>
<tr><td>CPU only</td><td>(no features needed)</td></tr>
</tbody>
</table>
</div>
<h2 id="install-from-cratesio"><a class="header" href="#install-from-cratesio">Install from crates.io</a></h2>
<pre><code class="language-bash">cargo install mistralrs-cli --features "&lt;your-features&gt;"
</code></pre>
<p>Example:</p>
<pre><code class="language-bash">cargo install mistralrs-cli --features "cuda flash-attn cudnn"
</code></pre>
<h2 id="build-from-source"><a class="header" href="#build-from-source">Build from Source</a></h2>
<pre><code class="language-bash">git clone https://github.com/EricLBuehler/mistral.rs.git
cd mistral.rs
cargo install --path mistralrs-cli --features "&lt;your-features&gt;"
</code></pre>
<p>Example:</p>
<pre><code class="language-bash">cargo build --release --features "cuda flash-attn cudnn"
</code></pre>
<h2 id="docker"><a class="header" href="#docker">Docker</a></h2>
<p>Docker images are available for quick deployment:</p>
<pre><code class="language-bash">docker pull ghcr.io/ericlbuehler/mistral.rs:latest
docker run --gpus all -p 1234:1234 ghcr.io/ericlbuehler/mistral.rs:latest \
  serve -m Qwen/Qwen3-4B
</code></pre>
<p><a href="https://github.com/EricLBuehler/mistral.rs/pkgs/container/mistral.rs">Docker images on GitHub Container Registry</a></p>
<p>Learn more about running Docker containers: https://docs.docker.com/engine/reference/run/</p>
<h2 id="python-sdk"><a class="header" href="#python-sdk">Python SDK</a></h2>
<p>Install the Python package:</p>
<pre><code class="language-bash">pip install mistralrs-cuda    # For NVIDIA GPUs
pip install mistralrs-metal   # For Apple Silicon
pip install mistralrs-mkl     # For Intel CPUs
pip install mistralrs         # CPU-only
</code></pre>
<ul>
<li><a href="#python-sdk-installation">Full installation instructions</a></li>
<li><a href="#mistralrs-python-sdk">SDK documentation</a></li>
</ul>
<h2 id="verify-installation"><a class="header" href="#verify-installation">Verify Installation</a></h2>
<p>After installation, verify everything works:</p>
<pre><code class="language-bash"># Check CLI is installed
mistralrs --help

# Run system diagnostics
mistralrs doctor

# Test with a small model
mistralrs run -m Qwen/Qwen3-0.6B
</code></pre>
<h2 id="getting-models"><a class="header" href="#getting-models">Getting Models</a></h2>
<h3 id="from-hugging-face-hub-default"><a class="header" href="#from-hugging-face-hub-default">From Hugging Face Hub (Default)</a></h3>
<p>Models download automatically from Hugging Face Hub:</p>
<pre><code class="language-bash">mistralrs run -m meta-llama/Llama-3.2-3B-Instruct
</code></pre>
<p>For gated models, authenticate first:</p>
<pre><code class="language-bash">mistralrs login
# Or: mistralrs run --token-source env:HF_TOKEN -m &lt;model&gt;
</code></pre>
<h3 id="from-local-files"><a class="header" href="#from-local-files">From Local Files</a></h3>
<p>Pass a path to a downloaded model:</p>
<pre><code class="language-bash">mistralrs run -m /path/to/model
</code></pre>
<h3 id="running-gguf-models"><a class="header" href="#running-gguf-models">Running GGUF Models</a></h3>
<pre><code class="language-bash">mistralrs run --format gguf -m author/model-repo -f model-quant.gguf
</code></pre>
<p>Specify tokenizer if needed:</p>
<pre><code class="language-bash">mistralrs run --format gguf -m author/model-repo -f file.gguf -t author/official-tokenizer
</code></pre>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<ul>
<li><a href="#mistralrs-cli-reference">CLI Reference</a> - All commands and options</li>
<li><a href="#http-server">HTTP API</a> - Run as an OpenAI-compatible server</li>
<li><a href="#mistralrs-python-sdk">Python SDK</a> - Python package documentation</li>
<li><a href="#troubleshooting-1">Troubleshooting</a> - Common issues and solutions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cargo-features-reference"><a class="header" href="#cargo-features-reference">Cargo Features Reference</a></h1>
<p>This document provides a complete reference for all cargo features available in mistral.rs.</p>
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Description</th><th>Platform</th><th>Requires</th></tr>
</thead>
<tbody>
<tr><td><code>cuda</code></td><td>NVIDIA GPU acceleration</td><td>Linux, Windows</td><td>CUDA toolkit</td></tr>
<tr><td><code>cudnn</code></td><td>NVIDIA cuDNN backend</td><td>Linux, Windows</td><td><code>cuda</code>, cuDNN</td></tr>
<tr><td><code>flash-attn</code></td><td>FlashAttention V2</td><td>Linux, Windows</td><td><code>cuda</code>, CC &gt;= 8.0</td></tr>
<tr><td><code>flash-attn-v3</code></td><td>FlashAttention V3</td><td>Linux, Windows</td><td><code>cuda</code>, CC &gt;= 9.0</td></tr>
<tr><td><code>metal</code></td><td>Apple GPU acceleration</td><td>macOS</td><td>-</td></tr>
<tr><td><code>accelerate</code></td><td>Apple CPU acceleration</td><td>macOS</td><td>-</td></tr>
<tr><td><code>mkl</code></td><td>Intel MKL acceleration</td><td>Linux, Windows</td><td>Intel MKL</td></tr>
<tr><td><code>nccl</code></td><td>Multi-GPU (NVIDIA NCCL)</td><td>Linux</td><td><code>cuda</code>, NCCL</td></tr>
<tr><td><code>ring</code></td><td>Multi-GPU/node (TCP ring)</td><td>All</td><td>-</td></tr>
</tbody>
</table>
</div>
<h2 id="gpu-acceleration-features"><a class="header" href="#gpu-acceleration-features">GPU Acceleration Features</a></h2>
<h3 id="cuda"><a class="header" href="#cuda"><code>cuda</code></a></h3>
<p>Enables NVIDIA GPU acceleration via CUDA. This is the primary feature for running on NVIDIA GPUs.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>NVIDIA GPU</li>
<li>CUDA toolkit installed</li>
<li>Linux or Windows (WSL supported)</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features cuda
cargo install mistralrs-cli --features cuda
</code></pre>
<p><strong>What it enables:</strong></p>
<ul>
<li>GPU tensor operations via CUDA</li>
<li>PagedAttention on CUDA devices</li>
<li>Quantized inference on GPU</li>
</ul>
<hr>
<h3 id="cudnn"><a class="header" href="#cudnn"><code>cudnn</code></a></h3>
<p>Enables NVIDIA cuDNN for optimized neural network primitives. Provides faster convolutions and other operations.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><code>cuda</code> feature</li>
<li>cuDNN library installed</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features "cuda cudnn"
</code></pre>
<hr>
<h3 id="flash-attn"><a class="header" href="#flash-attn"><code>flash-attn</code></a></h3>
<p>Enables FlashAttention V2 for faster attention computation. Significantly reduces memory usage and improves throughput.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><code>cuda</code> feature (automatically enabled)</li>
<li>GPU with compute capability &gt;= 8.0 (Ampere or newer)</li>
</ul>
<p><strong>Compatible GPUs:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Architecture</th><th>Compute Capability</th><th>Example GPUs</th></tr>
</thead>
<tbody>
<tr><td>Ampere</td><td>8.0, 8.6</td><td>RTX 30 series, A100, A40</td></tr>
<tr><td>Ada Lovelace</td><td>8.9</td><td>RTX 40 series, L40S</td></tr>
<tr><td>Blackwell</td><td>10.0, 12.0</td><td>RTX 50 series</td></tr>
</tbody>
</table>
</div>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features "cuda flash-attn cudnn"
</code></pre>
<blockquote>
<p>Note: FlashAttention V2 and V3 are mutually exclusive. Do not enable both.</p>
</blockquote>
<hr>
<h3 id="flash-attn-v3"><a class="header" href="#flash-attn-v3"><code>flash-attn-v3</code></a></h3>
<p>Enables FlashAttention V3 for Hopper architecture GPUs. Provides additional performance improvements over V2 on supported hardware.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><code>cuda</code> feature (automatically enabled)</li>
<li>GPU with compute capability &gt;= 9.0 (Hopper)</li>
</ul>
<p><strong>Compatible GPUs:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Architecture</th><th>Compute Capability</th><th>Example GPUs</th></tr>
</thead>
<tbody>
<tr><td>Hopper</td><td>9.0</td><td>H100, H800</td></tr>
</tbody>
</table>
</div>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features "cuda flash-attn-v3 cudnn"
</code></pre>
<blockquote>
<p>Note: FlashAttention V2 and V3 are mutually exclusive. Do not enable both.</p>
</blockquote>
<hr>
<h3 id="metal"><a class="header" href="#metal"><code>metal</code></a></h3>
<p>Enables Apple Metal GPU acceleration for macOS devices.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>macOS with Apple Silicon or AMD GPU</li>
<li>macOS only (not available on Linux)</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features metal
</code></pre>
<p><strong>What it enables:</strong></p>
<ul>
<li>GPU tensor operations via Metal</li>
<li>PagedAttention on Metal devices (opt-in via <code>--paged-attn</code>)</li>
<li>Quantized inference on Apple GPUs</li>
</ul>
<blockquote>
<p>Note: PagedAttention is disabled by default on Metal. Enable with <code>--paged-attn</code> flag.</p>
</blockquote>
<hr>
<h2 id="cpu-acceleration-features"><a class="header" href="#cpu-acceleration-features">CPU Acceleration Features</a></h2>
<h3 id="accelerate"><a class="header" href="#accelerate"><code>accelerate</code></a></h3>
<p>Enables Apple’s Accelerate framework for optimized CPU operations on macOS.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>macOS</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features accelerate
# Or combined with Metal:
cargo build --release --features "metal accelerate"
</code></pre>
<hr>
<h3 id="mkl"><a class="header" href="#mkl"><code>mkl</code></a></h3>
<p>Enables Intel Math Kernel Library (MKL) for optimized CPU operations.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Intel MKL installed</li>
<li>Intel CPU recommended (works on AMD but Intel-optimized)</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features mkl
</code></pre>
<hr>
<h2 id="distributed-inference-features"><a class="header" href="#distributed-inference-features">Distributed Inference Features</a></h2>
<h3 id="nccl"><a class="header" href="#nccl"><code>nccl</code></a></h3>
<p>Enables multi-GPU distributed inference using NVIDIA NCCL (NVIDIA Collective Communications Library). Implements tensor parallelism for splitting large models across multiple GPUs.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><code>cuda</code> feature (automatically enabled)</li>
<li>Multiple NVIDIA GPUs</li>
<li>NCCL library</li>
<li>World size must be a power of 2 (1, 2, 4, 8, etc.)</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features "cuda nccl"

# Run with specific GPU count
MISTRALRS_MN_LOCAL_WORLD_SIZE=2 mistralrs serve -m Qwen/Qwen3-30B-A3B-Instruct
</code></pre>
<p><strong>Environment Variables:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_MN_LOCAL_WORLD_SIZE</code></td><td>Number of GPUs to use (defaults to all)</td></tr>
<tr><td><code>MISTRALRS_NO_NCCL=1</code></td><td>Disable NCCL and use device mapping instead</td></tr>
</tbody>
</table>
</div>
<p><strong>Multi-node setup</strong> requires additional environment variables. See <a href="#nccl-in-mistralrs">NCCL documentation</a> for details.</p>
<blockquote>
<p>Note: When NCCL is enabled, automatic device mapping is disabled.</p>
</blockquote>
<hr>
<h3 id="ring"><a class="header" href="#ring"><code>ring</code></a></h3>
<p>Enables distributed tensor-parallel inference using a TCP-based ring topology. Works across multiple machines without requiring NCCL.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>World size must be a power of 2 (2, 4, 8, etc.)</li>
<li>TCP ports must be open between nodes</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">cargo build --release --features ring

# Configure via JSON file
export RING_CONFIG=path/to/ring_config.json
mistralrs serve -m model-id
</code></pre>
<p><strong>Configuration:</strong></p>
<p>Create a JSON configuration file for each process:</p>
<pre><code class="language-json">{
  "master_ip": "0.0.0.0",
  "master_port": 1234,
  "port": 12345,
  "right_port": 12346,
  "rank": 0,
  "world_size": 2
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>master_ip</code></td><td>IP address for master node</td></tr>
<tr><td><code>master_port</code></td><td>Port for master node</td></tr>
<tr><td><code>port</code></td><td>Local port for incoming connections</td></tr>
<tr><td><code>right_port</code></td><td>Port of right neighbor in ring</td></tr>
<tr><td><code>right_ip</code></td><td>IP of right neighbor (optional, defaults to localhost)</td></tr>
<tr><td><code>rank</code></td><td>Process rank (0 to world_size-1)</td></tr>
<tr><td><code>world_size</code></td><td>Total number of processes (must be power of 2)</td></tr>
</tbody>
</table>
</div>
<p>See <a href="#ring-backend-in-mistralrs">Ring documentation</a> for detailed setup instructions.</p>
<hr>
<h2 id="feature-combinations"><a class="header" href="#feature-combinations">Feature Combinations</a></h2>
<h3 id="recommended-combinations-by-hardware"><a class="header" href="#recommended-combinations-by-hardware">Recommended Combinations by Hardware</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Hardware</th><th>Recommended Features</th></tr>
</thead>
<tbody>
<tr><td>NVIDIA Ampere+ (RTX 30/40, A100)</td><td><code>cuda cudnn flash-attn</code></td></tr>
<tr><td>NVIDIA Hopper (H100)</td><td><code>cuda cudnn flash-attn-v3</code></td></tr>
<tr><td>NVIDIA older GPUs</td><td><code>cuda cudnn</code></td></tr>
<tr><td>Apple Silicon</td><td><code>metal accelerate</code></td></tr>
<tr><td>Intel CPU</td><td><code>mkl</code></td></tr>
<tr><td>Generic CPU</td><td>(no features needed)</td></tr>
<tr><td>Multi-GPU NVIDIA</td><td><code>cuda cudnn flash-attn nccl</code></td></tr>
<tr><td>Multi-node/cross-platform</td><td><code>ring</code> (plus GPU features)</td></tr>
</tbody>
</table>
</div>
<h3 id="installation-examples"><a class="header" href="#installation-examples">Installation Examples</a></h3>
<pre><code class="language-bash"># NVIDIA GPU with all optimizations
cargo install mistralrs-cli --features "cuda cudnn flash-attn"

# Apple Silicon
cargo install mistralrs-cli --features "metal accelerate"

# Intel CPU
cargo install mistralrs-cli --features "mkl"

# Multi-GPU NVIDIA setup
cargo install mistralrs-cli --features "cuda cudnn flash-attn nccl"

# Build from source with CUDA
git clone https://github.com/EricLBuehler/mistral.rs.git
cd mistral.rs
cargo build --release --features "cuda cudnn flash-attn"
</code></pre>
<hr>
<h2 id="internal-features"><a class="header" href="#internal-features">Internal Features</a></h2>
<p>These features are primarily for library development and are not typically used directly:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>pyo3_macros</code></td><td>Python bindings support (used by mistralrs-pyo3)</td></tr>
<tr><td><code>utoipa</code></td><td>OpenAPI documentation generation</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="python-package-features"><a class="header" href="#python-package-features">Python Package Features</a></h2>
<p>The Python SDK is distributed as separate packages with features pre-configured:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Package</th><th>Equivalent Features</th></tr>
</thead>
<tbody>
<tr><td><code>mistralrs-cuda</code></td><td><code>cuda cudnn flash-attn</code></td></tr>
<tr><td><code>mistralrs-metal</code></td><td><code>metal accelerate</code></td></tr>
<tr><td><code>mistralrs-mkl</code></td><td><code>mkl</code></td></tr>
<tr><td><code>mistralrs</code></td><td>CPU only</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-bash">pip install mistralrs-cuda    # NVIDIA GPUs
pip install mistralrs-metal   # Apple Silicon
pip install mistralrs-mkl     # Intel CPUs
pip install mistralrs         # Generic CPU
</code></pre>
<hr>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="diagnosing-issues"><a class="header" href="#diagnosing-issues">Diagnosing Issues</a></h3>
<p>Use <code>mistralrs doctor</code> to diagnose your system configuration and verify features are working correctly:</p>
<pre><code class="language-bash">mistralrs doctor
</code></pre>
<p>This command checks:</p>
<ul>
<li>Detected hardware (GPUs, CPU features)</li>
<li>Installed libraries (CUDA, cuDNN, etc.)</li>
<li>Feature compatibility</li>
<li>Common configuration issues</li>
</ul>
<h3 id="feature-not-working"><a class="header" href="#feature-not-working">Feature not working</a></h3>
<ol>
<li>
<p>Run <code>mistralrs doctor</code> to check system configuration</p>
</li>
<li>
<p>Verify the feature is enabled in your build:</p>
<pre><code class="language-bash">cargo build --release --features "your-features" -v
</code></pre>
</li>
<li>
<p>Check hardware compatibility (especially for flash-attn)</p>
</li>
<li>
<p>Ensure required libraries are installed (CUDA, cuDNN, MKL, etc.)</p>
</li>
</ol>
<h3 id="conflicting-features"><a class="header" href="#conflicting-features">Conflicting features</a></h3>
<ul>
<li><code>flash-attn</code> and <code>flash-attn-v3</code> are mutually exclusive</li>
<li><code>metal</code> is macOS-only; don’t use with <code>cuda</code></li>
<li><code>nccl</code> requires <code>cuda</code></li>
</ul>
<h3 id="build-errors"><a class="header" href="#build-errors">Build errors</a></h3>
<ul>
<li><strong>CUDA not found</strong>: Ensure CUDA toolkit is installed and <code>nvcc</code> is in PATH</li>
<li><strong>MKL not found</strong>: Install Intel oneAPI or standalone MKL</li>
<li><strong>Metal errors on Linux</strong>: Remove <code>metal</code> feature (macOS only)</li>
</ul>
<p>See <a href="#troubleshooting-1">Troubleshooting</a> for more solutions.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mistralrs-cli-reference"><a class="header" href="#mistralrs-cli-reference">mistralrs CLI Reference</a></h1>
<p>This is the comprehensive CLI reference for <code>mistralrs</code>. The CLI provides commands for interactive mode, HTTP server, builtin UI, quantization, and system diagnostics.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#commands">Commands</a>
<ul>
<li><a href="#run---interactive-mode">run</a>: run model in interactive mode</li>
<li><a href="#serve---http-server">serve</a>: start HTTP/MCP server and (optionally) the UI</li>
<li><a href="#from-config---toml-configuration">from-config</a>: run from a <a href="#mistralrs-cli-toml-config">TOML configuration file</a></li>
<li><a href="#quantize---uqff-generation">quantize</a>: generate UQFF quantized model file</li>
<li><a href="#tune---recommendations">tune</a>: recommend quantization + device mapping for a model</li>
<li><a href="#doctor---system-diagnostics">doctor</a>: run system diagnostics and environment checks</li>
<li><a href="#login---huggingface-authentication">login</a>: authenticate with HuggingFace Hub</li>
<li><a href="#cache---model-management">cache</a>: manage the HuggingFace model cache</li>
<li><a href="#bench---performance-benchmarking">bench</a>: run performance benchmarks</li>
<li><a href="#completions---shell-completions">completions</a>: generate shell completions</li>
</ul>
</li>
<li><a href="#model-types">Model Types</a>
<ul>
<li><a href="#auto">auto</a></li>
<li><a href="#text">text</a></li>
<li><a href="#vision">vision</a></li>
<li><a href="#diffusion">diffusion</a></li>
<li><a href="#speech">speech</a></li>
<li><a href="#embedding">embedding</a></li>
</ul>
</li>
<li><a href="#features-1">Features</a>
<ul>
<li><a href="#isq-quantization">ISQ Quantization</a></li>
<li><a href="#uqff-files">UQFF Files</a></li>
<li><a href="#pagedattention">PagedAttention</a></li>
<li><a href="#device-mapping">Device Mapping</a></li>
<li><a href="#lora-and-x-lora">LoRA and X-LoRA</a></li>
<li><a href="#chat-templates">Chat Templates</a></li>
<li><a href="#web-search">Web Search</a></li>
<li><a href="#thinking-mode">Thinking Mode</a></li>
</ul>
</li>
<li><a href="#global-options">Global Options</a></li>
<li><a href="#interactive-commands">Interactive Commands</a></li>
</ul>
<hr>
<h2 id="commands"><a class="header" href="#commands">Commands</a></h2>
<h3 id="run---interactive-mode"><a class="header" href="#run---interactive-mode">run - Interactive Mode</a></h3>
<p>Start a model in interactive mode for conversational use.</p>
<pre><code class="language-bash">mistralrs run [MODEL_TYPE] -m &lt;MODEL_ID&gt; [OPTIONS]
</code></pre>
<p>Note: <code>MODEL_TYPE</code> is optional and defaults to <code>auto</code> if not specified. This allows a shorter syntax.</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Run a text model interactively (shorthand - auto type is implied)
mistralrs run -m Qwen/Qwen3-4B

# Explicit auto type (equivalent to above)
mistralrs run -m Qwen/Qwen3-4B

# Run with thinking mode enabled
mistralrs run -m Qwen/Qwen3-4B --enable-thinking

# Run a vision model
mistralrs run -m google/gemma-3-4b-it
</code></pre>
<p><strong>Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--enable-thinking</code></td><td>Enable thinking mode for models that support it</td></tr>
</tbody>
</table>
</div>
<p>The <code>run</code> command also accepts all <a href="#runtime-options">runtime options</a>.</p>
<hr>
<h3 id="serve---http-server"><a class="header" href="#serve---http-server">serve - HTTP Server</a></h3>
<p>Start an HTTP server with OpenAI-compatible API endpoints.</p>
<pre><code class="language-bash">mistralrs serve [MODEL_TYPE] -m &lt;MODEL_ID&gt; [OPTIONS]
</code></pre>
<p>Note: <code>MODEL_TYPE</code> is optional and defaults to <code>auto</code> if not specified.</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Start server on default port 1234 (shorthand)
mistralrs serve -m Qwen/Qwen3-4B

# Explicit auto type (equivalent to above)
mistralrs serve -m Qwen/Qwen3-4B

# Start server with web UI
mistralrs serve -m Qwen/Qwen3-4B --ui

# Start server on custom port
mistralrs serve -m Qwen/Qwen3-4B -p 3000

# Start server with MCP support
mistralrs serve -m Qwen/Qwen3-4B --mcp-port 8081
</code></pre>
<p><strong>Server Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-p, --port &lt;PORT&gt;</code></td><td><code>1234</code></td><td>HTTP server port</td></tr>
<tr><td><code>--host &lt;HOST&gt;</code></td><td><code>0.0.0.0</code></td><td>Bind address</td></tr>
<tr><td><code>--ui</code></td><td>disabled</td><td>Serve built-in web UI at <code>/ui</code></td></tr>
<tr><td><code>--mcp-port &lt;PORT&gt;</code></td><td>none</td><td>MCP protocol server port</td></tr>
<tr><td><code>--mcp-config &lt;PATH&gt;</code></td><td>none</td><td>MCP client configuration file</td></tr>
</tbody>
</table>
</div>
<p>The <code>serve</code> command also accepts all <a href="#runtime-options">runtime options</a>.</p>
<hr>
<h3 id="quantize---uqff-generation"><a class="header" href="#quantize---uqff-generation">quantize - UQFF Generation</a></h3>
<p>Generate UQFF (Unified Quantized File Format) files from a model. Supports multiple quantization types in a single command.</p>
<pre><code class="language-bash">mistralrs quantize &lt;MODEL_TYPE&gt; -m &lt;MODEL_ID&gt; --isq &lt;LEVEL&gt;[,&lt;LEVEL&gt;...] -o &lt;OUTPUT&gt;
</code></pre>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Quantize to a single type (file output)
mistralrs quantize -m Qwen/Qwen3-4B --isq q4k -o qwen3-4b-uqff/qwen3-4b-q4k.uqff

# Quantize to a single type (directory output, auto-named)
mistralrs quantize -m Qwen/Qwen3-4B --isq q4k -o qwen3-4b-uqff/

# Quantize to multiple types at once (directory output)
mistralrs quantize -m Qwen/Qwen3-4B --isq q4k,q8_0 -o qwen3-4b-uqff/

# Equivalent: repeated --isq flags
mistralrs quantize -m Qwen/Qwen3-4B --isq q4k --isq q8_0 -o qwen3-4b-uqff/

# Quantize a vision model
mistralrs quantize -m google/gemma-3-4b-it --isq 4 -o gemma3-4b-uqff/

# Quantize with imatrix for better quality
mistralrs quantize -m Qwen/Qwen3-4B --isq q4k --imatrix imatrix.dat -o qwen3-4b-uqff/qwen3-4b-q4k.uqff
</code></pre>
<p>When using directory output mode, the <code>quantize</code> command automatically:</p>
<ul>
<li>Generates a <code>README.md</code> model card with Hugging Face frontmatter and example commands</li>
<li>Prints the <code>huggingface-cli upload</code> command to upload your UQFF to Hugging Face</li>
</ul>
<p><strong>Quantize Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Required</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-m, --model-id &lt;ID&gt;</code></td><td>Yes</td><td>Model ID or local path</td></tr>
<tr><td><code>--isq &lt;LEVEL&gt;</code></td><td>Yes</td><td>Quantization level(s), comma-separated or repeated (see <a href="#isq-quantization">ISQ Quantization</a>)</td></tr>
<tr><td><code>-o, --output &lt;PATH&gt;</code></td><td>Yes</td><td>Output path: <code>.uqff</code> file (single ISQ) or directory (auto-named per ISQ type)</td></tr>
<tr><td><code>--isq-organization &lt;TYPE&gt;</code></td><td>No</td><td>ISQ organization strategy: <code>default</code> or <code>moqe</code></td></tr>
<tr><td><code>--imatrix &lt;PATH&gt;</code></td><td>No</td><td>imatrix file for enhanced quantization</td></tr>
<tr><td><code>--calibration-file &lt;PATH&gt;</code></td><td>No</td><td>Calibration file for imatrix generation</td></tr>
<tr><td><code>--no-readme</code></td><td>No</td><td>Skip automatic README.md model card generation</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="tune---recommendations"><a class="header" href="#tune---recommendations">tune - Recommendations</a></h3>
<p>Get quantization and device mapping recommendations for a model. The tune command analyzes your hardware and shows all quantization options with their estimated memory usage, context room, and quality trade-offs.</p>
<pre><code class="language-bash">mistralrs tune [MODEL_TYPE] -m &lt;MODEL_ID&gt; [OPTIONS]
</code></pre>
<p>Note: <code>MODEL_TYPE</code> is optional and defaults to <code>auto</code> if not specified, which supports all model types. See <a href="#auto">details</a>.</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Get balanced recommendations (shorthand)
mistralrs tune -m Qwen/Qwen3-4B

# Get quality-focused recommendations
mistralrs tune -m Qwen/Qwen3-4B --profile quality

# Get fast inference recommendations
mistralrs tune -m Qwen/Qwen3-4B --profile fast

# Output as JSON
mistralrs tune -m Qwen/Qwen3-4B --json

# Generate a TOML config file with recommendations
mistralrs tune -m Qwen/Qwen3-4B --emit-config config.toml
</code></pre>
<p><strong>Example Output (CUDA):</strong></p>
<pre><code>Tuning Analysis
===============

Model: Qwen/Qwen3-4B
Profile: Balanced
Backend: cuda
Total VRAM: 24.0 GB

Quantization Options
--------------------
┌─────────────┬───────────┬────────┬──────────────┬───────────────┬──────────────────┐
│ Quant       │ Est. Size │ VRAM % │ Context Room │ Quality       │ Status           │
├─────────────┼───────────┼────────┼──────────────┼───────────────┼──────────────────┤
│ None (FP16) │ 8.50 GB   │ 35%    │ 48k          │ Baseline      │ ✅ Fits          │
│ Q8_0        │ 4.50 GB   │ 19%    │ 96k          │ Near-lossless │ 🚀 Recommended   │
│ Q6K         │ 3.70 GB   │ 15%    │ 128k (max)   │ Good          │ ✅ Fits          │
│ Q5K         │ 3.20 GB   │ 13%    │ 128k (max)   │ Good          │ ✅ Fits          │
│ Q4K         │ 2.60 GB   │ 11%    │ 128k (max)   │ Acceptable    │ ✅ Fits          │
│ Q3K         │ 2.00 GB   │ 8%     │ 128k (max)   │ Degraded      │ ✅ Fits          │
│ Q2K         │ 1.50 GB   │ 6%     │ 128k (max)   │ Degraded      │ ✅ Fits          │
└─────────────┴───────────┴────────┴──────────────┴───────────────┴──────────────────┘

Recommended Command
-------------------
  mistralrs serve -m Qwen/Qwen3-4B --isq q8_0

[INFO] PagedAttention is available (mode: auto)
</code></pre>
<p><strong>Example Output (Metal):</strong></p>
<p>On macOS with Metal, the command recommends Apple Format Quantization (AFQ) types:</p>
<pre><code>Quantization Options
--------------------
┌─────────────┬───────────┬────────┬──────────────┬───────────────┬──────────────────┐
│ Quant       │ Est. Size │ VRAM % │ Context Room │ Quality       │ Status           │
├─────────────┼───────────┼────────┼──────────────┼───────────────┼──────────────────┤
│ None (FP16) │ 8.50 GB   │ 53%    │ 24k          │ Baseline      │ ✅ Fits          │
│ AFQ8        │ 4.50 GB   │ 28%    │ 56k          │ Near-lossless │ 🚀 Recommended   │
│ AFQ6        │ 3.70 GB   │ 23%    │ 64k          │ Good          │ ✅ Fits          │
│ AFQ4        │ 2.60 GB   │ 16%    │ 128k (max)   │ Acceptable    │ ✅ Fits          │
│ AFQ3        │ 2.00 GB   │ 13%    │ 128k (max)   │ Degraded      │ ✅ Fits          │
│ AFQ2        │ 1.50 GB   │ 9%     │ 128k (max)   │ Degraded      │ ✅ Fits          │
└─────────────┴───────────┴────────┴──────────────┴───────────────┴──────────────────┘
</code></pre>
<p><strong>Status Legend:</strong></p>
<ul>
<li>🚀 <strong>Recommended</strong>: Best option for your profile and hardware</li>
<li>✅ <strong>Fits</strong>: Model fits entirely in GPU memory</li>
<li>⚠️ <strong>Hybrid</strong>: Model requires CPU offloading (slower due to PCIe bottleneck)</li>
<li>❌ <strong>Too Large</strong>: Model doesn’t fit even with CPU offload</li>
</ul>
<p><strong>Tune Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--profile &lt;PROFILE&gt;</code></td><td><code>balanced</code></td><td>Tuning profile: <code>quality</code>, <code>balanced</code>, or <code>fast</code></td></tr>
<tr><td><code>--json</code></td><td>disabled</td><td>Output JSON instead of human-readable text</td></tr>
<tr><td><code>--emit-config &lt;PATH&gt;</code></td><td>none</td><td>Emit a TOML config file with recommended settings</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="doctor---system-diagnostics"><a class="header" href="#doctor---system-diagnostics">doctor - System Diagnostics</a></h3>
<p>Run comprehensive system diagnostics and environment checks. The doctor command helps identify configuration issues and validates your system is ready for inference.</p>
<pre><code class="language-bash">mistralrs doctor [OPTIONS]
</code></pre>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Run diagnostics
mistralrs doctor

# Output as JSON
mistralrs doctor --json
</code></pre>
<p><strong>Checks Performed:</strong></p>
<ul>
<li><strong>CPU Extensions</strong>: AVX, AVX2, AVX-512, FMA support (x86 only; ARM shows NEON)</li>
<li><strong>Binary/Hardware Match</strong>: Validates CUDA/Metal features match detected hardware</li>
<li><strong>GPU Compute Capability</strong>: Reports compute version and Flash Attention v2/v3 compatibility</li>
<li><strong>Flash Attention Features</strong>: Warns if hardware supports FA but binary doesn’t have it enabled</li>
<li><strong>Hugging Face Connectivity</strong>: Tests connection and token validity using a gated model</li>
<li><strong>HF Cache</strong>: Verifies cache directory is writable</li>
<li><strong>Disk Space</strong>: Checks available storage</li>
</ul>
<p><strong>Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--json</code></td><td>Output JSON instead of human-readable text</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="login---huggingface-authentication"><a class="header" href="#login---huggingface-authentication">login - HuggingFace Authentication</a></h3>
<p>Authenticate with HuggingFace Hub by saving your token to the local cache.</p>
<pre><code class="language-bash">mistralrs login [OPTIONS]
</code></pre>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Interactive login (prompts for token)
mistralrs login

# Provide token directly
mistralrs login --token hf_xxxxxxxxxxxxx
</code></pre>
<p>The token is saved to the standard HuggingFace cache location:</p>
<ul>
<li>Linux/macOS: <code>~/.cache/huggingface/token</code></li>
<li>Windows: <code>C:\Users\&lt;user&gt;\.cache\huggingface\token</code></li>
</ul>
<p>If the <code>HF_HOME</code> environment variable is set, the token is saved to <code>$HF_HOME/token</code>.</p>
<p><strong>Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--token &lt;TOKEN&gt;</code></td><td>Provide token directly (non-interactive)</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="cache---model-management"><a class="header" href="#cache---model-management">cache - Model Management</a></h3>
<p>Manage the HuggingFace model cache. List cached models or delete specific models.</p>
<pre><code class="language-bash">mistralrs cache &lt;SUBCOMMAND&gt;
</code></pre>
<p><strong>Subcommands:</strong></p>
<h4 id="cache-list"><a class="header" href="#cache-list">cache list</a></h4>
<p>List all cached models with their sizes and last used times.</p>
<pre><code class="language-bash">mistralrs cache list
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code>HuggingFace Model Cache
-----------------------

┌──────────────────────────┬──────────┬─────────────┐
│ Model                    │ Size     │ Last Used   │
├──────────────────────────┼──────────┼─────────────┤
│ Qwen/Qwen3-4B            │ 8.5 GB   │ today       │
│ google/gemma-3-4b-it     │ 6.2 GB   │ 2 days ago  │
│ meta-llama/Llama-3.2-3B  │ 5.8 GB   │ 1 week ago  │
└──────────────────────────┴──────────┴─────────────┘

Total: 3 models, 20.5 GB
Cache directory: /home/user/.cache/huggingface/hub
</code></pre>
<h4 id="cache-delete"><a class="header" href="#cache-delete">cache delete</a></h4>
<p>Delete a specific model from the cache.</p>
<pre><code class="language-bash">mistralrs cache delete -m &lt;MODEL_ID&gt;
</code></pre>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Delete a specific model
mistralrs cache delete -m Qwen/Qwen3-4B

# Delete a model with organization
mistralrs cache delete -m meta-llama/Llama-3.2-3B
</code></pre>
<hr>
<h3 id="bench---performance-benchmarking"><a class="header" href="#bench---performance-benchmarking">bench - Performance Benchmarking</a></h3>
<p>Run performance benchmarks to measure prefill and decode speeds.</p>
<pre><code class="language-bash">mistralrs bench [MODEL_TYPE] -m &lt;MODEL_ID&gt; [OPTIONS]
</code></pre>
<p>Note: <code>MODEL_TYPE</code> is optional and defaults to <code>auto</code> if not specified.</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Run default benchmark (512 prompt tokens, 128 generated tokens, 3 iterations)
mistralrs bench -m Qwen/Qwen3-4B

# Custom prompt and generation lengths
mistralrs bench -m Qwen/Qwen3-4B --prompt-len 1024 --gen-len 256

# More iterations for better statistics
mistralrs bench -m Qwen/Qwen3-4B --iterations 10

# With ISQ quantization
mistralrs bench -m Qwen/Qwen3-4B --isq q4k
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code>Benchmark Results
=================

Model: Qwen/Qwen3-4B
Iterations: 3

┌────────────────────────┬─────────────────┬─────────────────┐
│ Test                   │ T/s             │ Latency         │
├────────────────────────┼─────────────────┼─────────────────┤
│ Prefill (512 tokens)   │ 2847.3 ± 45.2   │ 179.82 ms (TTFT)│
│ Decode (128 tokens)    │ 87.4 ± 2.1      │ 11.44 ms/T      │
└────────────────────────┴─────────────────┴─────────────────┘
</code></pre>
<ul>
<li><strong>T/s</strong>: Tokens per second (throughput)</li>
<li><strong>Latency</strong>: For prefill, shows TTFT (Time To First Token) in milliseconds. For decode, shows ms per token.</li>
</ul>
<p><strong>Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--prompt-len &lt;N&gt;</code></td><td><code>512</code></td><td>Number of tokens in prompt (prefill test)</td></tr>
<tr><td><code>--gen-len &lt;N&gt;</code></td><td><code>128</code></td><td>Number of tokens to generate (decode test)</td></tr>
<tr><td><code>--iterations &lt;N&gt;</code></td><td><code>3</code></td><td>Number of benchmark iterations</td></tr>
<tr><td><code>--warmup &lt;N&gt;</code></td><td><code>1</code></td><td>Number of warmup runs (discarded)</td></tr>
</tbody>
</table>
</div>
<p>The <code>bench</code> command also accepts all model loading options (ISQ, device mapping, etc.).</p>
<hr>
<h3 id="from-config---toml-configuration"><a class="header" href="#from-config---toml-configuration">from-config - TOML Configuration</a></h3>
<p>Run the CLI from a TOML configuration file. This is the recommended way to run multiple models simultaneously, including models of different types (e.g., text + vision + embedding).</p>
<p>See <a href="#mistralrs-cli-toml-config">CLI_CONFIG.md</a> for full TOML configuration format details.</p>
<pre><code class="language-bash">mistralrs from-config --file &lt;PATH&gt;
</code></pre>
<p><strong>Example:</strong></p>
<pre><code class="language-bash">mistralrs from-config --file config.toml
</code></pre>
<p><strong>Multi-model example</strong> (config.toml):</p>
<pre><code class="language-toml">command = "serve"

[server]
port = 1234
ui = true

[[models]]
kind = "auto"
model_id = "Qwen/Qwen3-4B"

[[models]]
kind = "vision"
model_id = "google/gemma-3-4b-it"

[[models]]
kind = "embedding"
model_id = "google/embeddinggemma-300m"
</code></pre>
<hr>
<h3 id="completions---shell-completions"><a class="header" href="#completions---shell-completions">completions - Shell Completions</a></h3>
<p>Generate shell completions for your shell.</p>
<pre><code class="language-bash">mistralrs completions &lt;SHELL&gt;
</code></pre>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Generate bash completions
mistralrs completions bash &gt; ~/.local/share/bash-completion/completions/mistralrs

# Generate zsh completions
mistralrs completions zsh &gt; ~/.zfunc/_mistralrs

# Generate fish completions
mistralrs completions fish &gt; ~/.config/fish/completions/mistralrs.fish
</code></pre>
<p><strong>Supported Shells:</strong> <code>bash</code>, <code>zsh</code>, <code>fish</code>, <code>elvish</code>, <code>powershell</code></p>
<hr>
<h2 id="model-types"><a class="header" href="#model-types">Model Types</a></h2>
<h3 id="auto"><a class="header" href="#auto">auto</a></h3>
<p>Auto-detect model type. This is the recommended option for most models and is on by default simply by leaving out the explicit model type.</p>
<pre><code class="language-bash">mistralrs run -m Qwen/Qwen3-4B
mistralrs serve -m Qwen/Qwen3-4B
</code></pre>
<p>The <code>auto</code> type supports text, vision, and other model types through automatic detection.</p>
<h3 id="text"><a class="header" href="#text">text</a></h3>
<p>Explicit text generation model configuration.</p>
<pre><code class="language-bash">mistralrs run text -m Qwen/Qwen3-4B
mistralrs serve text -m Qwen/Qwen3-4B
</code></pre>
<h3 id="vision"><a class="header" href="#vision">vision</a></h3>
<p>Vision-language models that can process images and text.</p>
<pre><code class="language-bash">mistralrs run vision -m google/gemma-3-4b-it
mistralrs serve vision -m google/gemma-3-4b-it
</code></pre>
<p><strong>Vision Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--max-edge &lt;SIZE&gt;</code></td><td>Maximum edge length for image resizing (aspect ratio preserved)</td></tr>
<tr><td><code>--max-num-images &lt;N&gt;</code></td><td>Maximum number of images per request</td></tr>
<tr><td><code>--max-image-length &lt;SIZE&gt;</code></td><td>Maximum image dimension for device mapping</td></tr>
</tbody>
</table>
</div>
<h3 id="diffusion"><a class="header" href="#diffusion">diffusion</a></h3>
<p>Image generation models using diffusion.</p>
<pre><code class="language-bash">mistralrs run diffusion -m black-forest-labs/FLUX.1-schnell
mistralrs serve diffusion -m black-forest-labs/FLUX.1-schnell
</code></pre>
<h3 id="speech"><a class="header" href="#speech">speech</a></h3>
<p>Speech synthesis models.</p>
<pre><code class="language-bash">mistralrs run speech -m nari-labs/Dia-1.6B
mistralrs serve speech -m nari-labs/Dia-1.6B
</code></pre>
<h3 id="embedding"><a class="header" href="#embedding">embedding</a></h3>
<p>Text embedding models. These do not support interactive mode but can be used with the HTTP server.</p>
<pre><code class="language-bash">mistralrs serve embedding -m google/embeddinggemma-300m
</code></pre>
<hr>
<h2 id="features-1"><a class="header" href="#features-1">Features</a></h2>
<h3 id="isq-quantization"><a class="header" href="#isq-quantization">ISQ Quantization</a></h3>
<p>In-situ quantization (ISQ) reduces model memory usage by quantizing weights at load time. See <a href="#in-situ-quantization-isq">details about ISQ here</a>.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># Simple bit-width quantization
mistralrs run -m Qwen/Qwen3-4B --isq 4
mistralrs run -m Qwen/Qwen3-4B --isq 8

# GGML-style quantization
mistralrs run -m Qwen/Qwen3-4B --isq q4_0
mistralrs run -m Qwen/Qwen3-4B --isq q4_1
mistralrs run -m Qwen/Qwen3-4B --isq q4k
mistralrs run -m Qwen/Qwen3-4B --isq q5k
mistralrs run -m Qwen/Qwen3-4B --isq q6k
</code></pre>
<p><strong>ISQ Organization:</strong></p>
<pre><code class="language-bash"># Use MOQE organization for potentially better quality
mistralrs run -m Qwen/Qwen3-4B --isq q4k --isq-organization moqe
</code></pre>
<hr>
<h3 id="uqff-files"><a class="header" href="#uqff-files">UQFF Files</a></h3>
<p>UQFF (Unified Quantized File Format) provides pre-quantized model files for faster loading.</p>
<p><strong>Generate UQFF files:</strong></p>
<pre><code class="language-bash">mistralrs quantize -m Qwen/Qwen3-4B --isq q4k -o qwen3-4b-uqff/
</code></pre>
<p><strong>Load from UQFF:</strong></p>
<pre><code class="language-bash"># Specify just the first shard -- remaining shards are auto-discovered
mistralrs run -m Qwen/Qwen3-4B --from-uqff q4k-0.uqff
</code></pre>
<p><strong>Multiple UQFF files (semicolon-separated, for different quantizations in one load):</strong></p>
<pre><code class="language-bash">mistralrs run -m Qwen/Qwen3-4B --from-uqff "q4k-0.uqff;q8_0-0.uqff"
</code></pre>
<blockquote>
<p>Note: Shard auto-discovery means you no longer need to list every shard file. Specifying <code>q4k-0.uqff</code> will automatically find <code>q4k-1.uqff</code>, <code>q4k-2.uqff</code>, etc.</p>
</blockquote>
<hr>
<h3 id="pagedattention"><a class="header" href="#pagedattention">PagedAttention</a></h3>
<p>PagedAttention enables efficient memory management for the KV cache. It is automatically enabled on CUDA and disabled on Metal/CPU by default.</p>
<p><strong>Control PagedAttention:</strong></p>
<pre><code class="language-bash"># Auto mode (default): enabled on CUDA, disabled on Metal/CPU
mistralrs serve -m Qwen/Qwen3-4B --paged-attn auto

# Force enable
mistralrs serve -m Qwen/Qwen3-4B --paged-attn on

# Force disable
mistralrs serve -m Qwen/Qwen3-4B --paged-attn off
</code></pre>
<p><strong>Memory allocation options (mutually exclusive):</strong></p>
<pre><code class="language-bash"># Allocate for specific context length (recommended)
mistralrs serve -m Qwen/Qwen3-4B --pa-context-len 8192

# Allocate specific GPU memory in MB
mistralrs serve -m Qwen/Qwen3-4B --pa-memory-mb 4096

# Allocate fraction of GPU memory (0.0-1.0)
mistralrs serve -m Qwen/Qwen3-4B --pa-memory-fraction 0.8
</code></pre>
<p><strong>Additional options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--pa-block-size &lt;SIZE&gt;</code></td><td>Tokens per block (default: 32 on CUDA)</td></tr>
<tr><td><code>--pa-cache-type &lt;TYPE&gt;</code></td><td>KV cache quantization type (default: auto)</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="device-mapping"><a class="header" href="#device-mapping">Device Mapping</a></h3>
<p>Control how model layers are distributed across devices.</p>
<p><strong>Automatic mapping:</strong></p>
<pre><code class="language-bash"># Use defaults (automatic)
mistralrs run -m Qwen/Qwen3-4B
</code></pre>
<p><strong>Manual layer assignment:</strong></p>
<pre><code class="language-bash"># Assign 10 layers to GPU 0, 20 layers to GPU 1
mistralrs run -m Qwen/Qwen3-4B -n "0:10;1:20"

# Equivalent long form
mistralrs run -m Qwen/Qwen3-4B --device-layers "0:10;1:20"
</code></pre>
<p><strong>CPU-only execution:</strong></p>
<pre><code class="language-bash">mistralrs run -m Qwen/Qwen3-4B --cpu
</code></pre>
<p><strong>Topology file:</strong></p>
<pre><code class="language-bash">mistralrs run -m Qwen/Qwen3-4B --topology topology.yaml
</code></pre>
<p><strong>Custom HuggingFace cache:</strong></p>
<pre><code class="language-bash">mistralrs run -m Qwen/Qwen3-4B --hf-cache /path/to/cache
</code></pre>
<p><strong>Device mapping options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-n, --device-layers &lt;MAPPING&gt;</code></td><td>auto</td><td>Device layer mapping (format: <code>ORD:NUM;...</code>)</td></tr>
<tr><td><code>--topology &lt;PATH&gt;</code></td><td>none</td><td>Topology YAML file for device mapping</td></tr>
<tr><td><code>--hf-cache &lt;PATH&gt;</code></td><td>none</td><td>Custom HuggingFace cache directory</td></tr>
<tr><td><code>--cpu</code></td><td>disabled</td><td>Force CPU-only execution</td></tr>
<tr><td><code>--max-seq-len &lt;LEN&gt;</code></td><td><code>4096</code></td><td>Max sequence length for automatic device mapping</td></tr>
<tr><td><code>--max-batch-size &lt;SIZE&gt;</code></td><td><code>1</code></td><td>Max batch size for automatic device mapping</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="lora-and-x-lora"><a class="header" href="#lora-and-x-lora">LoRA and X-LoRA</a></h3>
<p>Apply LoRA or X-LoRA adapters to models.</p>
<p><strong>LoRA:</strong></p>
<pre><code class="language-bash"># Single LoRA adapter
mistralrs run -m Qwen/Qwen3-4B --lora my-lora-adapter

# Multiple LoRA adapters (semicolon-separated)
mistralrs run -m Qwen/Qwen3-4B --lora "adapter1;adapter2"
</code></pre>
<p><strong>X-LoRA:</strong></p>
<pre><code class="language-bash"># X-LoRA adapter with ordering file
mistralrs run -m Qwen/Qwen3-4B --xlora my-xlora-adapter --xlora-order ordering.json

# With target non-granular index
mistralrs run -m Qwen/Qwen3-4B --xlora my-xlora-adapter --xlora-order ordering.json --tgt-non-granular-index 2
</code></pre>
<hr>
<h3 id="chat-templates"><a class="header" href="#chat-templates">Chat Templates</a></h3>
<p>Override the model’s default chat template.</p>
<p><strong>Use a template file:</strong></p>
<pre><code class="language-bash"># JSON template file
mistralrs run -m Qwen/Qwen3-4B --chat-template template.json

# Jinja template file
mistralrs run -m Qwen/Qwen3-4B --chat-template template.jinja
</code></pre>
<p><strong>Explicit Jinja override:</strong></p>
<pre><code class="language-bash">mistralrs run -m Qwen/Qwen3-4B --jinja-explicit custom.jinja
</code></pre>
<hr>
<h3 id="web-search"><a class="header" href="#web-search">Web Search</a></h3>
<p>Enable web search capabilities (requires an embedding model).</p>
<pre><code class="language-bash"># Enable search with default embedding model
mistralrs run -m Qwen/Qwen3-4B --enable-search

# Specify embedding model
mistralrs run -m Qwen/Qwen3-4B --enable-search --search-embedding-model embedding-gemma
</code></pre>
<hr>
<h3 id="thinking-mode"><a class="header" href="#thinking-mode">Thinking Mode</a></h3>
<p>Enable thinking/reasoning mode for models that support it (like DeepSeek, Qwen3).</p>
<pre><code class="language-bash">mistralrs run -m Qwen/Qwen3-4B --enable-thinking
</code></pre>
<p>In interactive mode, thinking content is displayed in gray text before the final response.</p>
<hr>
<h2 id="global-options"><a class="header" href="#global-options">Global Options</a></h2>
<p>These options apply to all commands.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--seed &lt;SEED&gt;</code></td><td>none</td><td>Random seed for reproducibility</td></tr>
<tr><td><code>-l, --log &lt;PATH&gt;</code></td><td>none</td><td>Log all requests and responses to file</td></tr>
<tr><td><code>--token-source &lt;SOURCE&gt;</code></td><td><code>cache</code></td><td>HuggingFace authentication token source</td></tr>
<tr><td><code>-V, --version</code></td><td>N/A</td><td>Print version information and exit</td></tr>
<tr><td><code>-h, --help</code></td><td>N/A</td><td>Print help message (use with any subcommand)</td></tr>
</tbody>
</table>
</div>
<p><strong>Token source formats:</strong></p>
<ul>
<li><code>cache</code> - Use cached HuggingFace token (default)</li>
<li><code>literal:&lt;token&gt;</code> - Use literal token value</li>
<li><code>env:&lt;var&gt;</code> - Read token from environment variable</li>
<li><code>path:&lt;file&gt;</code> - Read token from file</li>
<li><code>none</code> - No authentication</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Set random seed
mistralrs run -m Qwen/Qwen3-4B --seed 42

# Enable logging
mistralrs run -m Qwen/Qwen3-4B --log requests.log

# Use token from environment variable
mistralrs run -m meta-llama/Llama-3.2-3B-Instruct --token-source env:HF_TOKEN
</code></pre>
<hr>
<h2 id="runtime-options"><a class="header" href="#runtime-options">Runtime Options</a></h2>
<p>These options are available for both <code>run</code> and <code>serve</code> commands.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--max-seqs &lt;N&gt;</code></td><td><code>32</code></td><td>Maximum concurrent sequences</td></tr>
<tr><td><code>--no-kv-cache</code></td><td>disabled</td><td>Disable KV cache entirely</td></tr>
<tr><td><code>--prefix-cache-n &lt;N&gt;</code></td><td><code>16</code></td><td>Number of prefix caches to hold (0 to disable)</td></tr>
<tr><td><code>-c, --chat-template &lt;PATH&gt;</code></td><td>none</td><td>Custom chat template file (.json or .jinja)</td></tr>
<tr><td><code>-j, --jinja-explicit &lt;PATH&gt;</code></td><td>none</td><td>Explicit JINJA template override</td></tr>
<tr><td><code>--enable-search</code></td><td>disabled</td><td>Enable web search</td></tr>
<tr><td><code>--search-embedding-model &lt;MODEL&gt;</code></td><td>none</td><td>Embedding model for search</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="model-source-options"><a class="header" href="#model-source-options">Model Source Options</a></h2>
<p>These options are common across model types.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-m, --model-id &lt;ID&gt;</code></td><td>HuggingFace model ID or local path (required)</td></tr>
<tr><td><code>-t, --tokenizer &lt;PATH&gt;</code></td><td>Path to local tokenizer.json file</td></tr>
<tr><td><code>-a, --arch &lt;ARCH&gt;</code></td><td>Model architecture (auto-detected if not specified)</td></tr>
<tr><td><code>--dtype &lt;TYPE&gt;</code></td><td>Model data type (default: <code>auto</code>)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="format-options"><a class="header" href="#format-options">Format Options</a></h2>
<p>For loading quantized models.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--format &lt;FORMAT&gt;</code></td><td>Model format: <code>plain</code>, <code>gguf</code>, or <code>ggml</code> (auto-detected)</td></tr>
<tr><td><code>-f, --quantized-file &lt;FILE&gt;</code></td><td>Quantized model filename(s) for GGUF/GGML (semicolon-separated)</td></tr>
<tr><td><code>--tok-model-id &lt;ID&gt;</code></td><td>Model ID for tokenizer when using quantized format</td></tr>
<tr><td><code>--gqa &lt;VALUE&gt;</code></td><td>GQA value for GGML models (default: 1)</td></tr>
</tbody>
</table>
</div>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Load a GGUF model
mistralrs run -m Qwen/Qwen3-4B --format gguf -f model.gguf

# Multiple GGUF files
mistralrs run -m Qwen/Qwen3-4B --format gguf -f "model-part1.gguf;model-part2.gguf"
</code></pre>
<hr>
<h2 id="interactive-commands"><a class="header" href="#interactive-commands">Interactive Commands</a></h2>
<p>When running in interactive mode (<code>mistralrs run</code>), the following commands are available:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>\help</code></td><td>Display help message</td></tr>
<tr><td><code>\exit</code></td><td>Quit interactive mode</td></tr>
<tr><td><code>\system &lt;message&gt;</code></td><td>Add a system message without running the model</td></tr>
<tr><td><code>\clear</code></td><td>Clear the chat history</td></tr>
<tr><td><code>\temperature &lt;float&gt;</code></td><td>Set sampling temperature (0.0 to 2.0)</td></tr>
<tr><td><code>\topk &lt;int&gt;</code></td><td>Set top-k sampling value (&gt;0)</td></tr>
<tr><td><code>\topp &lt;float&gt;</code></td><td>Set top-p sampling value (0.0 to 1.0)</td></tr>
</tbody>
</table>
</div>
<p><strong>Examples:</strong></p>
<pre><code>&gt; \system Always respond as a pirate.
&gt; \temperature 0.7
&gt; \topk 50
&gt; Hello!
Ahoy there, matey! What brings ye to these waters?
&gt; \clear
&gt; \exit
</code></pre>
<p><strong>Vision Model Interactive Mode:</strong></p>
<p>For vision models, you can include images in your prompts by specifying file paths or URLs:</p>
<pre><code>&gt; Describe this image: /path/to/image.jpg
&gt; Compare these images: image1.png image2.png
&gt; Describe the image and transcribe the audio: photo.jpg recording.mp3
</code></pre>
<p><strong>Note</strong>: The CLI automatically detects paths to supported image and audio files within your prompt. You do not need special syntax; simply paste the absolute or relative path to the file.</p>
<p>Supported image formats: PNG, JPEG, BMP, GIF, WebP
Supported audio formats: WAV, MP3, FLAC, OGG</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mistralrs-cli-toml-config"><a class="header" href="#mistralrs-cli-toml-config">mistralrs-cli TOML Config</a></h1>
<p><code>mistralrs-cli</code> can run entirely from a single TOML configuration file. This config supports multiple models and mirrors the CLI options.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<pre><code class="language-bash">mistralrs from-config --file path/to/config.toml
</code></pre>
<h2 id="quick-example"><a class="header" href="#quick-example">Quick Example</a></h2>
<pre><code class="language-toml">command = "serve"

[server]
port = 1234
ui = true

[runtime]
max_seqs = 32

[[models]]
kind = "auto"
model_id = "Qwen/Qwen3-4B"

[models.quantization]
in_situ_quant = "q4k"
</code></pre>
<h2 id="complete-reference"><a class="header" href="#complete-reference">Complete Reference</a></h2>
<h3 id="top-level-options"><a class="header" href="#top-level-options">Top-Level Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Commands</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>command</code></td><td>all</td><td>Required. Either <code>"serve"</code> or <code>"run"</code></td></tr>
<tr><td><code>enable_thinking</code></td><td>run</td><td>Enable thinking mode (default: false)</td></tr>
<tr><td><code>default_model_id</code></td><td>serve</td><td>Default model ID for API requests (must match a model_id in [[models]])</td></tr>
</tbody>
</table>
</div>
<h3 id="global-section"><a class="header" href="#global-section">[global] Section</a></h3>
<p>Global options that apply to the entire run.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>seed</code></td><td>none</td><td>Random seed for reproducibility</td></tr>
<tr><td><code>log</code></td><td>none</td><td>Log all requests/responses to this file path</td></tr>
<tr><td><code>token_source</code></td><td><code>"cache"</code></td><td>HuggingFace auth: <code>"cache"</code>, <code>"none"</code>, <code>"literal:&lt;token&gt;"</code>, <code>"env:&lt;var&gt;"</code>, <code>"path:&lt;file&gt;"</code></td></tr>
</tbody>
</table>
</div>
<h3 id="server-section-serve-only"><a class="header" href="#server-section-serve-only">[server] Section (serve only)</a></h3>
<p>HTTP server configuration.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>port</code></td><td><code>1234</code></td><td>HTTP server port</td></tr>
<tr><td><code>host</code></td><td><code>"0.0.0.0"</code></td><td>Bind address</td></tr>
<tr><td><code>ui</code></td><td><code>false</code></td><td>Serve built-in web UI at <code>/ui</code></td></tr>
<tr><td><code>mcp_port</code></td><td>none</td><td>MCP protocol server port (enables MCP if set)</td></tr>
<tr><td><code>mcp_config</code></td><td>none</td><td>MCP client configuration file path</td></tr>
</tbody>
</table>
</div>
<h3 id="runtime-section"><a class="header" href="#runtime-section">[runtime] Section</a></h3>
<p>Runtime inference options.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>max_seqs</code></td><td><code>32</code></td><td>Maximum concurrent sequences</td></tr>
<tr><td><code>no_kv_cache</code></td><td><code>false</code></td><td>Disable KV cache entirely</td></tr>
<tr><td><code>prefix_cache_n</code></td><td><code>16</code></td><td>Number of prefix caches to hold (0 to disable)</td></tr>
<tr><td><code>chat_template</code></td><td>none</td><td>Custom chat template file (.json or .jinja)</td></tr>
<tr><td><code>jinja_explicit</code></td><td>none</td><td>Explicit JINJA template override</td></tr>
<tr><td><code>enable_search</code></td><td><code>false</code></td><td>Enable web search</td></tr>
<tr><td><code>search_embedding_model</code></td><td>none</td><td>Embedding model for search (e.g., <code>"embedding-gemma"</code>)</td></tr>
</tbody>
</table>
</div>
<h3 id="paged_attn-section"><a class="header" href="#paged_attn-section">[paged_attn] Section</a></h3>
<p>PagedAttention configuration.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>mode</code></td><td><code>"auto"</code></td><td><code>"auto"</code> (CUDA on, Metal off), <code>"on"</code>, or <code>"off"</code></td></tr>
<tr><td><code>context_len</code></td><td>none</td><td>Allocate KV cache for this context length</td></tr>
<tr><td><code>memory_mb</code></td><td>none</td><td>GPU memory to allocate in MB (conflicts with context_len)</td></tr>
<tr><td><code>memory_fraction</code></td><td>none</td><td>GPU memory utilization 0.0-1.0 (conflicts with above)</td></tr>
<tr><td><code>block_size</code></td><td><code>32</code></td><td>Tokens per block</td></tr>
<tr><td><code>cache_type</code></td><td><code>"auto"</code></td><td>KV cache type</td></tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> If none of <code>context_len</code>, <code>memory_mb</code>, or <code>memory_fraction</code> are specified, defaults to 90% of available VRAM. Each are mutually exclusive.</p>
<h3 id="models-section"><a class="header" href="#models-section">[[models]] Section</a></h3>
<p>Define one or more models. Each <code>[[models]]</code> entry creates a new model.</p>
<h4 id="top-level-model-options"><a class="header" href="#top-level-model-options">Top-Level Model Options</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Required</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>kind</code></td><td>yes</td><td>Model type: <code>"auto"</code>, <code>"text"</code>, <code>"vision"</code>, <code>"diffusion"</code>, <code>"speech"</code>, <code>"embedding"</code></td></tr>
<tr><td><code>model_id</code></td><td>yes</td><td>HuggingFace model ID or local path</td></tr>
<tr><td><code>tokenizer</code></td><td>no</td><td>Path to local tokenizer.json</td></tr>
<tr><td><code>arch</code></td><td>no</td><td>Model architecture (auto-detected if not specified)</td></tr>
<tr><td><code>dtype</code></td><td><code>"auto"</code></td><td>Data type: <code>"auto"</code>, <code>"f16"</code>, <code>"bf16"</code>, <code>"f32"</code></td></tr>
<tr><td><code>chat_template</code></td><td>no</td><td>Per-model chat template override</td></tr>
<tr><td><code>jinja_explicit</code></td><td>no</td><td>Per-model JINJA template override</td></tr>
</tbody>
</table>
</div>
<h4 id="modelsformat---model-format"><a class="header" href="#modelsformat---model-format">[models.format] - Model Format</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>format</code></td><td>auto</td><td><code>"plain"</code> (safetensors), <code>"gguf"</code>, or <code>"ggml"</code></td></tr>
<tr><td><code>quantized_file</code></td><td>none</td><td>Quantized filename(s) for GGUF/GGML (semicolon-separated)</td></tr>
<tr><td><code>tok_model_id</code></td><td>none</td><td>Model ID for tokenizer when using quantized format</td></tr>
<tr><td><code>gqa</code></td><td><code>1</code></td><td>GQA value for GGML models</td></tr>
</tbody>
</table>
</div>
<h4 id="modelsadapter---lorax-lora"><a class="header" href="#modelsadapter---lorax-lora">[models.adapter] - LoRA/X-LoRA</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>lora</code></td><td>LoRA adapter ID(s), semicolon-separated</td></tr>
<tr><td><code>xlora</code></td><td>X-LoRA adapter ID (conflicts with lora)</td></tr>
<tr><td><code>xlora_order</code></td><td>X-LoRA ordering JSON file (requires xlora)</td></tr>
<tr><td><code>tgt_non_granular_index</code></td><td>Target non-granular index for X-LoRA</td></tr>
</tbody>
</table>
</div>
<h4 id="modelsquantization---isquqff"><a class="header" href="#modelsquantization---isquqff">[models.quantization] - ISQ/UQFF</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>in_situ_quant</code></td><td>ISQ level: <code>"4"</code>, <code>"8"</code>, <code>"q4_0"</code>, <code>"q4k"</code>, <code>"q6k"</code>, etc.</td></tr>
<tr><td><code>from_uqff</code></td><td>UQFF file(s) to load (semicolon-separated). Shards are auto-discovered: specifying the first shard (e.g., <code>q4k-0.uqff</code>) automatically finds <code>q4k-1.uqff</code>, etc.</td></tr>
<tr><td><code>isq_organization</code></td><td>ISQ strategy: <code>"default"</code> or <code>"moqe"</code></td></tr>
<tr><td><code>imatrix</code></td><td>imatrix file for enhanced quantization</td></tr>
<tr><td><code>calibration_file</code></td><td>Calibration file for imatrix generation</td></tr>
</tbody>
</table>
</div>
<h4 id="modelsdevice---device-mapping"><a class="header" href="#modelsdevice---device-mapping">[models.device] - Device Mapping</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>cpu</code></td><td><code>false</code></td><td>Force CPU-only (must be consistent across all models)</td></tr>
<tr><td><code>device_layers</code></td><td>auto</td><td>Layer mapping, e.g., <code>["0:10", "1:20"]</code> format: <code>ORD:NUM;...</code></td></tr>
<tr><td><code>topology</code></td><td>none</td><td>Topology YAML file</td></tr>
<tr><td><code>hf_cache</code></td><td>none</td><td>Custom HuggingFace cache directory</td></tr>
<tr><td><code>max_seq_len</code></td><td><code>4096</code></td><td>Max sequence length for auto device mapping</td></tr>
<tr><td><code>max_batch_size</code></td><td><code>1</code></td><td>Max batch size for auto device mapping</td></tr>
</tbody>
</table>
</div>
<h4 id="modelsvision---vision-options"><a class="header" href="#modelsvision---vision-options">[models.vision] - Vision Options</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>max_edge</code></td><td>Maximum edge length for image resizing</td></tr>
<tr><td><code>max_num_images</code></td><td>Maximum images per request</td></tr>
<tr><td><code>max_image_length</code></td><td>Maximum image dimension for device mapping</td></tr>
</tbody>
</table>
</div>
<h2 id="full-examples"><a class="header" href="#full-examples">Full Examples</a></h2>
<h3 id="multi-model-server-with-ui"><a class="header" href="#multi-model-server-with-ui">Multi-Model Server with UI</a></h3>
<pre><code class="language-toml">command = "serve"

[global]
seed = 42

[server]
host = "0.0.0.0"
port = 1234
ui = true

[runtime]
max_seqs = 32
enable_search = true
search_embedding_model = "embedding-gemma"

[paged_attn]
mode = "auto"

[[models]]
kind = "auto"
model_id = "meta-llama/Llama-3.2-3B-Instruct"
dtype = "auto"

[models.quantization]
in_situ_quant = "q4k"

[[models]]
kind = "vision"
model_id = "Qwen/Qwen2-VL-2B-Instruct"

[models.vision]
max_num_images = 4

[[models]]
kind = "embedding"
model_id = "google/embeddinggemma-300m"
</code></pre>
<h3 id="interactive-mode-with-thinking"><a class="header" href="#interactive-mode-with-thinking">Interactive Mode with Thinking</a></h3>
<pre><code class="language-toml">command = "run"
enable_thinking = true

[runtime]
max_seqs = 16

[[models]]
kind = "auto"
model_id = "Qwen/Qwen3-4B"
</code></pre>
<h3 id="gguf-model"><a class="header" href="#gguf-model">GGUF Model</a></h3>
<pre><code class="language-toml">command = "serve"

[server]
port = 1234

[[models]]
kind = "text"
model_id = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"

[models.format]
format = "gguf"
quantized_file = "mistral-7b-instruct-v0.1.Q4_K_M.gguf"
tok_model_id = "mistralai/Mistral-7B-Instruct-v0.1"
</code></pre>
<h3 id="device-layer-mapping"><a class="header" href="#device-layer-mapping">Device Layer Mapping</a></h3>
<pre><code class="language-toml">command = "serve"

[[models]]
kind = "auto"
model_id = "meta-llama/Llama-3.1-70B-Instruct"

[models.device]
device_layers = ["0:40", "1:40"]

[models.quantization]
in_situ_quant = "q4k"
</code></pre>
<h2 id="notes"><a class="header" href="#notes">Notes</a></h2>
<ul>
<li><code>cpu</code> must be consistent across all models if specified</li>
<li><code>default_model_id</code> (serve only) must match a <code>model_id</code> in <code>[[models]]</code></li>
<li><code>search_embedding_model</code> requires <code>enable_search = true</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h1>
<p>Common issues and solutions for mistral.rs.</p>
<h2 id="debug-mode"><a class="header" href="#debug-mode">Debug Mode</a></h2>
<p>Enable debug mode for more information:</p>
<pre><code class="language-bash">MISTRALRS_DEBUG=1 mistralrs run -m &lt;model&gt;
</code></pre>
<p>Debug mode causes:</p>
<ul>
<li>If loading a GGUF or GGML model, outputs a file containing the names, shapes, and types of each tensor:
<ul>
<li><code>mistralrs_gguf_tensors.txt</code> or <code>mistralrs_ggml_tensors.txt</code></li>
</ul>
</li>
<li>Increased logging verbosity</li>
</ul>
<h2 id="system-diagnostics"><a class="header" href="#system-diagnostics">System Diagnostics</a></h2>
<p>Run the built-in diagnostics tool:</p>
<pre><code class="language-bash">mistralrs doctor
</code></pre>
<p>This checks your system configuration and reports any issues.</p>
<h2 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h2>
<h3 id="cuda-issues"><a class="header" href="#cuda-issues">CUDA Issues</a></h3>
<p><strong>Setting the CUDA compiler path:</strong></p>
<ul>
<li>Set the <code>NVCC_CCBIN</code> environment variable during build</li>
</ul>
<p><strong>Error: <code>recompile with -fPIE</code>:</strong></p>
<ul>
<li>Some Linux distributions require compiling with <code>-fPIE</code></li>
<li>Set during build: <code>CUDA_NVCC_FLAGS=-fPIE cargo build --release --features cuda</code></li>
</ul>
<p><strong>Error: <code>CUDA_ERROR_NOT_FOUND</code> or symbol not found:</strong></p>
<ul>
<li>For non-quantized models, specify the data type to load and run in</li>
<li>Use one of <code>f32</code>, <code>f16</code>, <code>bf16</code> or <code>auto</code> (auto chooses based on device)</li>
<li>Example: <code>mistralrs run -m &lt;model&gt; -d auto</code></li>
</ul>
<p><strong>Minimum CUDA compute capability:</strong></p>
<ul>
<li>The minimum supported CUDA compute cap is <strong>5.3</strong></li>
<li>Set a specific compute cap with: <code>CUDA_COMPUTE_CAP=80 cargo build --release --features cuda</code></li>
</ul>
<h3 id="metal-issues-macos"><a class="header" href="#metal-issues-macos">Metal Issues (macOS)</a></h3>
<p><strong>Metal not found (error: unable to find utility “metal”):</strong></p>
<ol>
<li>
<p>Install Xcode:</p>
<pre><code class="language-bash">xcode-select --install
</code></pre>
</li>
<li>
<p>Set the active developer directory:</p>
<pre><code class="language-bash">sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer
</code></pre>
</li>
</ol>
<p><strong>error: cannot execute tool ‘metal’ due to missing Metal toolchain</strong></p>
<ol>
<li>Install Metal Toolchain:
<pre><code class="language-bash">xcodebuild -downloadComponent MetalToolchain
</code></pre>
</li>
</ol>
<p><strong>Disabling Metal kernel precompilation:</strong></p>
<ul>
<li>By default, Metal kernels are precompiled during build time for better performance</li>
<li>To skip precompilation (useful for CI or when Metal is not needed):
<pre><code class="language-bash">MISTRALRS_METAL_PRECOMPILE=0 cargo build --release --features metal
</code></pre>
</li>
</ul>
<h3 id="memory-issues"><a class="header" href="#memory-issues">Memory Issues</a></h3>
<p><strong>Disabling mmap loading:</strong></p>
<ul>
<li>Set <code>MISTRALRS_NO_MMAP=1</code> to disable memory-mapped file loading</li>
<li>Forces all tensor data into memory</li>
<li>Useful if you’re seeing mmap-related errors</li>
</ul>
<p><strong>Out of memory errors:</strong></p>
<ul>
<li>Try using quantization: <code>--isq q4k</code> or <code>--isq q8_0</code></li>
<li>Use device mapping to offload layers: <code>-n 0:16;cpu:16</code></li>
<li>Reduce context length with PagedAttention: <code>--pa-context-len 4096</code></li>
</ul>
<h3 id="model-loading-issues"><a class="header" href="#model-loading-issues">Model Loading Issues</a></h3>
<p><strong>Model type not auto-detected:</strong></p>
<ul>
<li>If auto-detection fails, please <a href="https://github.com/EricLBuehler/mistral.rs/issues">raise an issue</a></li>
<li>You can manually specify the architecture if needed</li>
</ul>
<p><strong>Chat template issues:</strong></p>
<ul>
<li>Templates are usually auto-detected</li>
<li>Override with: <code>-c /path/to/template.jinja</code></li>
<li>See <a href="#chat-templates-and-tokenizer-customization">Chat Templates</a> for details</li>
</ul>
<h2 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h2>
<p>If you’re still stuck:</p>
<ul>
<li><a href="https://discord.gg/SZrecqK8qw">Discord</a> - Community support</li>
<li><a href="https://matrix.to/#/#mistral.rs:matrix.org">Matrix</a> - Alternative chat</li>
<li><a href="https://github.com/EricLBuehler/mistral.rs/issues">GitHub Issues</a> - Bug reports and feature requests</li>
</ul>
<p>When reporting issues, please include:</p>
<ol>
<li>Output of <code>mistralrs doctor</code></li>
<li>Full error message</li>
<li>Command you ran</li>
<li>Hardware (GPU model, OS)</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mistralrs-rust-sdk"><a class="header" href="#mistralrs-rust-sdk">mistralrs Rust SDK</a></h1>
<p>The <code>mistralrs</code> crate provides a high-level Rust API for running LLM inference with mistral.rs.</p>
<blockquote>
<p><strong>Full API reference:</strong> <a href="https://docs.rs/mistralrs">docs.rs/mistralrs</a></p>
</blockquote>
<p><strong>Table of contents</strong></p>
<ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#model-builders">Model Builders</a></li>
<li><a href="#request-types">Request Types</a></li>
<li><a href="#streaming">Streaming</a></li>
<li><a href="#structured-output">Structured Output</a></li>
<li><a href="#tool-calling">Tool Calling</a></li>
<li><a href="#agents">Agents</a></li>
<li><a href="#blocking-api">Blocking API</a></li>
<li><a href="#feature-flags">Feature Flags</a></li>
<li><a href="#examples">Examples</a></li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<pre><code class="language-bash">cargo add mistralrs
</code></pre>
<p>Or in your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
mistralrs = "0.7"
</code></pre>
<p>For GPU acceleration, enable the appropriate feature:</p>
<pre><code class="language-toml">mistralrs = { version = "0.7", features = ["metal"] }     # macOS
mistralrs = { version = "0.7", features = ["cuda"] }       # NVIDIA
</code></pre>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<pre class="playground"><code class="language-rust">use mistralrs::{IsqBits, ModelBuilder, TextMessages, TextMessageRole};

#[tokio::main]
async fn main() -&gt; mistralrs::error::Result&lt;()&gt; {
    let model = ModelBuilder::new("Qwen/Qwen3-4B")
        .with_auto_isq(IsqBits::Four)
        .build()
        .await?;

    let response = model.chat("What is Rust's ownership model?").await?;
    println!("{response}");
    Ok(())
}</code></pre>
<h2 id="model-builders"><a class="header" href="#model-builders">Model Builders</a></h2>
<p>All models are created through builder structs. Use <code>ModelBuilder</code> for auto-detection, or a specific builder for more control.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Builder</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td><code>ModelBuilder</code></td><td>Auto-detects model type (text, vision, embedding)</td></tr>
<tr><td><code>TextModelBuilder</code></td><td>Text generation models</td></tr>
<tr><td><code>VisionModelBuilder</code></td><td>Vision + text models (image/audio input)</td></tr>
<tr><td><code>GgufModelBuilder</code></td><td>GGUF quantized model files</td></tr>
<tr><td><code>EmbeddingModelBuilder</code></td><td>Text embedding models</td></tr>
<tr><td><code>DiffusionModelBuilder</code></td><td>Image generation (e.g., FLUX)</td></tr>
<tr><td><code>SpeechModelBuilder</code></td><td>Speech synthesis (e.g., Dia)</td></tr>
<tr><td><code>LoraModelBuilder</code></td><td>Text model with LoRA adapters</td></tr>
<tr><td><code>XLoraModelBuilder</code></td><td>Text model with X-LoRA adapters</td></tr>
<tr><td><code>AnyMoeModelBuilder</code></td><td>AnyMoE Mixture of Experts</td></tr>
<tr><td><code>TextSpeculativeBuilder</code></td><td>Speculative decoding (target + draft)</td></tr>
</tbody>
</table>
</div>
<p>All builders share common configuration methods:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let model = TextModelBuilder::new("Qwen/Qwen3-4B")
    .with_auto_isq(IsqBits::Four)      // Platform-optimal quantization
    .with_logging()                      // Enable logging
    .with_paged_attn(                    // PagedAttention for memory efficiency
        PagedAttentionMetaBuilder::default().build()?
    )
    .build()
    .await?;
<span class="boring">}</span></code></pre>
<p>Key builder methods include <code>with_isq()</code>, <code>with_auto_isq()</code>, <code>with_dtype()</code>, <code>with_force_cpu()</code>, <code>with_device_mapping()</code>, <code>with_chat_template()</code>, <code>with_paged_attn()</code>, <code>with_max_num_seqs()</code>, <code>with_mcp_client()</code>, and more. See the <a href="https://docs.rs/mistralrs">API docs</a> for the full list.</p>
<h2 id="request-types"><a class="header" href="#request-types">Request Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Use When</th><th>Sampling</th></tr>
</thead>
<tbody>
<tr><td><code>TextMessages</code></td><td>Simple text-only chat</td><td>Deterministic</td></tr>
<tr><td><code>VisionMessages</code></td><td>Prompt includes images or audio</td><td>Deterministic</td></tr>
<tr><td><code>RequestBuilder</code></td><td>Tools, logprobs, custom sampling, constraints, or web search</td><td>Configurable</td></tr>
</tbody>
</table>
</div>
<p><code>TextMessages</code> and <code>VisionMessages</code> convert into <code>RequestBuilder</code> via <code>Into&lt;RequestBuilder&gt;</code> if you start simple and later need more control.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simple
let messages = TextMessages::new()
    .add_message(TextMessageRole::User, "Hello!");
let response = model.send_chat_request(messages).await?;

// Advanced
let request = RequestBuilder::new()
    .add_message(TextMessageRole::System, "You are helpful.")
    .add_message(TextMessageRole::User, "Hello!")
    .set_tools(tools)
    .with_sampling(SamplingParams { temperature: Some(0.7), ..Default::default() });
let response = model.send_chat_request(request).await?;
<span class="boring">}</span></code></pre>
<h2 id="streaming"><a class="header" href="#streaming">Streaming</a></h2>
<p><code>Model::stream_chat_request</code> returns a <code>Stream</code> that implements <code>futures::Stream</code>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::StreamExt;
use mistralrs::*;

let mut stream = model.stream_chat_request(messages).await?;
while let Some(chunk) = stream.next().await {
    if let Response::Chunk(c) = chunk {
        if let Some(text) = c.choices.first().and_then(|ch| ch.delta.content.as_ref()) {
            print!("{text}");
        }
    }
}
<span class="boring">}</span></code></pre>
<h2 id="structured-output"><a class="header" href="#structured-output">Structured Output</a></h2>
<p>Derive <code>schemars::JsonSchema</code> on your type and the model will be constrained to produce valid JSON:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mistralrs::*;
use schemars::JsonSchema;
use serde::Deserialize;

#[derive(Deserialize, JsonSchema)]
struct City {
    name: String,
    country: String,
    population: u64,
}

let messages = TextMessages::new()
    .add_message(TextMessageRole::User, "Give me info about Paris.");

let city: City = model.generate_structured::&lt;City&gt;(messages).await?;
println!("{}: pop. {}", city.name, city.population);
<span class="boring">}</span></code></pre>
<h2 id="tool-calling"><a class="header" href="#tool-calling">Tool Calling</a></h2>
<h3 id="manual-tool-definition"><a class="header" href="#manual-tool-definition">Manual tool definition</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let tools = vec![Tool {
    tp: ToolType::Function,
    function: Function {
        description: Some("Get the weather for a location".to_string()),
        name: "get_weather".to_string(),
        parameters: Some(parameters_json),
    },
}];

let request = RequestBuilder::new()
    .add_message(TextMessageRole::User, "What's the weather in NYC?")
    .set_tools(tools);

let response = model.send_chat_request(request).await?;
<span class="boring">}</span></code></pre>
<h3 id="using-the-tool-macro"><a class="header" href="#using-the-tool-macro">Using the <code>#[tool]</code> macro</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mistralrs::tool;

#[tool(description = "Get the current weather for a location")]
fn get_weather(
    #[description = "The city name"] city: String,
) -&gt; Result&lt;String&gt; {
    Ok(format!("Sunny, 72F in {city}"))
}
<span class="boring">}</span></code></pre>
<p>See <a href="#tool-calling-1">Tool Calling</a> for full details, or the <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/mistralrs/examples/advanced/tools"><code>examples/advanced/tools/</code></a> example.</p>
<h2 id="agents"><a class="header" href="#agents">Agents</a></h2>
<p><code>AgentBuilder</code> wraps the tool-calling loop, automatically dispatching tool calls and feeding results back:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mistralrs::*;

let agent = AgentBuilder::new(model)
    .with_system_prompt("You are a helpful assistant with tools.")
    .with_sync_tool(get_weather_tool, get_weather_callback)
    .with_max_iterations(10)
    .build();

let response = agent.run("What's the weather in NYC and London?").await?;
println!("{}", response.final_text);
<span class="boring">}</span></code></pre>
<p>See the <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/mistralrs/examples/advanced/agent"><code>examples/advanced/agent/</code></a> example for streaming agents and the <code>#[tool]</code> macro.</p>
<h2 id="blocking-api"><a class="header" href="#blocking-api">Blocking API</a></h2>
<p>For non-async applications, use <code>BlockingModel</code>:</p>
<pre class="playground"><code class="language-rust">use mistralrs::blocking::BlockingModel;
use mistralrs::{IsqBits, ModelBuilder};

fn main() -&gt; mistralrs::error::Result&lt;()&gt; {
    let model = BlockingModel::from_builder(
        ModelBuilder::new("Qwen/Qwen3-4B")
            .with_auto_isq(IsqBits::Four),
    )?;
    let answer = model.chat("What is 2+2?")?;
    println!("{answer}");
    Ok(())
}</code></pre>
<blockquote>
<p><strong>Note:</strong> <code>BlockingModel</code> creates its own tokio runtime. Do not call it from within an existing tokio runtime.</p>
</blockquote>
<h2 id="feature-flags"><a class="header" href="#feature-flags">Feature Flags</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Effect</th></tr>
</thead>
<tbody>
<tr><td><code>cuda</code></td><td>CUDA GPU support</td></tr>
<tr><td><code>flash-attn</code></td><td>Flash Attention 2 kernels (requires <code>cuda</code>)</td></tr>
<tr><td><code>cudnn</code></td><td>cuDNN acceleration (requires <code>cuda</code>)</td></tr>
<tr><td><code>nccl</code></td><td>Multi-GPU via NCCL (requires <code>cuda</code>)</td></tr>
<tr><td><code>metal</code></td><td>Apple Metal GPU support</td></tr>
<tr><td><code>accelerate</code></td><td>Apple Accelerate framework</td></tr>
<tr><td><code>mkl</code></td><td>Intel MKL acceleration</td></tr>
</tbody>
</table>
</div>
<p>The default feature set (no flags) builds with pure Rust — no C compiler or system libraries required.</p>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<p>The crate includes 48 runnable examples organized by topic:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Examples</th></tr>
</thead>
<tbody>
<tr><td><strong>Getting Started</strong></td><td><code>text_generation</code>, <code>streaming</code>, <code>vision</code>, <code>gguf</code>, <code>gguf_locally</code>, <code>embedding</code></td></tr>
<tr><td><strong>Models</strong></td><td><code>text_models</code>, <code>vision_models</code>, <code>audio</code>, <code>diffusion</code>, <code>speech</code>, <code>multimodal</code></td></tr>
<tr><td><strong>Quantization</strong></td><td><code>isq</code>, <code>imatrix</code>, <code>uqff</code>, <code>topology</code>, <code>mixture_of_quant_experts</code></td></tr>
<tr><td><strong>Advanced</strong></td><td><code>tools</code>, <code>agent</code>, <code>grammar</code>, <code>json_schema</code>, <code>web_search</code>, <code>mcp_client</code>, <code>batching</code>, <code>paged_attn</code>, <code>speculative</code>, <code>lora</code>, <code>error_handling</code>, and more</td></tr>
<tr><td><strong>Cookbook</strong></td><td><code>cookbook_rag</code>, <code>cookbook_structured</code>, <code>cookbook_multiturn</code>, <code>cookbook_agent</code></td></tr>
</tbody>
</table>
</div>
<p>Run any example with:</p>
<pre><code class="language-bash">cargo run --release --features &lt;features&gt; --example &lt;name&gt;
</code></pre>
<p>Browse all examples: <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/mistralrs/examples"><code>mistralrs/examples/</code></a></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mistralrs-python-sdk"><a class="header" href="#mistralrs-python-sdk">mistralrs Python SDK</a></h1>
<p>Documentation for the <code>mistralrs</code> Python package.</p>
<blockquote>
<p><strong>Installation:</strong> See <a href="#python-sdk-installation">PYTHON_INSTALLATION.md</a> for installation instructions.</p>
</blockquote>
<p><strong>Table of contents</strong></p>
<ul>
<li>Full API reference: <a href="https://ericlbuehler.github.io/mistral.rs/pyo3/mistralrs.html">here</a></li>
<li>Model configuration (<code>Which</code> enum): <a href="#which">here</a></li>
<li>Multi-model support: <a href="#multi-model-support">here</a></li>
<li>MCP Client Configuration: <a href="#mcp-client">here</a></li>
<li>Example: <a href="#example">here</a></li>
<li>Embeddings example: <a href="#embeddings-example">here</a></li>
</ul>
<h2 id="which"><a class="header" href="#which"><code>Which</code></a></h2>
<p>Each <code>*_model_id</code> may be a HF hub repo or a local path. For quantized GGUF models, a list is accepted if multiple files must be specified.</p>
<h3 id="architecture-for-plain-models"><a class="header" href="#architecture-for-plain-models">Architecture for plain models</a></h3>
<p>If you do not specify the architecture, an attempt will be made to use the model’s config. If this fails, please raise an issue.</p>
<ul>
<li><code>Mistral</code></li>
<li><code>Gemma</code></li>
<li><code>Mixtral</code></li>
<li><code>Llama</code></li>
<li><code>Phi2</code></li>
<li><code>Phi3</code></li>
<li><code>Qwen2</code></li>
<li><code>Gemma2</code></li>
<li><code>GLM4</code></li>
<li><code>Starcoder2</code></li>
<li><code>Phi3_5MoE</code></li>
<li><code>DeepseekV2</code></li>
<li><code>DeepseekV3</code></li>
<li><code>Qwen3</code></li>
<li><code>Qwen3Moe</code></li>
<li><code>SmolLm3</code></li>
<li><code>GraniteMoeHybrid</code></li>
<li><code>GptOss</code></li>
</ul>
<h3 id="isq-organization"><a class="header" href="#isq-organization">ISQ Organization</a></h3>
<ul>
<li><code>Default</code></li>
<li><code>MoQE</code>: if applicable, only quantize MoE experts. https://arxiv.org/abs/2310.02410</li>
</ul>
<h3 id="architecture-for-vision-models"><a class="header" href="#architecture-for-vision-models">Architecture for vision models</a></h3>
<ul>
<li><code>Phi3V</code></li>
<li><code>Idefics2</code></li>
<li><code>LLaVaNext</code></li>
<li><code>LLaVa</code></li>
<li><code>VLlama</code></li>
<li><code>Qwen2VL</code></li>
<li><code>Idefics3</code></li>
<li><code>MiniCpmO</code></li>
<li><code>Phi4MM</code></li>
<li><code>Qwen2_5VL</code></li>
<li><code>Gemma3</code></li>
<li><code>Mistral3</code></li>
<li><code>Llama4</code></li>
<li><code>Gemma3n</code></li>
<li><code>Qwen3VL</code></li>
</ul>
<h3 id="architecture-for-diffusion-models"><a class="header" href="#architecture-for-diffusion-models">Architecture for diffusion models</a></h3>
<ul>
<li><code>Flux</code></li>
<li><code>FluxOffloaded</code></li>
</ul>
<h3 id="architecture-for-speech-models"><a class="header" href="#architecture-for-speech-models">Architecture for speech models</a></h3>
<ul>
<li><code>Dia</code></li>
</ul>
<h3 id="architecture-for-embedding-models"><a class="header" href="#architecture-for-embedding-models">Architecture for embedding models</a></h3>
<ul>
<li><code>EmbeddingGemma</code></li>
<li><code>Qwen3Embedding</code></li>
</ul>
<h3 id="isq-organization-1"><a class="header" href="#isq-organization-1">ISQ Organization</a></h3>
<ul>
<li><code>Default</code></li>
<li><code>MoQE</code>: if applicable, only quantize MoE experts. https://arxiv.org/abs/2310.02410</li>
</ul>
<blockquote>
<p>Note: <code>from_uqff</code> specifies a UQFF path to load from. If provided, this takes precedence over applying ISQ. For sharded models, you only need to specify the first shard (e.g., <code>q4k-0.uqff</code>) – the remaining shards are auto-discovered. For multiple different quantizations, use a semicolon delimiter (;).</p>
</blockquote>
<blockquote>
<p>Note: <code>enable_thinking</code> enables thinking for models that support the configuration.
Note: <code>truncate_sequence=True</code> trims prompts that would otherwise exceed the model’s maximum context length. Leave it <code>False</code> to receive a validation error instead.</p>
</blockquote>
<pre><code class="language-py">class Which(Enum):
    @dataclass
    class Plain:
        model_id: str
        arch: Architecture | None = None
        tokenizer_json: str | None = None
        topology: str | None = None
        organization: str | None = None
        from_uqff: str | list[str] | None = None
        write_uqff: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)
        calibration_file: str | None = None
        imatrix: str | None = None
        hf_cache_path: str | None = None

    @dataclass
    class XLora:
        xlora_model_id: str
        order: str
        arch: Architecture | None = None
        model_id: str | None = None
        tokenizer_json: str | None = None
        tgt_non_granular_index: int | None = None
        topology: str | None = None
        from_uqff: str | list[str] | None = None
        write_uqff: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)
        hf_cache_path: str | None = None

    @dataclass
    class Lora:
        adapter_model_id: str
        arch: Architecture | None = None
        model_id: str | None = None
        tokenizer_json: str | None = None
        topology: str | None = None
        from_uqff: str | list[str] | None = None
        write_uqff: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)
        hf_cache_path: str | None = None

    @dataclass
    class GGUF:
        quantized_model_id: str
        quantized_filename: str | list[str]
        tok_model_id: str | None = None
        topology: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)

    @dataclass
    class XLoraGGUF:
        quantized_model_id: str
        quantized_filename: str | list[str]
        xlora_model_id: str
        order: str
        tok_model_id: str | None = None
        tgt_non_granular_index: int | None = None
        topology: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)

    @dataclass
    class LoraGGUF:
        quantized_model_id: str
        quantized_filename: str | list[str]
        adapters_model_id: str
        order: str
        tok_model_id: str | None = None
        topology: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)

    @dataclass
    class GGML:
        quantized_model_id: str
        quantized_filename: str
        tok_model_id: str | None = None
        tokenizer_json: str | None = None
        gqa: int | None = None
        topology: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)

    @dataclass
    class XLoraGGML:
        quantized_model_id: str
        quantized_filename: str
        xlora_model_id: str
        order: str
        tok_model_id: str | None = None
        tgt_non_granular_index: int | None = None
        tokenizer_json: str | None = None
        gqa: int | None = None
        topology: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)

    @dataclass
    class LoraGGML:
        quantized_model_id: str
        quantized_filename: str
        adapters_model_id: str
        order: str
        tok_model_id: str | None = None
        tokenizer_json: str | None = None
        topology: str | None = None
        dtype: ModelDType = ModelDType.Auto
        auto_map_params: TextAutoMapParams | None = (None,)

    @dataclass
    class Embedding:
        model_id: str
        arch: EmbeddingArchitecture | None = None
        tokenizer_json: str | None = None
        topology: str | None = None
        from_uqff: str | list[str] | None = None
        write_uqff: str | None = None
        dtype: ModelDType = ModelDType.Auto
        hf_cache_path: str | None = None

    @dataclass
    class VisionPlain:
        model_id: str
        arch: VisionArchitecture
        tokenizer_json: str | None = None
        topology: str | None = None
        from_uqff: str | list[str] | None = None
        write_uqff: str | None = None
        dtype: ModelDType = ModelDType.Auto
        max_edge: int | None = None
        auto_map_params: VisionAutoMapParams | None = (None,)
        calibration_file: str | None = None
        imatrix: str | None = None
        hf_cache_path: str | None = None

    @dataclass
    class DiffusionPlain:
        model_id: str
        arch: DiffusionArchitecture
        dtype: ModelDType = ModelDType.Auto

    @dataclass
    class Speech:
        model_id: str
        arch: DiffusionArchitecture
        dac_model_id: str | None = None
        dtype: ModelDType = ModelDType.Auto
</code></pre>
<h2 id="multi-model-support"><a class="header" href="#multi-model-support">Multi-model Support</a></h2>
<p>The <code>mistralrs</code> Python SDK supports running multiple models using the <code>Runner</code> class with the <code>model_id</code> parameter. All request methods accept an optional <code>model_id</code> to target a specific model. When <code>model_id</code> is <code>None</code> or omitted, the default model is used. If aliases are configured (for example via the server config or Rust <code>MultiModelBuilder</code>), <code>list_models()</code> will return those aliases and you can pass them in requests; canonical pipeline names remain accepted.</p>
<h3 id="basic-usage-with-model_id"><a class="header" href="#basic-usage-with-model_id">Basic Usage with model_id</a></h3>
<pre><code class="language-python">import mistralrs

# Create a Runner with a vision model (Gemma 3 4B)
runner = mistralrs.Runner(
    which=mistralrs.Which.VisionPlain(
        model_id="google/gemma-3-4b-it",
        arch=mistralrs.VisionArchitecture.Gemma3,
    ),
    in_situ_quant="Q4K",
)

# List available models (model IDs are registered IDs, aliases if configured)
models = runner.list_models()
print(f"Available models: {models}")  # ["google/gemma-3-4b-it"]

# Send request to specific model using model_id parameter
response = runner.send_chat_completion_request(
    mistralrs.ChatCompletionRequest(
        messages=[{"role": "user", "content": "Hello!"}],
        max_tokens=100
    ),
    model_id="google/gemma-3-4b-it"  # Target specific model
)

# Send request without model_id (uses default model)
response = runner.send_chat_completion_request(
    mistralrs.ChatCompletionRequest(
        messages=[{"role": "user", "content": "Hello!"}],
        max_tokens=100
    )
)
</code></pre>
<h3 id="multi-model-management"><a class="header" href="#multi-model-management">Multi-model Management</a></h3>
<pre><code class="language-python"># List available models
models = runner.list_models()
print(f"Available models: {models}")

# Get/set default model
default_model = runner.get_default_model_id()
print(f"Default model: {default_model}")

# Change default model (model must be loaded)
runner.set_default_model_id("google/gemma-3-4b-it")

# List models with their status
models_with_status = runner.list_models_with_status()
for model_id, status in models_with_status:
    print(f"{model_id}: {status}")  # status is "loaded", "unloaded", or "reloading"
</code></pre>
<h3 id="model-unloading-and-reloading"><a class="header" href="#model-unloading-and-reloading">Model Unloading and Reloading</a></h3>
<p>You can unload models to free memory and reload them on demand:</p>
<pre><code class="language-python">model_id = "google/gemma-3-4b-it"

# Check if model is loaded
is_loaded = runner.is_model_loaded(model_id)
print(f"Model loaded: {is_loaded}")

# List models with their status
models_with_status = runner.list_models_with_status()
for mid, status in models_with_status:
    print(f"{mid}: {status}")

# Unload a model to free memory (preserves configuration for reload)
runner.unload_model(model_id)

# Check status after unload
is_loaded = runner.is_model_loaded(model_id)
print(f"Model loaded after unload: {is_loaded}")  # False

# Manually reload a model
runner.reload_model(model_id)

# Auto-reload: sending a request to an unloaded model will reload it automatically
response = runner.send_chat_completion_request(
    mistralrs.ChatCompletionRequest(
        messages=[{"role": "user", "content": "Hello!"}]
    ),
    model_id=model_id  # Will auto-reload if unloaded
)
</code></pre>
<h3 id="request-methods-with-model_id"><a class="header" href="#request-methods-with-model_id">Request Methods with model_id</a></h3>
<p>All request methods accept an optional <code>model_id</code> parameter:</p>
<pre><code class="language-python"># Chat completion
response = runner.send_chat_completion_request(request, model_id="model-id")

# Completion
response = runner.send_completion_request(request, model_id="model-id")

# Embeddings
embeddings = runner.send_embedding_request(request, model_id="model-id")

# Image generation
image = runner.generate_image(prompt, response_format, model_id="model-id")

# Audio generation
audio = runner.generate_audio(prompt, model_id="model-id")

# Tokenization
tokens = runner.tokenize_text(text, add_special_tokens=True, model_id="model-id")
text = runner.detokenize_text(tokens, skip_special_tokens=True, model_id="model-id")
</code></pre>
<p>When <code>model_id</code> is <code>None</code> or omitted, the default model is used.</p>
<h3 id="server-configuration"><a class="header" href="#server-configuration">Server Configuration</a></h3>
<p>For server-based multi-model deployment, see the <a href="#multi-model-support-1">multi-model documentation</a>.</p>
<h2 id="mcp-client"><a class="header" href="#mcp-client">MCP Client</a></h2>
<p>The <code>mistralrs</code> Python SDK now supports Model Context Protocol (MCP) clients, enabling AI assistants to connect to and interact with external tools and resources through standardized server interfaces.</p>
<h3 id="mcp-server-configuration"><a class="header" href="#mcp-server-configuration">MCP Server Configuration</a></h3>
<p>Configure MCP servers using <code>McpServerConfigPy</code>:</p>
<pre><code class="language-python"># HTTP-based MCP server with Bearer token authentication
http_server = mistralrs.McpServerConfigPy(
    id="web_search",
    name="Web Search MCP",
    source=mistralrs.McpServerSourcePy.Http(
        url="https://api.example.com/mcp",
        timeout_secs=30,
        headers={"X-API-Version": "v1"}  # Optional additional headers
    ),
    enabled=True,
    tool_prefix="web",  # Prefixes tool names to avoid conflicts
    resources=None,
    bearer_token="your-api-token"  # Automatically added as Authorization header
)

# Process-based MCP server for local tools
process_server = mistralrs.McpServerConfigPy(
    id="filesystem",
    name="Filesystem MCP",
    source=mistralrs.McpServerSourcePy.Process(
        command="mcp-server-filesystem",
        args=["--root", "/tmp"],
        work_dir=None,
        env={"MCP_LOG_LEVEL": "debug"}  # Optional environment variables
    ),
    enabled=True,
    tool_prefix="fs",
    resources=["file://**"],  # Resource patterns this client is interested in
    bearer_token=None  # Process servers typically don't need authentication
)

# WebSocket-based MCP server for real-time communication
websocket_server = mistralrs.McpServerConfigPy(
    id="realtime_data",
    name="Real-time Data MCP",
    source=mistralrs.McpServerSourcePy.WebSocket(
        url="wss://realtime.example.com/mcp",
        timeout_secs=60,
        headers=None
    ),
    enabled=True,
    tool_prefix="rt",
    resources=None,
    bearer_token="websocket-token"  # WebSocket Bearer token support
)
</code></pre>
<h3 id="mcp-client-configuration"><a class="header" href="#mcp-client-configuration">MCP Client Configuration</a></h3>
<p>Configure the MCP client using <code>McpClientConfigPy</code>:</p>
<pre><code class="language-python">mcp_config = mistralrs.McpClientConfigPy(
    servers=[http_server, process_server, websocket_server],
    auto_register_tools=True,  # Automatically discover and register tools
    tool_timeout_secs=30,      # Timeout for individual tool calls
    max_concurrent_calls=5     # Maximum concurrent tool calls across all servers
)
</code></pre>
<h3 id="integration-with-runner"><a class="header" href="#integration-with-runner">Integration with Runner</a></h3>
<p>Pass the MCP client configuration to the <code>Runner</code>:</p>
<pre><code class="language-python">runner = mistralrs.Runner(
    which=mistralrs.Which.GGUF(
        tok_model_id="mistralai/Mistral-7B-Instruct-v0.1",
        quantized_model_id="TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
        quantized_filename="mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    ),
    mcp_client_config=mcp_config  # MCP tools automatically registered
)
</code></pre>
<p>When <code>auto_register_tools=True</code>, the MCP client will:</p>
<ol>
<li>Connect to all enabled MCP servers</li>
<li>Discover available tools from each server</li>
<li>Register them for automatic tool calling with appropriate prefixes</li>
<li>Make them available during model conversations</li>
</ol>
<h3 id="mcp-transport-types"><a class="header" href="#mcp-transport-types">MCP Transport Types</a></h3>
<ul>
<li>
<p><strong>HTTP Transport</strong>: Best for public APIs, RESTful services, servers behind load balancers. Supports SSE (Server-Sent Events) and standard HTTP semantics.</p>
</li>
<li>
<p><strong>Process Transport</strong>: Best for local tools, development servers, sandboxed environments. Provides process isolation with no network overhead.</p>
</li>
<li>
<p><strong>WebSocket Transport</strong>: Best for interactive applications, real-time data, low-latency requirements. Supports persistent connections and server-initiated notifications.</p>
</li>
</ul>
<h3 id="authentication"><a class="header" href="#authentication">Authentication</a></h3>
<ul>
<li><strong>Bearer Tokens</strong>: Automatically added as <code>Authorization: Bearer &lt;token&gt;</code> header for HTTP and WebSocket connections</li>
<li><strong>Custom Headers</strong>: Additional headers can be specified for API keys, versioning, etc.</li>
<li><strong>Process Servers</strong>: Typically don’t require authentication as they run locally</li>
</ul>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<pre><code class="language-python">from mistralrs import Runner, Which, ChatCompletionRequest

runner = Runner(
    which=Which.GGUF(
        tok_model_id="mistralai/Mistral-7B-Instruct-v0.1",
        quantized_model_id="TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
        quantized_filename="mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    )
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[{"role":"user", "content":"Tell me a story about the Rust type system."}],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="embeddings-example"><a class="header" href="#embeddings-example">Embeddings example</a></h2>
<pre><code class="language-python">from mistralrs import EmbeddingArchitecture, EmbeddingRequest, Runner, Which

runner = Runner(
    which=Which.Embedding(
        model_id="google/embeddinggemma-300m",
        arch=EmbeddingArchitecture.EmbeddingGemma,
    )
)

embeddings = runner.send_embedding_request(
    EmbeddingRequest(
        input=[
            "task: query | text: superconductors",
            "task: query | text: graphene",
        ],
        truncate_sequence=True,
    )
)

print(len(embeddings), len(embeddings[0]))

# Swap the model_id and arch below to load Qwen/Qwen3-Embedding-0.6B instead:
# Runner(
#     which=Which.Embedding(
#         model_id="Qwen/Qwen3-Embedding-0.6B",
#         arch=EmbeddingArchitecture.Qwen3Embedding,
#     )
# )
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="python-sdk-installation"><a class="header" href="#python-sdk-installation">Python SDK Installation</a></h1>
<h2 id="quick-install-from-pypi-recommended"><a class="header" href="#quick-install-from-pypi-recommended">Quick Install from PyPI (Recommended)</a></h2>
<p>Pre-built wheels are available for common platforms. Choose the package that matches your hardware:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Hardware</th><th>Install Command</th></tr>
</thead>
<tbody>
<tr><td><strong>Recommended (auto-optimized)</strong></td><td><code>pip install mistralrs</code></td></tr>
<tr><td>NVIDIA GPUs (CUDA)</td><td><code>pip install mistralrs-cuda</code></td></tr>
<tr><td>Apple Silicon (Metal)</td><td><code>pip install mistralrs-metal</code></td></tr>
<tr><td>Apple Accelerate</td><td><code>pip install mistralrs-accelerate</code></td></tr>
<tr><td>Intel CPUs (MKL)</td><td><code>pip install mistralrs-mkl</code></td></tr>
</tbody>
</table>
</div>
<h3 id="platform-specific-optimizations"><a class="header" href="#platform-specific-optimizations">Platform-Specific Optimizations</a></h3>
<p>The <code>mistralrs</code> base package includes platform-specific optimizations:</p>
<ul>
<li><strong>macOS Apple Silicon</strong>: Metal GPU support built-in</li>
<li><strong>Linux/Windows x86_64</strong>: Intel MKL optimizations built-in</li>
<li><strong>Linux aarch64</strong>: CPU-only (use <code>mistralrs-cuda</code> for GPU support)</li>
</ul>
<p>All packages install the <code>mistralrs</code> Python module. The package suffix controls which accelerator features are enabled.</p>
<h3 id="supported-platforms"><a class="header" href="#supported-platforms">Supported Platforms</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Package</th><th style="text-align: center">Linux x86_64</th><th style="text-align: center">Linux aarch64</th><th style="text-align: center">Windows x86_64</th><th style="text-align: center">macOS aarch64</th></tr>
</thead>
<tbody>
<tr><td>mistralrs</td><td style="text-align: center">MKL</td><td style="text-align: center">CPU</td><td style="text-align: center">MKL</td><td style="text-align: center">Metal</td></tr>
<tr><td>mistralrs-cuda</td><td style="text-align: center">CUDA</td><td style="text-align: center">CUDA</td><td style="text-align: center">CUDA</td><td style="text-align: center">-</td></tr>
<tr><td>mistralrs-metal</td><td style="text-align: center">-</td><td style="text-align: center">-</td><td style="text-align: center">-</td><td style="text-align: center">Metal</td></tr>
<tr><td>mistralrs-accelerate</td><td style="text-align: center">-</td><td style="text-align: center">-</td><td style="text-align: center">-</td><td style="text-align: center">Accelerate</td></tr>
<tr><td>mistralrs-mkl</td><td style="text-align: center">MKL</td><td style="text-align: center">-</td><td style="text-align: center">MKL</td><td style="text-align: center">-</td></tr>
</tbody>
</table>
</div>
<p><strong>Python version</strong>: 3.10+ (wheels use abi3 for forward compatibility)</p>
<h3 id="windows-requirements"><a class="header" href="#windows-requirements">Windows Requirements</a></h3>
<p><strong>It is recommended to use WSL2 on Windows machines.</strong></p>
<p>On Windows, additional runtime dependencies may be required:</p>
<ul>
<li><strong>CUDA packages</strong>: Install the <a href="https://developer.nvidia.com/cuda-downloads">NVIDIA CUDA Toolkit</a> and ensure the <code>bin</code> directory is in your PATH</li>
<li><strong>MKL packages</strong>: Install the <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html">Intel oneAPI Math Kernel Library</a> runtime</li>
</ul>
<pre><code class="language-bash"># Example: Install with CUDA support
pip install mistralrs-cuda -v
</code></pre>
<h2 id="build-from-source-1"><a class="header" href="#build-from-source-1">Build from Source</a></h2>
<p>Building from source gives you access to the latest features and allows customization of build options.</p>
<h3 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h3>
<ol>
<li>
<p><strong>Install system packages:</strong></p>
<p>Ubuntu/Debian:</p>
<pre><code class="language-bash">sudo apt install libssl-dev pkg-config
</code></pre>
<p>macOS:</p>
<pre><code class="language-bash">brew install openssl pkg-config
</code></pre>
</li>
<li>
<p><strong>Install Rust</strong> from https://rustup.rs/:</p>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
</code></pre>
</li>
<li>
<p><strong>(Optional) Set up HuggingFace authentication</strong> for gated models:</p>
<pre><code class="language-bash">mkdir -p ~/.cache/huggingface
echo "YOUR_HF_TOKEN" &gt; ~/.cache/huggingface/token
</code></pre>
<p>Or use <code>huggingface-cli login</code>.</p>
</li>
</ol>
<h3 id="build-steps"><a class="header" href="#build-steps">Build Steps</a></h3>
<ol>
<li>
<p><strong>Clone the repository:</strong></p>
<pre><code class="language-bash">git clone https://github.com/EricLBuehler/mistral.rs.git
cd mistral.rs/mistralrs-pyo3
</code></pre>
</li>
<li>
<p><strong>Create and activate a virtual environment:</strong></p>
<pre><code class="language-bash">python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# or: .venv\Scripts\activate  # Windows
</code></pre>
</li>
<li>
<p><strong>Install maturin</strong> (Rust + Python build tool):</p>
<pre><code class="language-bash">pip install maturin[patchelf]
</code></pre>
</li>
<li>
<p><strong>Build and install:</strong></p>
<pre><code class="language-bash">maturin develop -r --features &lt;your-features&gt;
</code></pre>
</li>
</ol>
<h3 id="feature-flags-1"><a class="header" href="#feature-flags-1">Feature Flags</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>cuda</code></td><td>NVIDIA GPU support</td></tr>
<tr><td><code>flash-attn</code></td><td>Flash Attention (CUDA, Ampere+)</td></tr>
<tr><td><code>flash-attn-v3</code></td><td>Flash Attention v3 (CUDA, Hopper)</td></tr>
<tr><td><code>cudnn</code></td><td>cuDNN optimizations</td></tr>
<tr><td><code>metal</code></td><td>Apple Silicon GPU (macOS only)</td></tr>
<tr><td><code>accelerate</code></td><td>Apple Accelerate framework</td></tr>
<tr><td><code>mkl</code></td><td>Intel MKL</td></tr>
</tbody>
</table>
</div>
<p>Example with CUDA and Flash Attention:</p>
<pre><code class="language-bash">maturin develop -r --features "cuda flash-attn cudnn"
</code></pre>
<h2 id="verify-installation-1"><a class="header" href="#verify-installation-1">Verify Installation</a></h2>
<pre><code class="language-python">import mistralrs
print(mistralrs.__version__)
</code></pre>
<p>Quick test:</p>
<pre><code class="language-python">from mistralrs import Runner, Which, ChatCompletionRequest

runner = Runner(
    which=Which.Plain(model_id="Qwen/Qwen3-0.6B"),
)

response = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[{"role": "user", "content": "Hello!"}],
        max_tokens=50,
    )
)
print(response.choices[0].message.content)
</code></pre>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<ul>
<li><a href="#mistralrs-python-sdk">SDK Documentation</a> - Full SDK reference</li>
<li><a href="https://github.com/EricLBuehler/mistral.rs/tree/master/examples/python">Examples</a> - Python examples</li>
<li><a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/cookbook.ipynb">Cookbook</a> - Interactive tutorial</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="http-server"><a class="header" href="#http-server">HTTP server</a></h1>
<p>Mistral.rs provides a lightweight OpenAI API compatible HTTP server based on <a href="https://github.com/tokio-rs/axum">axum</a>. The request and response formats are supersets of the OpenAI API.</p>
<p>The API consists of the following endpoints. They can be viewed in your browser interactively by going to <code>http://localhost:&lt;port&gt;/docs</code>.</p>
<blockquote>
<p>ℹ️  Besides the HTTP endpoints described below, <code>mistralrs serve</code> can also expose the same functionality via the <strong>MCP protocol</strong>.
Enable it with <code>--mcp-port &lt;port&gt;</code> and see <a href="#mcp-protocol-support">MCP/server.md</a> for details.</p>
</blockquote>
<h2 id="additional-object-keys"><a class="header" href="#additional-object-keys">Additional object keys</a></h2>
<p>To support additional features, we have extended the completion and chat completion request objects. Both have the same keys added:</p>
<ul>
<li><code>top_k</code>: <code>int</code> | <code>null</code>. If non null, it is only relevant if positive.</li>
<li><code>grammar</code>: <code>{"type" : "regex" | "lark" | "json_schema" | "llguidance", "value": string}</code> or <code>null</code>. Grammar to use. This is mutually exclusive to the OpenAI-compatible <code>response_format</code>.</li>
<li><code>min_p</code>: <code>float</code> | <code>null</code>. If non null, it is only relevant if 1 &gt;= min_p &gt;= 0.</li>
<li><code>enable_thinking</code>: <code>bool</code>, default to <code>false</code>. Enable thinking for models that support it.</li>
<li><code>truncate_sequence</code>: <code>bool</code> | <code>null</code>. When <code>true</code>, requests that exceed the model context length will be truncated instead of rejected; otherwise the server returns a validation error. Embedding requests truncate tokens at the end of the prompt, while chat/completion requests truncate tokens at the start of the prompt.</li>
<li><code>repetition_penalty</code>: <code>float</code> | <code>null</code>. Penalty for repeating tokens. This is distinct from <code>frequency_penalty</code> and <code>presence_penalty</code> - it applies a direct multiplicative penalty to repeated token logits.</li>
<li><code>web_search_options</code>: <code>object</code> | <code>null</code>. Enable web search integration (see <a href="#web-search-tool-in-mistralrs">WEB_SEARCH.md</a>). Contains optional fields: <code>search_context_size</code> (“low”, “medium”, “high”), <code>user_location</code> (object with location info), <code>search_description</code> (override search tool description), <code>extract_description</code> (override extraction tool description).</li>
<li><code>reasoning_effort</code>: <code>string</code> | <code>null</code>. For Harmony-format models (like GPT-OSS), controls the depth of reasoning: <code>"low"</code>, <code>"medium"</code>, or <code>"high"</code>.</li>
<li><code>dry_multiplier</code>: <code>float</code> | <code>null</code>. DRY (Don’t Repeat Yourself) sampling multiplier. Controls the strength of the anti-repetition penalty.</li>
<li><code>dry_base</code>: <code>float</code> | <code>null</code>. DRY sampling base value.</li>
<li><code>dry_allowed_length</code>: <code>int</code> | <code>null</code>. DRY sampling allowed length before penalty applies.</li>
<li><code>dry_sequence_breakers</code>: <code>array of strings</code> | <code>null</code>. Tokens that reset the DRY penalty sequence.</li>
</ul>
<h2 id="response-extensions"><a class="header" href="#response-extensions">Response Extensions</a></h2>
<p>The response objects include additional fields beyond the standard OpenAI API:</p>
<h3 id="harmony-mode-responses"><a class="header" href="#harmony-mode-responses">Harmony Mode Responses</a></h3>
<p>For models using Harmony format (like GPT-OSS), responses may include additional reasoning content:</p>
<ul>
<li><code>reasoning_content</code>: <code>string</code> | <code>null</code>. Chain-of-thought reasoning from Harmony-format models. This field contains the model’s internal analysis and commentary that led to the final response. It is separate from the main <code>content</code> field.</li>
</ul>
<p>When streaming, <code>reasoning_content</code> appears in the <code>delta</code> object alongside <code>content</code>.</p>
<p><strong>Example response:</strong></p>
<pre><code class="language-json">{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "The answer is 42.",
      "reasoning_content": "Let me analyze this step by step..."
    }
  }]
}
</code></pre>
<h2 id="model-parameter-validation"><a class="header" href="#model-parameter-validation">Model Parameter Validation</a></h2>
<p>Mistral.rs validates that the <code>model</code> parameter in API requests matches the model that was actually loaded by the server. This ensures requests are processed by the correct model and prevents confusion.</p>
<p><strong>Behavior:</strong></p>
<ul>
<li>If the <code>model</code> parameter matches the loaded model name, the request proceeds normally</li>
<li>If the <code>model</code> parameter doesn’t match, the request fails with an error message indicating the mismatch</li>
<li>The special model name <code>"default"</code> can be used to bypass this validation entirely</li>
</ul>
<p><strong>Examples:</strong></p>
<ul>
<li>✅ Request with <code>"model": "meta-llama/Llama-3.2-3B-Instruct"</code> when <code>meta-llama/Llama-3.2-3B-Instruct</code> is loaded -&gt; <strong>succeeds</strong></li>
<li>❌ Request with <code>"model": "gpt-4"</code> when <code>mistral-7b-instruct</code> is loaded -&gt; <strong>fails</strong></li>
<li>✅ Request with <code>"model": "default"</code> regardless of loaded model -&gt; <strong>always succeeds</strong></li>
</ul>
<p><strong>Usage:</strong> Use <code>"default"</code> in the model field when you need to satisfy API clients that require a model parameter but don’t need to specify a particular model. This is demonstrated in all the examples below.</p>
<h2 id="post-v1chatcompletions"><a class="header" href="#post-v1chatcompletions"><code>POST</code>: <code>/v1/chat/completions</code></a></h2>
<p>Process an OpenAI compatible request, returning an OpenAI compatible response when finished. Please find the official OpenAI API documentation <a href="https://platform.openai.com/docs/api-reference/chat">here</a>. To control the interval keep-alive messages are sent, set the <code>KEEP_ALIVE_INTERVAL</code> environment variable to the desired time in ms.</p>
<p>To send a request with the Python <code>openai</code> library:</p>
<pre><code class="language-python">import openai

client = openai.OpenAI(
    base_url="http://localhost:1234/v1", # "http://&lt;Your api-server IP&gt;:port"
    api_key = "EMPTY"
)

completion = client.chat.completions.create(
model="default",
messages=[
    {"role": "system", "content": "You are Mistral.rs, an AI assistant."},
    {"role": "user", "content": "Write a story about Rust error handling."}
]
)

print(completion.choices[0].message)
</code></pre>
<p>Or with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{
"model": "default",
"messages": [
{
    "role": "system",
    "content": "You are Mistral.rs, an AI assistant."
},
{
    "role": "user",
    "content": "Write a story about Rust error handling."
}
]
}'
</code></pre>
<p>A streaming request can also be created by setting <code>"stream": true</code> in the request JSON. Please see <a href="https://cookbook.openai.com/examples/how_to_stream_completions">this</a> guide.</p>
<blockquote>
<p>ℹ️ Requests whose prompt exceeds the model’s maximum context length now fail unless you opt in to truncation. Set <code>"truncate_sequence": true</code> to drop the oldest prompt tokens while reserving room (equal to <code>max_tokens</code> when provided, otherwise one token) for generation. Specifically, tokens from the front of the prompt are dropped.</p>
</blockquote>
<h2 id="get-v1models"><a class="header" href="#get-v1models"><code>GET</code>: <code>/v1/models</code></a></h2>
<p>Returns the running models.</p>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:&lt;port&gt;/v1/models
</code></pre>
<h2 id="get--or-health"><a class="header" href="#get--or-health"><code>GET</code>: <code>/</code> or <code>/health</code></a></h2>
<p>Returns the server health.</p>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:&lt;port&gt;/health
</code></pre>
<h2 id="get-docs"><a class="header" href="#get-docs"><code>GET</code>: <code>/docs</code></a></h2>
<p>Returns OpenAPI API docs via SwaggerUI.</p>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:&lt;port&gt;/docs
</code></pre>
<h2 id="post-v1completions"><a class="header" href="#post-v1completions"><code>POST</code>: <code>/v1/completions</code></a></h2>
<p>Process an OpenAI compatible completions request, returning an OpenAI compatible response when finished. Please find the official OpenAI API documentation <a href="https://platform.openai.com/docs/api-reference/completions">here</a>.</p>
<h3 id="completions-specific-parameters"><a class="header" href="#completions-specific-parameters">Completions-specific parameters</a></h3>
<p>In addition to the common parameters listed above, the completions endpoint supports:</p>
<ul>
<li><code>best_of</code>: <code>int</code> | <code>null</code>. Generate <code>best_of</code> completions server-side and return the best one (the one with the highest log probability per token). When used with <code>n</code>, <code>best_of</code> must be greater than <code>n</code>.</li>
<li><code>echo</code>: <code>bool</code>, default <code>false</code>. Echo back the prompt in addition to the completion.</li>
</ul>
<p>To send a request with the Python <code>openai</code> library:</p>
<pre><code class="language-python">import openai

client = openai.OpenAI(
    base_url="http://localhost:1234/v1", # "http://&lt;Your api-server IP&gt;:port"
    api_key = "EMPTY"
)

completion = client.completions.create(
    model="default",
    prompt="What is Rust?",
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)

print(completion.choices[0].message)
</code></pre>
<p>Or with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{
"model": "default",
"prompt": "What is Rust?"
}'
</code></pre>
<blockquote>
<p>ℹ️ The <code>truncate_sequence</code> flag behaves the same way for the completions endpoint: keep it <code>false</code> (default) to receive a validation error, or set it to <code>true</code> to trim the prompt automatically.</p>
</blockquote>
<h2 id="post-v1embeddings"><a class="header" href="#post-v1embeddings"><code>POST</code>: <code>/v1/embeddings</code></a></h2>
<p>Serve an embedding model (for example, EmbeddingGemma) to enable this endpoint:</p>
<pre><code class="language-bash">mistralrs serve -m google/embeddinggemma-300m
</code></pre>
<p>In multi-model mode, include an <code>Embedding</code> entry in your selector config to expose it alongside chat models.</p>
<p>Create vector embeddings via the OpenAI-compatible endpoint. Supported request fields:</p>
<ul>
<li><code>input</code>: a single string, an array of strings, an array of token IDs (<code>[123, 456]</code>), or a batch of token arrays (<code>[[...], [...]]</code>).</li>
<li><code>encoding_format</code>: <code>"float"</code> (default) returns arrays of <code>f32</code>; <code>"base64"</code> returns Base64 strings.</li>
<li><code>dimensions</code>: currently unsupported; providing it yields a validation error.</li>
<li><code>truncate_sequence</code>: <code>bool</code>, default <code>false</code>. Set to <code>true</code> to clip over-length prompts instead of receiving a validation error.</li>
</ul>
<blockquote>
<p>ℹ️ Requests whose prompt exceeds the model’s maximum context length now fail unless you opt in to truncation. Embedding requests truncate tokens from the end of the prompt.</p>
</blockquote>
<p>Example (Python <code>openai</code> client):</p>
<pre><code class="language-python">import openai

client = openai.OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="EMPTY",
)

result = client.embeddings.create(
    model="default",
    input=[
        "Embeddings capture semantic relationships between texts.",
        "What is graphene?",
    ],
    truncate_sequence=True,
)

for item in result.data:
    print(item.index, len(item.embedding))
</code></pre>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer EMPTY" \
  -d '{
    "model": "default",
    "input": ["graphene conductivity", "superconductor basics"],
    "encoding_format": "base64",
    "truncate_sequence": false
  }'
</code></pre>
<p>Responses follow the OpenAI schema: <code>object: "list"</code>, <code>data[*].embedding</code> containing either float arrays or Base64 strings depending on <code>encoding_format</code>, and a <code>usage</code> block (<code>prompt_tokens</code>, <code>total_tokens</code>). At present those counters report <code>0</code> because token accounting for embeddings is not yet implemented.</p>
<h2 id="post-v1imagesgenerations"><a class="header" href="#post-v1imagesgenerations"><code>POST</code>: <code>/v1/images/generations</code></a></h2>
<p>Generate images using diffusion models (like FLUX). First, serve a diffusion model:</p>
<pre><code class="language-bash">mistralrs serve -m black-forest-labs/FLUX.1-schnell
</code></pre>
<p>Supported request fields:</p>
<ul>
<li><code>model</code>: Model identifier (use <code>"default"</code> to bypass validation)</li>
<li><code>prompt</code>: Text description of the image to generate</li>
<li><code>n</code>: Number of images to generate (default: 1)</li>
<li><code>response_format</code>: <code>"url"</code> or <code>"b64_json"</code> (default: <code>"url"</code>)</li>
<li><code>height</code>: Image height in pixels (default: 720)</li>
<li><code>width</code>: Image width in pixels (default: 1280)</li>
</ul>
<p>Example with Python:</p>
<pre><code class="language-python">import openai
import base64

client = openai.OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="EMPTY",
)

response = client.images.generate(
    model="default",
    prompt="A majestic snow-covered mountain at sunset",
    n=1,
    response_format="b64_json",
    size="1280x720",  # width x height
)

# Save the generated image
image_data = base64.b64decode(response.data[0].b64_json)
with open("output.png", "wb") as f:
    f.write(image_data)
</code></pre>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/images/generations \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer EMPTY" \
  -d '{
    "model": "default",
    "prompt": "A majestic snow-covered mountain at sunset",
    "n": 1,
    "response_format": "b64_json",
    "height": 720,
    "width": 1280
  }'
</code></pre>
<h2 id="post-v1audiospeech"><a class="header" href="#post-v1audiospeech"><code>POST</code>: <code>/v1/audio/speech</code></a></h2>
<p>Generate speech from text using speech models (like Dia). First, serve a speech model:</p>
<pre><code class="language-bash">mistralrs serve -m nari-labs/Dia-1.6B
</code></pre>
<p>Supported request fields:</p>
<ul>
<li><code>model</code>: Model identifier (use <code>"default"</code> to bypass validation)</li>
<li><code>input</code>: Text to convert to speech. For Dia models, use speaker tags like <code>[S1]</code> and <code>[S2]</code> to control multiple voices</li>
<li><code>response_format</code>: <code>"wav"</code> or <code>"pcm"</code> (only these formats are supported)</li>
</ul>
<blockquote>
<p>Note: The <code>voice</code> and <code>instructions</code> fields from the OpenAI API are currently ignored.</p>
</blockquote>
<p>Example with Python:</p>
<pre><code class="language-python">import requests

response = requests.post(
    "http://localhost:1234/v1/audio/speech",
    headers={
        "Content-Type": "application/json",
        "Authorization": "Bearer EMPTY",
    },
    json={
        "model": "default",
        "input": "[S1] Hello, how are you today? [S2] I'm doing great, thanks for asking!",
        "response_format": "wav",
    },
)

# Save the audio file
with open("output.wav", "wb") as f:
    f.write(response.content)
</code></pre>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/audio/speech \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer EMPTY" \
  -d '{
    "model": "default",
    "input": "[S1] Dia is an open weights text to dialogue model. [S2] Try it now!",
    "response_format": "wav"
  }' \
  --output output.wav
</code></pre>
<p>The response is raw audio data with the appropriate <code>Content-Type</code> header (<code>audio/wav</code> for WAV format, <code>audio/pcm</code> for PCM format).</p>
<h2 id="post-v1responses"><a class="header" href="#post-v1responses"><code>POST</code>: <code>/v1/responses</code></a></h2>
<p>Create a response using the OpenAI-compatible Responses API. Please find the official OpenAI API documentation <a href="https://platform.openai.com/docs/api-reference/responses">here</a>.</p>
<p>To send a request with the Python <code>openai</code> library:</p>
<pre><code class="language-python">import openai

client = openai.OpenAI(
    base_url="http://localhost:1234/v1",
    api_key = "EMPTY"
)

# First turn
resp1 = client.responses.create(
    model="default",
    input="Apples are delicious!"
)
print(resp1.output_text)

# Follow-up - no need to resend the first message
resp2 = client.responses.create(
    model="default",
    previous_response_id=resp1.id,
    input="Can you eat them?"
)
print(resp2.output_text)
</code></pre>
<p>Or with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{
"model": "default",
"input": "Tell me about Rust programming"
}'

# Follow-up using previous_response_id
curl http://localhost:1234/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{
"model": "default",
"previous_response_id": "resp_12345-uuid-here",
"input": "What makes it memory safe?"
}'
</code></pre>
<p>The API also supports multimodal inputs (images, audio) and streaming responses by setting <code>"stream": true</code> in the request JSON.</p>
<blockquote>
<p>ℹ️ The Responses API forwards <code>truncate_sequence</code> to underlying chat completions. Enable it if you want over-length conversations to be truncated rather than rejected.</p>
</blockquote>
<h2 id="get-v1responsesresponse_id"><a class="header" href="#get-v1responsesresponse_id"><code>GET</code>: <code>/v1/responses/{response_id}</code></a></h2>
<p>Retrieve a previously created response by its ID.</p>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/responses/resp_12345-uuid-here \
-H "Authorization: Bearer EMPTY"
</code></pre>
<h2 id="delete-v1responsesresponse_id"><a class="header" href="#delete-v1responsesresponse_id"><code>DELETE</code>: <code>/v1/responses/{response_id}</code></a></h2>
<p>Delete a stored response and its associated conversation history.</p>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl -X DELETE http://localhost:1234/v1/responses/resp_12345-uuid-here \
-H "Authorization: Bearer EMPTY"
</code></pre>
<h2 id="post-re_isq"><a class="header" href="#post-re_isq"><code>POST</code>: <code>/re_isq</code></a></h2>
<p>Reapply ISQ to the model if possible. Pass the names as a JSON object with the key <code>ggml_type</code> to a string (the quantization level).</p>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:&lt;port&gt;/re_isq -H "Content-Type: application/json" -H "Authorization: Bearer EMPTY" -d '{"ggml_type":"4"}'
</code></pre>
<h2 id="model-management-endpoints"><a class="header" href="#model-management-endpoints">Model Management Endpoints</a></h2>
<p>These endpoints allow dynamic management of loaded models, enabling you to free memory by unloading models and reload them on demand.</p>
<h3 id="post-v1modelsunload"><a class="header" href="#post-v1modelsunload"><code>POST</code>: <code>/v1/models/unload</code></a></h3>
<p>Unload a model from memory while preserving its configuration for later reload. The model can be reloaded manually or will auto-reload when a request is sent to it.</p>
<p><strong>Request body:</strong></p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct"
}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "unloaded"
}
</code></pre>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl -X POST http://localhost:1234/v1/models/unload \
  -H "Content-Type: application/json" \
  -d '{"model_id": "meta-llama/Llama-3.2-3B-Instruct"}'
</code></pre>
<h3 id="post-v1modelsreload"><a class="header" href="#post-v1modelsreload"><code>POST</code>: <code>/v1/models/reload</code></a></h3>
<p>Manually reload a previously unloaded model. This is also triggered automatically when a request is sent to an unloaded model.</p>
<p><strong>Request body:</strong></p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct"
}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "loaded"
}
</code></pre>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl -X POST http://localhost:1234/v1/models/reload \
  -H "Content-Type: application/json" \
  -d '{"model_id": "meta-llama/Llama-3.2-3B-Instruct"}'
</code></pre>
<h3 id="post-v1modelsstatus"><a class="header" href="#post-v1modelsstatus"><code>POST</code>: <code>/v1/models/status</code></a></h3>
<p>Get the current status of a specific model.</p>
<p><strong>Request body:</strong></p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct"
}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "loaded"
}
</code></pre>
<p>Example with <code>curl</code>:</p>
<pre><code class="language-bash">curl -X POST http://localhost:1234/v1/models/status \
  -H "Content-Type: application/json" \
  -d '{"model_id": "meta-llama/Llama-3.2-3B-Instruct"}'
</code></pre>
<h3 id="status-values"><a class="header" href="#status-values">Status Values</a></h3>
<p>The <code>status</code> field in responses can be one of:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>loaded</code></td><td>Model is loaded and ready to serve requests</td></tr>
<tr><td><code>unloaded</code></td><td>Model is unloaded but can be reloaded</td></tr>
<tr><td><code>reloading</code></td><td>Model is currently being reloaded</td></tr>
<tr><td><code>not_found</code></td><td>Model ID not recognized</td></tr>
<tr><td><code>no_loader_config</code></td><td>Model cannot be reloaded (missing loader configuration)</td></tr>
<tr><td><code>internal_error</code></td><td>An internal error occurred (check <code>error</code> field for details)</td></tr>
</tbody>
</table>
</div>
<p>When an error occurs, the response may include an <code>error</code> field with additional details:</p>
<pre><code class="language-json">{
  "model_id": "unknown-model",
  "status": "not_found",
  "error": null
}
</code></pre>
<h3 id="auto-reload-behavior"><a class="header" href="#auto-reload-behavior">Auto-Reload Behavior</a></h3>
<p>When a request (e.g., chat completion) is sent to an unloaded model, the model will automatically reload before processing the request. This enables a “lazy loading” pattern where models are only loaded when needed, helping manage GPU memory efficiently.</p>
<h3 id="models-list-with-status"><a class="header" href="#models-list-with-status">Models List with Status</a></h3>
<p>The <code>/v1/models</code> endpoint includes a <code>status</code> field for each model:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/models
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "object": "list",
  "data": [
    {
      "id": "default",
      "object": "model",
      "created": 1234567890,
      "owned_by": "local"
    },
    {
      "id": "meta-llama/Llama-3.2-3B-Instruct",
      "object": "model",
      "created": 1234567890,
      "owned_by": "local",
      "status": "loaded"
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="openresponses-api"><a class="header" href="#openresponses-api">OpenResponses API</a></h1>
<p>mistral.rs supports the <a href="https://www.openresponses.org/specification">OpenResponses API specification</a>.</p>
<h2 id="endpoints"><a class="header" href="#endpoints">Endpoints</a></h2>
<ul>
<li><code>POST /v1/responses</code> - Create a response</li>
<li><code>GET /v1/responses/{id}</code> - Retrieve a response</li>
<li><code>DELETE /v1/responses/{id}</code> - Delete a response</li>
<li><code>POST /v1/responses/{id}/cancel</code> - Cancel a background response</li>
</ul>
<h2 id="unsupported-parameters"><a class="header" href="#unsupported-parameters">Unsupported Parameters</a></h2>
<p>The following parameters are accepted for API compatibility but will return errors if set to non-default values:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Behavior</th></tr>
</thead>
<tbody>
<tr><td><code>parallel_tool_calls</code></td><td>Only <code>true</code> or omitted is supported; <code>false</code> returns an error</td></tr>
<tr><td><code>max_tool_calls</code></td><td>Not supported; setting any value returns an error</td></tr>
</tbody>
</table>
</div>
<h2 id="mistralrs-extensions"><a class="header" href="#mistralrs-extensions">mistral.rs Extensions</a></h2>
<p>These additional parameters are available beyond the spec:</p>
<ul>
<li><code>stop</code> - Stop sequences</li>
<li><code>repetition_penalty</code> - Token repetition penalty</li>
<li><code>top_k</code> - Top-k sampling</li>
<li><code>grammar</code> - Constrained generation grammar</li>
<li><code>min_p</code> - Min-p sampling</li>
<li><code>dry_multiplier</code>, <code>dry_base</code>, <code>dry_allowed_length</code>, <code>dry_sequence_breakers</code> - DRY sampling</li>
<li><code>web_search_options</code> - Web search integration</li>
</ul>
<p>See <a href="#http-server">HTTP.md</a> for usage examples.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="supported-models"><a class="header" href="#supported-models">Supported Models</a></h1>
<p>Complete reference for model support in mistral.rs.</p>
<h2 id="model-categories"><a class="header" href="#model-categories">Model Categories</a></h2>
<h3 id="text-models"><a class="header" href="#text-models">Text Models</a></h3>
<ul>
<li>Granite 4.0</li>
<li>SmolLM 3</li>
<li>DeepSeek V3</li>
<li>GPT-OSS</li>
<li>DeepSeek V2</li>
<li>Qwen 3 Next</li>
<li>Qwen 3 MoE</li>
<li>Phi 3.5 MoE</li>
<li>Qwen 3</li>
<li>GLM 4</li>
<li>GLM-4.7-Flash</li>
<li>GLM-4.7 (MoE)</li>
<li>Gemma 2</li>
<li>Qwen 2</li>
<li>Starcoder 2</li>
<li>Phi 3</li>
<li>Mixtral</li>
<li>Phi 2</li>
<li>Gemma</li>
<li>Llama</li>
<li>Mistral</li>
</ul>
<h3 id="vision-models"><a class="header" href="#vision-models">Vision Models</a></h3>
<ul>
<li>Qwen 3-VL</li>
<li>Qwen 3-VL MoE</li>
<li>Gemma 3n</li>
<li>Llama 4</li>
<li>Gemma 3</li>
<li>Mistral 3</li>
<li>Phi 4 multimodal</li>
<li>Qwen 2.5-VL</li>
<li>MiniCPM-O</li>
<li>Llama 3.2 Vision</li>
<li>Qwen 2-VL</li>
<li>Idefics 3</li>
<li>Idefics 2</li>
<li>LLaVA Next</li>
<li>LLaVA</li>
<li>Phi 3V</li>
</ul>
<h3 id="speech-models"><a class="header" href="#speech-models">Speech Models</a></h3>
<ul>
<li>Voxtral (ASR/speech-to-text)</li>
<li>Dia</li>
</ul>
<h3 id="image-generation-models"><a class="header" href="#image-generation-models">Image Generation Models</a></h3>
<ul>
<li>FLUX</li>
</ul>
<h3 id="embedding-models"><a class="header" href="#embedding-models">Embedding Models</a></h3>
<ul>
<li>Embedding Gemma</li>
<li>Qwen 3 Embedding</li>
</ul>
<p><a href="https://github.com/EricLBuehler/mistral.rs/issues/156">Request a new model</a></p>
<h3 id="supported-gguf-architectures"><a class="header" href="#supported-gguf-architectures">Supported GGUF Architectures</a></h3>
<p><strong>Plain:</strong></p>
<ul>
<li>llama</li>
<li>phi2</li>
<li>phi3</li>
<li>starcoder2</li>
<li>qwen2</li>
<li>qwen3</li>
</ul>
<p><strong>With adapters:</strong></p>
<ul>
<li>llama</li>
<li>phi3</li>
</ul>
<h2 id="quantization-support"><a class="header" href="#quantization-support">Quantization Support</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>GGUF</th><th>GGML</th><th>ISQ</th></tr>
</thead>
<tbody>
<tr><td>Mistral</td><td>✅</td><td></td><td>✅</td></tr>
<tr><td>Gemma</td><td></td><td></td><td>✅</td></tr>
<tr><td>Llama</td><td>✅</td><td>✅</td><td>✅</td></tr>
<tr><td>Mixtral</td><td>✅</td><td></td><td>✅</td></tr>
<tr><td>Phi 2</td><td>✅</td><td></td><td>✅</td></tr>
<tr><td>Phi 3</td><td>✅</td><td></td><td>✅</td></tr>
<tr><td>Phi 3.5 MoE</td><td></td><td></td><td>✅</td></tr>
<tr><td>Qwen 2.5</td><td></td><td></td><td>✅</td></tr>
<tr><td>Phi 3 Vision</td><td></td><td></td><td>✅</td></tr>
<tr><td>Idefics 2</td><td></td><td></td><td>✅</td></tr>
<tr><td>Gemma 2</td><td></td><td></td><td>✅</td></tr>
<tr><td>GLM4</td><td></td><td></td><td>✅</td></tr>
<tr><td>GLM-4.7-Flash (MoE)</td><td></td><td></td><td>✅</td></tr>
<tr><td>GLM-4.7 (MoE)</td><td></td><td></td><td>✅</td></tr>
<tr><td>Starcoder 2</td><td></td><td>✅</td><td>✅</td></tr>
<tr><td>LLaVa Next</td><td></td><td></td><td>✅</td></tr>
<tr><td>LLaVa</td><td></td><td></td><td>✅</td></tr>
<tr><td>Llama 3.2 Vision</td><td></td><td></td><td>✅</td></tr>
<tr><td>Qwen2-VL</td><td></td><td></td><td>✅</td></tr>
<tr><td>Idefics 3</td><td></td><td></td><td>✅</td></tr>
<tr><td>Deepseek V2</td><td></td><td></td><td>✅</td></tr>
<tr><td>Deepseek V3</td><td></td><td></td><td>✅</td></tr>
<tr><td>MiniCPM-O 2.6</td><td></td><td></td><td>✅</td></tr>
<tr><td>Qwen2.5-VL</td><td></td><td></td><td>✅</td></tr>
<tr><td>Gemma 3</td><td></td><td></td><td>✅</td></tr>
<tr><td>Mistral 3</td><td></td><td></td><td>✅</td></tr>
<tr><td>Llama 4</td><td></td><td></td><td>✅</td></tr>
<tr><td>Qwen 3</td><td>✅</td><td></td><td>✅</td></tr>
<tr><td>SmolLM3</td><td></td><td></td><td>✅</td></tr>
<tr><td>Dia 1.6b</td><td></td><td></td><td>✅</td></tr>
<tr><td>Voxtral</td><td></td><td></td><td>✅</td></tr>
<tr><td>Gemma 3n</td><td></td><td></td><td>✅</td></tr>
<tr><td>Qwen 3 VL</td><td></td><td></td><td>✅</td></tr>
<tr><td>Qwen 3-VL MoE</td><td></td><td></td><td>✅</td></tr>
<tr><td>Qwen 3 Next</td><td></td><td></td><td>✅</td></tr>
<tr><td>Phi 4 Multimodal</td><td></td><td></td><td>✅</td></tr>
<tr><td>Granite 4.0</td><td></td><td></td><td>✅</td></tr>
<tr><td>GPT-OSS</td><td></td><td></td><td>✅</td></tr>
</tbody>
</table>
</div>
<h2 id="device-mapping-support"><a class="header" href="#device-mapping-support">Device Mapping Support</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model category</th><th>Supported</th></tr>
</thead>
<tbody>
<tr><td>Plain</td><td>✅</td></tr>
<tr><td>GGUF</td><td>✅</td></tr>
<tr><td>GGML</td><td></td></tr>
<tr><td>Vision Plain</td><td>✅</td></tr>
</tbody>
</table>
</div>
<h2 id="x-lora-and-lora-support"><a class="header" href="#x-lora-and-lora-support">X-LoRA and LoRA Support</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>X-LoRA</th><th>X-LoRA+GGUF</th><th>X-LoRA+GGML</th></tr>
</thead>
<tbody>
<tr><td>Mistral</td><td>✅</td><td>✅</td><td></td></tr>
<tr><td>Gemma</td><td>✅</td><td></td><td></td></tr>
<tr><td>Llama</td><td>✅</td><td>✅</td><td>✅</td></tr>
<tr><td>Mixtral</td><td>✅</td><td>✅</td><td></td></tr>
<tr><td>Phi 2</td><td>✅</td><td></td><td></td></tr>
<tr><td>Phi 3</td><td>✅</td><td>✅</td><td></td></tr>
<tr><td>Phi 3.5 MoE</td><td></td><td></td><td></td></tr>
<tr><td>Qwen 2.5</td><td></td><td></td><td></td></tr>
<tr><td>Phi 3 Vision</td><td></td><td></td><td></td></tr>
<tr><td>Idefics 2</td><td></td><td></td><td></td></tr>
<tr><td>Gemma 2</td><td>✅</td><td></td><td></td></tr>
<tr><td>GLM4</td><td>✅</td><td></td><td></td></tr>
<tr><td>GLM-4.7-Flash (MoE)</td><td></td><td></td><td></td></tr>
<tr><td>GLM-4.7 (MoE)</td><td></td><td></td><td></td></tr>
<tr><td>Starcoder 2</td><td>✅</td><td></td><td></td></tr>
<tr><td>LLaVa Next</td><td></td><td></td><td></td></tr>
<tr><td>LLaVa</td><td></td><td></td><td></td></tr>
<tr><td>Qwen2-VL</td><td></td><td></td><td></td></tr>
<tr><td>Idefics 3</td><td></td><td></td><td></td></tr>
<tr><td>Deepseek V2</td><td></td><td></td><td></td></tr>
<tr><td>Deepseek V3</td><td></td><td></td><td></td></tr>
<tr><td>MiniCPM-O 2.6</td><td></td><td></td><td></td></tr>
<tr><td>Qwen2.5-VL</td><td></td><td></td><td></td></tr>
<tr><td>Gemma 3</td><td></td><td></td><td></td></tr>
<tr><td>Mistral 3</td><td></td><td></td><td></td></tr>
<tr><td>Llama 4</td><td></td><td></td><td></td></tr>
<tr><td>Qwen 3</td><td></td><td></td><td></td></tr>
<tr><td>SmolLM3</td><td>✅</td><td></td><td></td></tr>
<tr><td>Gemma 3n</td><td></td><td></td><td></td></tr>
<tr><td>Voxtral</td><td></td><td></td><td></td></tr>
<tr><td>Qwen 3 VL</td><td></td><td></td><td></td></tr>
<tr><td>Qwen 3-VL MoE</td><td></td><td></td><td></td></tr>
<tr><td>Qwen 3 Next</td><td></td><td></td><td></td></tr>
<tr><td>Phi 4 Multimodal</td><td></td><td></td><td></td></tr>
<tr><td>Llama 3.2 Vision</td><td></td><td></td><td></td></tr>
<tr><td>Granite 4.0</td><td></td><td></td><td></td></tr>
<tr><td>GPT-OSS</td><td></td><td></td><td></td></tr>
</tbody>
</table>
</div>
<h2 id="anymoe-support"><a class="header" href="#anymoe-support">AnyMoE Support</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>AnyMoE</th></tr>
</thead>
<tbody>
<tr><td>Mistral 7B</td><td>✅</td></tr>
<tr><td>Gemma</td><td>✅</td></tr>
<tr><td>Llama</td><td>✅</td></tr>
<tr><td>Mixtral</td><td></td></tr>
<tr><td>Phi 2</td><td>✅</td></tr>
<tr><td>Phi 3</td><td>✅</td></tr>
<tr><td>Phi 3.5 MoE</td><td></td></tr>
<tr><td>Qwen 2.5</td><td>✅</td></tr>
<tr><td>Phi 3 Vision</td><td></td></tr>
<tr><td>Idefics 2</td><td></td></tr>
<tr><td>Gemma 2</td><td>✅</td></tr>
<tr><td>GLM-4.7-Flash (MoE)</td><td></td></tr>
<tr><td>GLM-4.7 (MoE)</td><td></td></tr>
<tr><td>Starcoder 2</td><td>✅</td></tr>
<tr><td>LLaVa Next</td><td>✅</td></tr>
<tr><td>LLaVa</td><td>✅</td></tr>
<tr><td>Llama 3.2 Vision</td><td></td></tr>
<tr><td>Qwen2-VL</td><td></td></tr>
<tr><td>Idefics 3</td><td>✅</td></tr>
<tr><td>Deepseek V2</td><td></td></tr>
<tr><td>Deepseek V3</td><td></td></tr>
<tr><td>MiniCPM-O 2.6</td><td></td></tr>
<tr><td>Qwen2.5-VL</td><td></td></tr>
<tr><td>Gemma 3</td><td>✅</td></tr>
<tr><td>Mistral 3</td><td>✅</td></tr>
<tr><td>Llama 4</td><td></td></tr>
<tr><td>Qwen 3</td><td></td></tr>
<tr><td>SmolLM3</td><td>✅</td></tr>
<tr><td>Gemma 3n</td><td></td></tr>
<tr><td>Voxtral</td><td></td></tr>
<tr><td>Qwen 3 VL</td><td></td></tr>
<tr><td>Qwen 3-VL MoE</td><td></td></tr>
<tr><td>Qwen 3 Next</td><td></td></tr>
<tr><td>Phi 4 Multimodal</td><td></td></tr>
<tr><td>Dia 1.6b</td><td></td></tr>
<tr><td>Granite 4.0</td><td></td></tr>
<tr><td>GPT-OSS</td><td></td></tr>
</tbody>
</table>
</div>
<h2 id="using-derivative-models"><a class="header" href="#using-derivative-models">Using Derivative Models</a></h2>
<p>Model type is auto-detected. Use flags for quantized models and adapters:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model Type</th><th>Required Arguments</th></tr>
</thead>
<tbody>
<tr><td>Plain</td><td><code>-m &lt;model-id&gt;</code></td></tr>
<tr><td>GGUF Quantized</td><td><code>-m &lt;model-id&gt; --format gguf -f &lt;file&gt;</code></td></tr>
<tr><td>ISQ Quantized</td><td><code>-m &lt;model-id&gt; --isq &lt;level&gt;</code></td></tr>
<tr><td>UQFF Quantized</td><td><code>-m &lt;model-id&gt; --from-uqff &lt;file&gt;</code></td></tr>
<tr><td>LoRA</td><td><code>-m &lt;model-id&gt; --lora &lt;adapter&gt;</code></td></tr>
<tr><td>X-LoRA</td><td><code>-m &lt;model-id&gt; --xlora &lt;adapter&gt; --xlora-order &lt;file&gt;</code></td></tr>
</tbody>
</table>
</div>
<h3 id="example-zephyr-gguf-model"><a class="header" href="#example-zephyr-gguf-model">Example: Zephyr GGUF model</a></h3>
<pre><code class="language-bash">mistralrs serve -p 1234 --log output.txt --format gguf -t HuggingFaceH4/zephyr-7b-beta -m TheBloke/zephyr-7B-beta-GGUF -f zephyr-7b-beta.Q5_0.gguf
</code></pre>
<h3 id="chat-templates-and-tokenizer"><a class="header" href="#chat-templates-and-tokenizer">Chat Templates and Tokenizer</a></h3>
<p>Mistral.rs will attempt to automatically load a chat template and tokenizer. This enables high flexibility across models and ensures accurate and flexible chat templating. However, this behavior can be customized.</p>
<ul>
<li><a href="#adapter-model-support">Adapter models documentation</a></li>
<li><a href="#chat-templates-and-tokenizer-customization">Chat templates documentation</a></li>
<li><a href="#examples-of-lora-and-x-lora-models">LoRA and X-LoRA examples</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vision-model-support-in-mistralrs"><a class="header" href="#vision-model-support-in-mistralrs">Vision model support in mistral.rs</a></h1>
<p>Mistral.rs supports various modalities of models, including vision models. Vision models take images and text as input and have the capability to reason over both.</p>
<p>Please see docs for the following model types:</p>
<ul>
<li>Phi 3 Vision: <a href="#phi-3-vision-model-microsoftphi-35-vision-instruct">PHI3V.md</a></li>
<li>Idefics2: <a href="#idefics-2-model-huggingfacem4idefics2-8b-chatty">IDEFICS2.md</a></li>
<li>LLaVA and LLaVANext: <a href="#llava-and-llavanext-model-llava-hf-model-family">LLAVA.md</a></li>
<li>Llama 3.2 Vision: <a href="#llama-32-vision-model-meta-llamallama-32-11b-vision-instruct">VLLAMA.md</a></li>
<li>Qwen2-VL: <a href="#qwen-2-vision-model-qwen2-vl-collection">QWEN2VL.md</a></li>
<li>Idefics 3 and Smol VLM: <a href="#idefics-3-vision-huggingfacem4idefics3-8b-llama3">IDEFICS3.md</a></li>
<li>Phi 4 Multimodal: <a href="#phi-4-multimodal-model-microsoftphi-4-multimodal-instruct">PHI4MM.md</a></li>
<li>Gemma 3: <a href="#gemma-3-model-googlegemma-3-4b-it">GEMMA3.md</a></li>
<li>Gemma 3n: <a href="#gemma-3n-model-googlegemma-3n-e4b-it">GEMMA3N.md</a></li>
<li>Mistral 3: <a href="#mistral-small-31-model-mistralaimistral-small-31-24b-instruct-2503">MISTRAL3.md</a></li>
<li>Llama 4: <a href="#llama-4-series-meta-llamallama-4-scout-17b-16e-instruct">LLAMA4.md</a></li>
<li>Qwen 3-VL: <a href="#qwen-3-vision-model-qwen3-vl-collection">QWEN3VL.md</a></li>
<li>MiniCPM-O 2.6: <a href="#minicpm-o-26-model-openbmbminicpm-o-2_6">MINICPMO_2_6.md</a></li>
</ul>
<blockquote>
<p>Note for the Python and HTTP APIs:
We follow the OpenAI specification for structuring the image messages and allow both base64 encoded images as well as a URL/path to the image. There are many examples of this, see <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v.py">this Python example</a>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="image-generation-model-support-in-mistralrs"><a class="header" href="#image-generation-model-support-in-mistralrs">Image generation model support in mistral.rs</a></h1>
<p>Mistral.rs supports various modalities of models, including image generation models. Image generation models take text as input and generate images.</p>
<p>Please see docs for the following model types:</p>
<ul>
<li>FLUX.1 <a href="#flux1-model-black-forest-labsflux1-schnell">FLUX.md</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="speech-model-support-in-mistralrs"><a class="header" href="#speech-model-support-in-mistralrs">Speech model support in mistral.rs</a></h1>
<p>Mistral.rs supports various modalities of models, including speech models. Speech models handle audio-to-text (ASR) and text-to-speech (TTS) tasks.</p>
<p>Please see docs for the following model types:</p>
<ul>
<li>Voxtral (ASR/speech-to-text): <a href="#voxtral-model-mistralaivoxtral-mini-4b-realtime-2602">VOXTRAL.md</a></li>
<li>Dia (text-to-speech): <a href="#dia-16b-model-nari-labsdia-16b">DIA.md</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="embeddings-overview"><a class="header" href="#embeddings-overview">Embeddings Overview</a></h1>
<p>Mistral.rs can load embedding models alongside chat, vision, diffusion, and speech workloads. Embedding models
produce dense vector representations that you can use for similarity search, clustering, reranking, and other
semantic tasks.</p>
<h2 id="supported-models-1"><a class="header" href="#supported-models-1">Supported models</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>Notes</th><th>Documentation</th></tr>
</thead>
<tbody>
<tr><td>EmbeddingGemma</td><td>Google’s multilingual embedding model.</td><td><a href="#embeddinggemma">EMBEDDINGGEMMA.md</a></td></tr>
<tr><td>Qwen3 Embedding</td><td>Qwen’s general-purpose embedding encoder.</td><td><a href="#qwen3-embedding">QWEN3_EMBEDDING.md</a></td></tr>
</tbody>
</table>
</div>
<blockquote>
<p>Have another embedding model you would like supported? Open an issue with the model ID and configuration.</p>
</blockquote>
<h2 id="usage-overview"><a class="header" href="#usage-overview">Usage overview</a></h2>
<ol>
<li><strong>Choose a model</strong> from the table above.</li>
<li><strong>Load it through one of our APIs:</strong>
<ul>
<li>CLI/HTTP</li>
<li>Python</li>
<li>Rust</li>
</ul>
</li>
</ol>
<p>Detailed examples for each model live in their dedicated documentation pages.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="deepseek-v2-deepseek-aideepseek-v2-lite"><a class="header" href="#deepseek-v2-deepseek-aideepseek-v2-lite">DeepSeek V2: <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite"><code>deepseek-ai/DeepSeek-V2-Lite</code></a></a></h1>
<p>The DeepSeek V2 is a mixture of expert (MoE) model featuring <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite#5-model-architecture">“Multi-head Latent Attention”</a>.</p>
<ul>
<li>Context length of <strong>32k tokens</strong> (Lite model), <strong>128k tokens</strong> (full model)</li>
<li>64 routed experts (Lite model), 160 routed experts (full model)</li>
</ul>
<pre><code class="language-bash">mistralrs run --isq 4 -m deepseek-ai/DeepSeek-V2-Lite
</code></pre>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>This model supports MoQE which can be activated in the ISQ organization parameter within the various APIs, as demonstrated below:</p>
</blockquote>
<pre><code class="language-bash">mistralrs run --isq 4 -m deepseek-ai/DeepSeek-V2-Lite --isq-organization moqe
</code></pre>
<h2 id="http-api"><a class="header" href="#http-api">HTTP API</a></h2>
<pre><code class="language-bash">mistralrs serve --isq 4 -p 1234 -m deepseek-ai/DeepSeek-V2-Lite
</code></pre>
<pre><code class="language-py">import openai

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-1"><a class="header" href="#python-sdk-1">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="deepseek-ai/DeepSeek-V2-Lite",
        arch=Architecture.DeepseekV2,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk"><a class="header" href="#rust-sdk">Rust SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/text_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, PagedAttentionMetaBuilder, TextMessageRole, TextMessages, TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("deepseek-ai/DeepSeek-V2-Lite")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .with_paged_attn(|| PagedAttentionMetaBuilder::default().build())?
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="deepseek-v3-deepseek-aideepseek-v3-deepseek-aideepseek-r1"><a class="header" href="#deepseek-v3-deepseek-aideepseek-v3-deepseek-aideepseek-r1">DeepSeek V3: <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"><code>deepseek-ai/DeepSeek-V3</code></a>, <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"><code>deepseek-ai/DeepSeek-R1</code></a></a></h1>
<p>The DeepSeek V3 is a mixture of expert (MoE) model.</p>
<pre><code class="language-bash">mistralrs run --isq 4 -m deepseek-ai/DeepSeek-R1
</code></pre>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>The non-distill versions of the DeepSeek R1 models share the DeepSeek V3 architecture.</p>
</blockquote>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>This model supports MoQE which can be activated in the ISQ organization parameter within the various APIs, as demonstrated below:</p>
</blockquote>
<pre><code class="language-bash">mistralrs run --isq 4 -m deepseek-ai/DeepSeek-R1 --isq-organization moqe
</code></pre>
<h2 id="running-the-distill-models"><a class="header" href="#running-the-distill-models">Running the distill models</a></h2>
<p>The various <a href="https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d">distillation</a> models can be run out of the box.</p>
<pre><code class="language-bash">mistralrs run --isq 4 -m deepseek-ai/DeepSeek-R1-Distill-Llama-8B
mistralrs run --isq 4 -m deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
mistralrs run --isq 4 -m deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
</code></pre>
<h2 id="http-api-1"><a class="header" href="#http-api-1">HTTP API</a></h2>
<pre><code class="language-bash">mistralrs serve --isq 4 -p 1234 -m deepseek-ai/DeepSeek-R1
</code></pre>
<pre><code class="language-py">import openai

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-2"><a class="header" href="#python-sdk-2">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="deepseek-ai/DeepSeek-R1",
        arch=Architecture.DeepseekV3,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-1"><a class="header" href="#rust-sdk-1">Rust SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/text_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, PagedAttentionMetaBuilder, TextMessageRole, TextMessages, TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("deepseek-ai/DeepSeek-R1")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .with_paged_attn(|| PagedAttentionMetaBuilder::default().build())?
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma-2-model"><a class="header" href="#gemma-2-model">Gemma 2 Model</a></h1>
<p><strong><a href="https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315">See the Gemma 2 model Collection</a></strong></p>
<p>The Gemma 2 models are a family of text-to-text decoder-only LLMs. As such, the methods to use them are the same as with all other text-to-text LLMs supported by mistral.rs.</p>
<h2 id="http-api-2"><a class="header" href="#http-api-2">HTTP API</a></h2>
<pre><code class="language-py">import openai

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-3"><a class="header" href="#python-sdk-3">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="google/gemma-2-9b-it",
        arch=Architecture.Gemma2,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma-3-model-googlegemma-3-4b-it"><a class="header" href="#gemma-3-model-googlegemma-3-4b-it">Gemma 3 Model: <a href="https://huggingface.co/google/gemma-3-4b-it"><code>google/gemma-3-4b-it</code></a></a></h1>
<p>The Gemma 3 model is a family of multimodal (text+vision) models with 128k context length. The collection can be found <a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d">here</a>, with model sizes ranging from 4B to 27B.</p>
<p>We support the Gemma 3 Model in the Rust, Python, and HTTP APIs, including ISQ for increased performance.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<h2 id="http-server-1"><a class="header" href="#http-server-1">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/gemma3.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong></p>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666">
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is this?
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>image shows Mount Washington in New Hampshire, USA. It's a prominent peak in the White Mountains, known for its extreme weather conditions and being the highest peak in the Northeastern United States. The image captures it covered in snow with a dramatic sky above. The structures at the summit are communication towers.



The winding path visible on the mountain slopes appears to be part of the Mount Washington Auto Road, a historic road that allows vehicles to drive to the summit.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 -m google/gemma-3-12b-it
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI
import httpx
import textwrap
import json


client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")


completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is this?",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust"><a class="header" href="#rust">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the Gemma 3 model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new("google/gemma-3-12b-it")
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python"><a class="header" href="#python">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/gemma3.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="google/gemma-3-12b-it",
        arch=VisionArchitecture.Gemma3,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is this?",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma-3n-model-googlegemma-3n-e4b-it"><a class="header" href="#gemma-3n-model-googlegemma-3n-e4b-it">Gemma 3n Model: <a href="https://huggingface.co/google/gemma-3n-E4B-it"><code>google/gemma-3n-E4B-it</code></a></a></h1>
<p>Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs. These models support over 140 spoken languages.</p>
<p>The Gemma 3n Model has support in the Rust, Python, and HTTP APIs. Additionally, the Gemma 3n Model supports ISQ for increased performance.</p>
<ul>
<li>
<p><strong>Full multimodal support</strong>: mistral.rs supports text, audio, and vision inputs to Gemma 3n!</p>
</li>
<li>
<p><strong>🪆 mistral.rs supports dynamically resizing the Gemma 3n model with that MatFormer architecture!</strong></p>
<p>Gemma 3n implements the MatFormer architecture, which allows one model to be resized dynamically and tune performance on resource-constrained systems.</p>
<p>Mistral.rs supports this feature!</p>
<p>You can access it using the <code>matformer_config_path</code> (<a href="https://github.com/EricLBuehler/mistral.rs/blob/master/matformer_configs/gemma3n.csv">example config</a>) and <code>matformer_slice_name</code> arguments throughout the APIs.</p>
</li>
<li>
<p><strong>Prequantized UQFF models:</strong></p>
<ul>
<li><a href="https://huggingface.co/EricB/gemma-3n-E4B-it-UQFF">Gemma 3n E4B</a></li>
<li><a href="https://huggingface.co/EricB/gemma-3n-E2B-it-UQFF">Gemma 3n E2B</a></li>
</ul>
</li>
</ul>
<h2 id="using-matformer-with-gemma-3n"><a class="header" href="#using-matformer-with-gemma-3n">Using MatFormer with Gemma 3n</a></h2>
<p>MatFormer allows you to dynamically adjust the model size based on your resource constraints. The Gemma 3n model comes with several pre-configured slices that offer different performance/resource trade-offs.</p>
<p>You can read more about MatFormer in mistral.rs <a href="#matformer-matryoshka-transformer-support">here</a>.</p>
<h3 id="available-slices"><a class="header" href="#available-slices">Available Slices</a></h3>
<p>The default configuration file (<a href="https://github.com/EricLBuehler/mistral.rs/blob/master/matformer_configs/gemma3n.csv"><code>matformer_configs/gemma3n.csv</code></a>) includes:</p>
<ul>
<li><strong>Main model</strong> (3.98B params, 35 layers) - Full model with best performance</li>
<li><strong>Config for official E2B Model</strong> (1.91B params, 30 layers) - Balanced performance/efficiency</li>
<li>Various intermediate configurations from E1.96B to E3.79B with different layer and FFN configurations</li>
</ul>
<h3 id="command-line-example"><a class="header" href="#command-line-example">Command Line Example</a></h3>
<pre><code class="language-bash"># Run with the E2.49B slice for balanced performance/efficiency
mistralrs run vision -m google/gemma-3n-E4B-it \
  --matformer-config-path matformer_configs/gemma3n.csv \
  --matformer-slice-name "Config for E2.49B (block-level)"
</code></pre>
<h3 id="python-sdk-example"><a class="header" href="#python-sdk-example">Python SDK Example</a></h3>
<pre><code class="language-python">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

# Use the E2.49B slice for balanced performance/efficiency
runner = Runner(
    which=Which.VisionPlain(
        model_id="google/gemma-3n-E4B-it",
        arch=VisionArchitecture.Gemma3n,
        matformer_config_path="matformer_configs/gemma3n.csv",
        matformer_slice_name="Config for E2.49B (block-level)",
    ),
)

# The model will use 35 layers with mixed FFN dimensions (4096 for early layers, 8192 for middle)
# This results in ~37% parameter reduction while maintaining better performance than E2B
res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="ignore",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What do you see in this image?",
                    },
                ],
            }
        ],
        max_tokens=100,
    )
)
print(res.choices[0].message.content)
</code></pre>
<h3 id="rust-sdk-example"><a class="header" href="#rust-sdk-example">Rust SDK Example</a></h3>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};
use std::path::PathBuf;

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    // Build model with MatFormer E2.49B configuration
    let model = VisionModelBuilder::new("google/gemma-3n-E4B-it")
        .with_isq(IsqType::Q4K)
        .with_matformer_config_path(PathBuf::from("matformer_configs/gemma3n.csv"))
        .with_matformer_slice_name("Config for E2.49B (block-level)".to_string())
        .with_logging()
        .build()
        .await?;

    let bytes = match reqwest::blocking::get(
        "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "Describe this image briefly.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    println!("Using E2.49B slice: 35 layers, 2.49B effective params");
    
    Ok(())
}</code></pre>
<h3 id="choosing-the-right-slice"><a class="header" href="#choosing-the-right-slice">Choosing the Right Slice</a></h3>
<ul>
<li><strong>Resource-constrained environments</strong>: Use “Config for official E2B Model” (1.91B params)</li>
<li><strong>Balanced performance</strong>: Try E2.49B to E2.98B configurations (block-level configs offer better balance)</li>
<li><strong>Maximum quality</strong>: Use “Main model” (3.98B params) or omit MatFormer configuration entirely</li>
</ul>
<p>The slice selection allows you to:</p>
<ul>
<li>Reduce memory usage proportionally to the parameter count</li>
<li>Speed up inference roughly linearly with the number of layers</li>
<li>Maintain acceptable quality for many use cases with smaller slices</li>
</ul>
<h2 id="http-server-2"><a class="header" href="#http-server-2">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/gemma3n.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666"></p>
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>Please describe this image in detail.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image captures a breathtaking, wide-angle view of a majestic mountain covered in a blanket of snow. The mountain dominates the frame, its peak reaching towards a partly cloudy sky. The snow cover is uneven, with patches of exposed dark rock and textured snow formations creating a visually interesting surface. 

A winding, snow-covered path or road snakes its way up the mountainside, appearing as a bright white line against the darker slopes. This path draws the eye upwards towards the summit, where a few structures, possibly communication towers or observation points, are visible. 

The lower slopes of the mountain are covered in a dense forest of evergreen trees, their dark green hues contrasting beautifully with the white snow. The forest extends down into a valley, hinting at a wider landscape beyond the frame. 

The sky above is a mix of pale blue and soft grey clouds, with some darker, more dramatic cloud formations near the top of the mountain. The lighting suggests it might be early morning or late afternoon, casting subtle shadows across the mountain's surface and highlighting its contours. 

The overall impression is one of grandeur, tranquility, and the raw beauty of a winter landscape. The scale of the mountain is impressive, and the winding path invites a sense of exploration and adventure.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 -m google/gemma-3n-E4B-it

# Or with MatFormer for balanced performance:
mistralrs serve vision -p 1234 -m google/gemma-3n-E4B-it \
  --matformer-config-path matformer_configs/gemma3n.csv \
  --matformer-slice-name "Config for E2.49B (block-level)"
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="ignore",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "Please describe this image in detail.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-1"><a class="header" href="#rust-1">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the Gemma 3n model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new("google/gemma-3n-E4B-it")
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "Please describe the image in detail.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python-1"><a class="header" href="#python-1">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/gemma3n.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="google/gemma-3n-E4B-it",
        arch=VisionArchitecture.Gemma3n,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="ignore",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "Please describe this image in detail.",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h3 id="openai-http-api"><a class="header" href="#openai-http-api">OpenAI HTTP API</a></h3>
<p>Audio is delivered with the <code>audio_url</code> content-type that mirrors OpenAIʼs official specification:</p>
<pre><code class="language-json">{
  "role": "user",
  "content": [
    {
      "type": "audio_url",
      "audio_url": { "url": "https://upload.wikimedia.org/wikipedia/commons/4/42/Bird_singing.ogg" }
    },
    {
      "type": "image_url",
      "image_url": { "url": "https://www.allaboutbirds.org/guide/assets/og/528129121-1200px.jpg" }
    },
    {
      "type": "text",
      "text": "Describe what is happening in this clip in as much detail as possible."
    }
  ]
}
</code></pre>
<h3 id="rust-sdk-2"><a class="header" href="#rust-sdk-2">Rust SDK</a></h3>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{AudioInput, IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new("google/gemma-3n-E4B-it")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .build()
        .await?;

    let audio_bytes = reqwest::blocking::get(
        "https://upload.wikimedia.org/wikipedia/commons/4/42/Bird_singing.ogg",
    )?
    .bytes()?
    .to_vec();
    let audio = AudioInput::from_bytes(&amp;audio_bytes)?;

    let image_bytes = reqwest::blocking::get(
        "https://www.allaboutbirds.org/guide/assets/og/528129121-1200px.jpg",
    )?
    .bytes()?
    .to_vec();
    let image = image::load_from_memory(&amp;image_bytes)?;

    let messages = VisionMessages::new()
        .add_multimodal_message(
            TextMessageRole::User,
            "Describe in detail what is happening.",
            vec![image],
            vec![audio],
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    Ok(())
}</code></pre>
<p>With this, you now have a single-call pipeline that fuses <em>sound</em>, <em>vision</em>, and <em>text</em> – all running locally through <code>mistral.rs</code>! 🔥</p>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="glm4-model"><a class="header" href="#glm4-model">GLM4 Model</a></h1>
<p><strong><a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e">See the GLM4 model Collection</a></strong></p>
<p>GLM4 is a series of open, multilingual, and multimodal large language models. The text-to-text LLM backbones in GLM4 are supported by mistral.rs.</p>
<h2 id="http-api-3"><a class="header" href="#http-api-3">HTTP API</a></h2>
<pre><code class="language-py">import openai

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-4"><a class="header" href="#python-sdk-4">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="THUDM/GLM-4-9B-0414",
        arch=Architecture.GLM4,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="glm-47-flash-moe-zai-orgglm-47-flash"><a class="header" href="#glm-47-flash-moe-zai-orgglm-47-flash">GLM-4.7-Flash (MoE): <a href="https://huggingface.co/zai-org/GLM-4.7-Flash"><code>zai-org/GLM-4.7-Flash</code></a></a></h1>
<p>GLM-4.7-Flash is a mixture of experts (MoE) model from the GLM family with MLA (Multi-head Latent Attention) architecture.</p>
<h2 id="http-api-4"><a class="header" href="#http-api-4">HTTP API</a></h2>
<p>Start the server:</p>
<pre><code class="language-bash">mistralrs serve --isq 4 -p 1234 -m zai-org/GLM-4.7-Flash
</code></pre>
<p>Send requests using an OpenAI-compatible client:</p>
<pre><code class="language-py">import openai

client = openai.Client(base_url="http://localhost:1234/v1", api_key="foobar")

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-5"><a class="header" href="#python-sdk-5">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="zai-org/GLM-4.7-Flash",
        arch=Architecture.GLM4MoeLite,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-3"><a class="header" href="#rust-sdk-3">Rust SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/text_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, TextMessages, TextModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("zai-org/GLM-4.7-Flash")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="glm-47-moe-zai-orgglm-47"><a class="header" href="#glm-47-moe-zai-orgglm-47">GLM-4.7 (MoE): <a href="https://huggingface.co/zai-org/GLM-4.7"><code>zai-org/GLM-4.7</code></a></a></h1>
<p>GLM-4.7 is a mixture of experts (MoE) model from the GLM family with standard GQA attention and partial RoPE.</p>
<h2 id="http-api-5"><a class="header" href="#http-api-5">HTTP API</a></h2>
<p>Start the server:</p>
<pre><code class="language-bash">mistralrs serve --isq 4 -p 1234 -m zai-org/GLM-4.7
</code></pre>
<p>Send requests using an OpenAI-compatible client:</p>
<pre><code class="language-py">import openai

client = openai.Client(base_url="http://localhost:1234/v1", api_key="foobar")

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-6"><a class="header" href="#python-sdk-6">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="zai-org/GLM-4.7",
        arch=Architecture.GLM4Moe,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-4"><a class="header" href="#rust-sdk-4">Rust SDK</a></h2>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, TextMessages, TextModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("zai-org/GLM-4.7")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gpt-oss"><a class="header" href="#gpt-oss">GPT-OSS</a></h1>
<p>GPT-OSS is a Mixture of Experts (MoE) language model with specialized attention mechanisms and efficient quantization. Key features include:</p>
<ul>
<li>MXFP4 quantized MoE experts for efficient inference</li>
<li>Per-head attention sinks for improved attention patterns</li>
<li>YARN RoPE scaling for extended context</li>
<li>Hybrid cache supporting both full and sliding window attention</li>
</ul>
<pre><code class="language-bash">mistralrs run -m openai/gpt-oss-20b
</code></pre>
<blockquote>
<p>Note: GPT-OSS MoE experts are pre-quantized in MXFP4 format. ISQ can be applied to attention layers only.</p>
</blockquote>
<blockquote>
<p>Note: PagedAttention is not supported for GPT-OSS due to custom attention with sinks.</p>
</blockquote>
<h2 id="http-api-6"><a class="header" href="#http-api-6">HTTP API</a></h2>
<p>You can find a more detailed example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/gpt_oss.py">here</a>.</p>
<pre><code class="language-bash">mistralrs serve -p 1234 -m openai/gpt-oss-20b
</code></pre>
<pre><code class="language-py">import openai

client = openai.OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})

while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-7"><a class="header" href="#python-sdk-7">Python SDK</a></h2>
<p>You can find a more detailed example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/gpt_oss.py">here</a>.</p>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="openai/gpt-oss-20b",
        arch=Architecture.GptOss,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-5"><a class="header" href="#rust-sdk-5">Rust SDK</a></h2>
<p>You can find a more detailed example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/text_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{TextMessageRole, TextMessages, TextModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("openai/gpt-oss-20b")
        .with_logging()
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="technical-details"><a class="header" href="#technical-details">Technical Details</a></h2>
<h3 id="mxfp4-quantization"><a class="header" href="#mxfp4-quantization">MXFP4 Quantization</a></h3>
<p>GPT-OSS MoE experts use MXFP4 (4-bit microscaling floating point) quantization for compact and efficient storage:</p>
<ul>
<li><code>gate_up_proj</code>: Packed experts with MXFP4 weights</li>
<li><code>down_proj</code>: Packed experts with MXFP4 weights</li>
<li>Scales stored at 1 byte per 32 elements</li>
</ul>
<h3 id="attention-with-sinks"><a class="header" href="#attention-with-sinks">Attention with Sinks</a></h3>
<p>The model uses per-head attention sinks that are added to attention logits before softmax, helping to regularize attention patterns. This custom attention mechanism is incompatible with PagedAttention.</p>
<h3 id="isq-support"><a class="header" href="#isq-support">ISQ Support</a></h3>
<p>In-situ quantization (ISQ) can be applied to attention projection layers:</p>
<ul>
<li><code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>, <code>o_proj</code></li>
<li><code>lm_head</code></li>
</ul>
<p>MoE expert layers are already MXFP4 quantized and excluded from ISQ.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="qwen-3-collection"><a class="header" href="#qwen-3-collection">Qwen 3: <a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"><code>collection</code></a></a></h1>
<p>The Qwen 3 family is a collection of hybrid reasoning MoE and non-MoE models ranging from 0.6b to 235b parameters.</p>
<pre><code class="language-bash">mistralrs run --isq 4 -m Qwen/Qwen3-8B
mistralrs run --isq 4 -m Qwen/Qwen3-30B-A3B
</code></pre>
<blockquote>
<p>Note: mistral.rs can load all <a href="https://huggingface.co/Qwen/Qwen3-14B-FP8">FP8 pre-quantized versions</a> natively! Simply replace the model ID.</p>
</blockquote>
<blockquote>
<p>Note: tool calling support is fully implemented for the Qwen 3 models, including agentic web search.</p>
</blockquote>
<h2 id="enabling-thinking"><a class="header" href="#enabling-thinking">Enabling thinking</a></h2>
<p>The Qwen 3 models are hybrid reasoning models which can be controlled at inference-time. <strong>By default, reasoning is enabled for these models.</strong> To dynamically control this, it is recommended to either add <code>/no_think</code> or <code>/think</code> to your prompt. Alternatively, you can specify the <code>enable_thinking</code> flag as detailed by the API-specific examples.</p>
<h2 id="http-api-7"><a class="header" href="#http-api-7">HTTP API</a></h2>
<p>You can find a more detailed example demonstrating enabling/disabling thinking <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/qwen3.py">here</a>.</p>
<pre><code class="language-bash">mistralrs serve --isq 4 -p 1234 -m Qwen/Qwen3-8B
</code></pre>
<pre><code class="language-py">import openai

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
        # enable_thinking=False,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-8"><a class="header" href="#python-sdk-8">Python SDK</a></h2>
<p>You can find a more detailed example demonstrating enabling/disabling thinking <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/qwen3.py">here</a>.</p>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="Qwen/Qwen3-8B",
        arch=Architecture.Qwen3,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
        # enable_thinking=False,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-6"><a class="header" href="#rust-sdk-6">Rust SDK</a></h2>
<p>You can find a more detailed example demonstrating enabling/disabling thinking <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/text_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, PagedAttentionMetaBuilder, TextMessageRole, TextMessages, TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("Qwen/Qwen3-8B")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .with_paged_attn(|| PagedAttentionMetaBuilder::default().build())?
        .build()
        .await?;

    let messages = TextMessages::new()
        // .enable_thinking(false)
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="smollm3-huggingfacetbsmollm3-3b"><a class="header" href="#smollm3-huggingfacetbsmollm3-3b">SmolLM3: <a href="https://huggingface.co/HuggingFaceTB/SmolLM3-3B"><code>HuggingFaceTB/SmolLM3-3B</code></a></a></h1>
<p>SmolLM3 is a 3B parameter long-context hybrid reasoning language model. It supports 6 languages, advanced reasoning and long context. SmolLM3 is a fully open model that offers strong performance at the 3B–4B scale.</p>
<p><strong>Default, easiest:</strong></p>
<pre><code class="language-bash">mistralrs run --isq 8 -m HuggingFaceTB/SmolLM3-3B
</code></pre>
<p><strong>UQFF prequantized:</strong></p>
<pre><code class="language-bash">mistralrs run -m EricB/SmolLM3-3B-UQFF --from-uqff smollm33b-q4k-0.uqff
</code></pre>
<blockquote>
<p>Note: tool calling support is fully implemented for the SmolLM3 models, including agentic web search.</p>
</blockquote>
<blockquote>
<p>Check out prequantized UQFF SmolLM3 here: https://huggingface.co/EricB/SmolLM3-3B-UQFF</p>
</blockquote>
<h2 id="enabling-thinking-1"><a class="header" href="#enabling-thinking-1">Enabling thinking</a></h2>
<p>The SmolLM3 models are hybrid reasoning models which can be controlled at inference-time. <strong>By default, reasoning is enabled for these models.</strong> To dynamically control this, it is recommended to either add <code>/no_think</code> or <code>/think</code> to your prompt. Alternatively, you can specify the <code>enable_thinking</code> flag as detailed by the API-specific examples.</p>
<h2 id="http-api-8"><a class="header" href="#http-api-8">HTTP API</a></h2>
<p>You can find a more detailed example demonstrating enabling/disabling thinking <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/smollm3.py">here</a>.</p>
<pre><code class="language-bash">mistralrs serve --isq 8 -p 1234 -m HuggingFaceTB/SmolLM3-3B
</code></pre>
<pre><code class="language-py">import openai

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
        # enable_thinking=False,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-9"><a class="header" href="#python-sdk-9">Python SDK</a></h2>
<p>You can find a more detailed example demonstrating enabling/disabling thinking <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/smollm3.py">here</a>.</p>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="HuggingFaceTB/SmolLM3-3B",
        arch=Architecture.SmolLm3,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
        # enable_thinking=False,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-7"><a class="header" href="#rust-sdk-7">Rust SDK</a></h2>
<p>You can find a more detailed example demonstrating enabling/disabling thinking <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/text_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, PagedAttentionMetaBuilder, TextMessageRole, TextMessages, TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("HuggingFaceTB/SmolLM3-3B")
        .with_isq(IsqType::Q8_0)
        .with_logging()
        .with_paged_attn(|| PagedAttentionMetaBuilder::default().build())?
        .build()
        .await?;

    let messages = TextMessages::new()
        // .enable_thinking(false)
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="idefics-2-model-huggingfacem4idefics2-8b-chatty"><a class="header" href="#idefics-2-model-huggingfacem4idefics2-8b-chatty">Idefics 2 Model: <a href="https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty"><code>HuggingFaceM4/idefics2-8b-chatty</code></a></a></h1>
<p>The Idefics 2 Model has support in the Rust, Python, and HTTP APIs. The Idefics 2 Model also supports ISQ for increased performance.</p>
<blockquote>
<p>Note: Some of examples use our <a href="https://huggingface.co/collections/lamm-mit/cephalo-664f3342267c4890d2f46b33">Cephalo model series</a> but could be used with any model ID.</p>
</blockquote>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<h2 id="http-server-3"><a class="header" href="#http-server-3">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/idefics2.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg" width="1000" height="666"></p>
<p><strong>Prompt:</strong></p>
<pre><code>What is shown in this image?
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image depicts a group of orange ants climbing over a black pole. The ants are moving in the same direction, forming a line as they ascend the pole.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 --isq 4 -m HuggingFaceM4/idefics2-8b-chatty
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is shown in this image?",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-2"><a class="header" href="#rust-2">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the Idefics 2 model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new(
        "HuggingFaceM4/idefics2-8b-chatty",
    )
    .with_isq(IsqType::Q4K)
    .with_logging()
    .build()
    .await?;

    let bytes = match reqwest::blocking::get(
        "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_idefics_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        image,
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}
</code></pre>
<h2 id="python-2"><a class="header" href="#python-2">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="lamm-mit/Cephalo-Idefics-2-vision-8b-beta",
        arch=VisionArchitecture.Idefics2,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is shown in this image?",
                    },
                ],
            },
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="idefics-3-vision-huggingfacem4idefics3-8b-llama3"><a class="header" href="#idefics-3-vision-huggingfacem4idefics3-8b-llama3">Idefics 3 Vision: <a href="https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3"><code>HuggingFaceM4/Idefics3-8B-Llama3</code></a></a></h1>
<p>Mistral.rs supports the Idefics 3 vision model, with examples in the Rust, Python, and HTTP APIs. ISQ quantization is supported to allow running the model with less memory requirements.</p>
<p>UQFF quantizations are also available.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<blockquote>
<p>Note: When using device mapping or model topology, only the text model and its layers will be managed. This is because it contains most of the model parameters. Check the Hugging Face text model config for more information or raise an issue.</p>
</blockquote>
<h2 id="toc"><a class="header" href="#toc">ToC</a></h2>
<ul>
<li><a href="#idefics-3-vision-huggingfacem4idefics3-8b-llama3">Idefics 3 Vision: <code>HuggingFaceM4/Idefics3-8B-Llama3</code></a>
<ul>
<li><a href="#toc">ToC</a></li>
<li><a href="#using-the--smol-vlm-models">Using the 🤗 Smol VLM models</a></li>
<li><a href="#interactive-mode">Interactive mode</a></li>
<li><a href="#http-server-4">HTTP server</a></li>
<li><a href="#rust-3">Rust</a></li>
<li><a href="#python-3">Python</a></li>
<li><a href="#uqff-models">UQFF models</a></li>
</ul>
</li>
</ul>
<h2 id="using-the--smol-vlm-models"><a class="header" href="#using-the--smol-vlm-models">Using the <a href="HuggingFaceTB/SmolVLM-Instruct">🤗 Smol VLM</a> models</a></h2>
<p>Simply substitute the Idefics 3 model ID (<code>HuggingFaceM4/Idefics3-8B-Llama3</code>) with the Smol VLM one (<code>HuggingFaceTB/SmolVLM-Instruct</code>)!</p>
<h2 id="interactive-mode"><a class="header" href="#interactive-mode">Interactive mode</a></h2>
<p>Mistral.rs supports interactive mode for vision models! It is an easy way to interact with the model.</p>
<ol>
<li>Start up interactive mode with the Idefics 3 model</li>
</ol>
<pre><code>mistralrs run vision --isq 4 -m HuggingFaceM4/Idefics3-8B-Llama3
</code></pre>
<ol start="2">
<li>Ask a question</li>
</ol>
<pre><code>&gt; \image https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Rosa_Precious_platinum.jpg/220px-Rosa_Precious_platinum.jpg What is this image?
The image depicts a single, large, red rose in full bloom. The rose is positioned against a blurred background that suggests a natural setting, possibly outdoors. The petals of the rose are vividly red with a slight sheen, indicating that they are wet, likely from recent rainfall or dew. The petals are tightly packed and have a velvety texture, which is characteristic of roses. The edges of the petals are slightly curled and appear to be glistening with water droplets, enhancing the overall freshness and beauty of the flower.

The stem of the rose is visible and appears to be green, with a few small thorns scattered along its length. The stem is slender and supports the weight of the large, showy head of the rose. The leaves that accompany the stem are not fully visible in the image but are implied by the presence of the stem.

The background is out of focus, which helps to emphasize the rose as the main subject of the image. The blurred background suggests a natural environment, possibly a garden or a field, with hints of greenery and possibly other flowers or plants. The lighting in the image is natural, likely from sunlight, which casts soft shadows on the petals and adds depth to the scene.

The overall composition of the image focuses on the rose, making it the central point of interest. The wetness of the petals adds a dynamic element to the stillness of the flower, giving it a sense of life and vitality. This could symbolize themes of beauty, nature, and perhaps even passion or love.

In summary, this image captures a single red rose in full bloom with wet petals against a blurred natural background. The rose is the focal point, with its vibrant red color and glistening petals drawing attention. The natural lighting and out-of-focus background enhance the beauty and freshness of the flower.
</code></pre>
<ol start="4">
<li>Continue the chat by passing another image.</li>
</ol>
<pre><code>&gt; \image https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Rosa_Precious_platinum.jpg/220px-Rosa_Precious_platinum.jpg What is this image?
The image depicts a single, large, red rose in full bloom. The rose is positioned against a blurred background that suggests a natural setting, possibly outdoors. The petals of the rose are vividly red with a slight sheen, indicating that they are wet, likely from recent rainfall or dew. The petals are tightly packed and have a velvety texture, which is characteristic of roses. The edges of the petals are slightly curled and appear to be glistening with water droplets, enhancing the overall freshness and beauty of the flower.

The stem of the rose is visible and appears to be green, with a few small thorns scattered along its length. The stem is slender and supports the weight of the large, showy head of the rose. The leaves that accompany the stem are not fully visible in the image but are implied by the presence of the stem.

The background is out of focus, which helps to emphasize the rose as the main subject of the image. The blurred background suggests a natural environment, possibly a garden or a field, with hints of greenery and possibly other flowers or plants. The lighting in the image is natural, likely from sunlight, which casts soft shadows on the petals and adds depth to the scene.

The overall composition of the image focuses on the rose, making it the central point of interest. The wetness of the petals adds a dynamic element to the stillness of the flower, giving it a sense of life and vitality. This could symbolize themes of beauty, nature, and perhaps even passion or love.

In summary, this image captures a single red rose in full bloom with wet petals against a blurred natural background. The rose is the focal point, with its vibrant red color and glistening petals drawing attention. The natural lighting and out-of-focus background enhance the beauty and freshness of the flower.
&gt; \image https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg What mountain is this?
The mountain is Mount Washington.
</code></pre>
<h2 id="http-server-4"><a class="header" href="#http-server-4">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/idefics3.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666"></p>
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is shown in this image? Write a detailed response analyzing the scene.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image depicts a majestic mountain landscape under a partly cloudy sky, characterized by its rugged and snow-covered peaks. The mountain is prominently featured in the center of the image, showcasing its expansive and undulating terrain. The summit of the mountain is capped with snow, indicating that it might be winter or early springtime.

The slopes of the mountain are steep and uneven, covered with patches of snow that appear to have been recently fallen or freshly groomed for skiing or other winter activities. There are visible ski trails descending from the summit down towards what seems to be a valley below, suggesting that this location could be a popular ski resort area.

In addition to the main peak, there are smaller hills and ridges surrounding it on both sides. These secondary peaks also have varying degrees of snow cover but appear less prominent than the central peak.

The sky above is mostly overcast with clouds covering most parts but allowing some sunlight to peek through in certain areas, casting soft shadows on parts of the mountainside. This lighting suggests that it might not be midday yet as there isn't an intense brightness typical for noon hours.

On closer inspection near one side of this grandeur scene stands tall trees without leaves; their bare branches starkly contrasting against both white snow and blue sky create an interesting... (cut off)
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 --isq 4 -m HuggingFaceM4/Idefics3-8B-Llama3
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-3"><a class="header" href="#rust-3">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

const MODEL_ID: &amp;str = "HuggingFaceM4/Idefics3-8B-Llama3";

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new(MODEL_ID)
        .with_isq(IsqType::Q8_0)
        .with_logging()
        .build()
        .await?;

    let bytes = match reqwest::blocking::get(
        "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<hr>
<h2 id="python-3"><a class="header" href="#python-3">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/idefics3.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="HuggingFaceM4/Idefics3-8B-Llama3",
        arch=VisionArchitecture.Idefics3,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is shown in this image?",
                    },
                ],
            },
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<h2 id="uqff-models"><a class="header" href="#uqff-models">UQFF models</a></h2>
<p>Coming soon!</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="llava-and-llavanext-model-llava-hf-model-family"><a class="header" href="#llava-and-llavanext-model-llava-hf-model-family">LLaVA and LLaVANext Model: <code>llava-hf model family</code></a></h1>
<p>The <a href="https://arxiv.org/abs/2310.03744">LLaVA</a> and <a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVANext</a> are great multimodal models that can handle both text and vision inputs.</p>
<p>This implementation supports both LLaVA and LLaVANext(which adds multi resolution image processing) and two types of LLM base model: llama and mistral. Currently it is tested on:</p>
<ul>
<li>llava-hf/llava-v1.6-mistral-7b-hf</li>
<li>llava-hf/llava-v1.6-vicuna-7b-hf</li>
<li>llava-hf/llava-1.5-7b-hf</li>
</ul>
<p>The LLaVA and LLaVANext Model has support in the Rust, Python, and HTTP APIs. The LLaVA and LLaVANext Model also supports ISQ for increased performance.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<h2 id="http-server-5"><a class="header" href="#http-server-5">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/llava_next.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666"></p>
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is shown in this image?
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Text: The image shows a steep, snow-covered hillside with a pine tree on the right side, close to the top. The landscape appears to be a mountainous area with winter conditions. There are no visible humans or permanent structures in the immediate vicinity that suggest this is a summer or recreational location. It's likely a cold, snowy day or season, and the slopes might be part of a mountainous region.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 --isq 4 -m llava-hf/llava-v1.6-mistral-7b-hf
# or for vicuna backend, specify the chat template:
mistralrs serve vision -p 1234 --isq 4 -c ./chat_templates/vicuna.json -m llava-hf/llava-v1.6-vicuna-7b-hf
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is shown in this image?",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-4"><a class="header" href="#rust-4">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the LLaVA and LLaVANext model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new(
        "llava-hf/llava-v1.6-mistral-7b-hf",
    )
    .with_isq(IsqType::Q4K)
    .with_logging()
    .build()
    .await?;

    let bytes = match reqwest::blocking::get(
        "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_llava_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        image,
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python-4"><a class="header" href="#python-4">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/llava_next.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="llava-hf/llava-v1.6-mistral-7b-hf",
        arch=VisionArchitecture.LLaVANext,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is shown in this image?",
                    },
                ],
            },
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="llama-32-vision-model-meta-llamallama-32-11b-vision-instruct"><a class="header" href="#llama-32-vision-model-meta-llamallama-32-11b-vision-instruct">Llama 3.2 Vision Model: <a href="https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct"><code>meta-llama/Llama-3.2-11B-Vision-Instruct</code></a></a></h1>
<p>Mistral.rs supports the Llama 3.2 vision model, with examples in the Rust, Python, and HTTP APIs. ISQ quantization is supported to allow running the model with less memory requirements.</p>
<p>UQFF quantizations are also available.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<blockquote>
<p>Note: Some examples use the <a href="lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k">Cephalo Llama 3.2 model</a>, a member of the <a href="https://huggingface.co/collections/lamm-mit/cephalo-664f3342267c4890d2f46b33">Cephalo</a> model collection. This model is finetune of Llama 3.2 with enhanced capabilities in scientific images. To use the base Llama 3.2 Vision model, simply use the <a href="https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct">associated model ID</a>.</p>
</blockquote>
<blockquote>
<p>Note: When using device mapping or model topology, only the text model and its layers will be managed. This is because it contains most of the model parameters. <em>The text model has 40 layers</em>.</p>
</blockquote>
<h2 id="toc-1"><a class="header" href="#toc-1">ToC</a></h2>
<ul>
<li><a href="#llama-32-vision-model-meta-llamallama-32-11b-vision-instruct">Llama 3.2 Vision Model: <code>meta-llama/Llama-3.2-11B-Vision-Instruct</code></a>
<ul>
<li><a href="#toc-1">ToC</a></li>
<li><a href="#interactive-mode-1">Interactive mode</a></li>
<li><a href="#http-server-6">HTTP server</a></li>
<li><a href="#rust-5">Rust</a></li>
<li><a href="#python-5">Python</a></li>
<li><a href="#uqff-models-1">UQFF models</a></li>
</ul>
</li>
</ul>
<h2 id="interactive-mode-1"><a class="header" href="#interactive-mode-1">Interactive mode</a></h2>
<p>Mistral.rs supports interactive mode for vision models! It is an easy way to interact with the model.</p>
<p>https://github.com/user-attachments/assets/4d11c35c-9ea2-42b8-8cab-5f7e8e2ee9ff</p>
<ol>
<li>Start up interactive mode with the Llama 3.2 model</li>
</ol>
<pre><code>mistralrs run vision --isq 4 -m lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k
</code></pre>
<ol start="2">
<li>Say hello!</li>
</ol>
<pre><code>&gt; Hello!
How can I assist you today?
</code></pre>
<ol start="3">
<li>Pass the model an image and ask a question.</li>
</ol>
<pre><code>&gt; Hello!
How can I assist you today?
&gt; \image https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Rosa_Precious_platinum.jpg/220px-Rosa_Precious_platinum.jpg What is this image?
The image shows a close-up view of a rose flower with dew drops on its petals. The rose is in full bloom, with its petals unfolding and displaying vibrant pink coloration. The dew drops on the petals create a delicate, glistening effect, adding to the overall visual appeal of the flower. The background is blurred, focusing attention on the intricate details of the rose.
</code></pre>
<ol start="4">
<li>Continue the chat by passing another image.</li>
</ol>
<pre><code>&gt; Hello!
How can I assist you today?
&gt; \image https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Rosa_Precious_platinum.jpg/220px-Rosa_Precious_platinum.jpg What is this image?
The image shows a close-up view of a rose flower with dew drops on its petals. The rose is in full bloom, with its petals unfolding and displaying vibrant pink coloration. The dew drops on the petals create a delicate, glistening effect, adding to the overall visual appeal of the flower. The background is blurred, focusing attention on the intricate details of the rose.
&gt; \image https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg What mountain is this?
The image appears to be of Mount Washington, which is the highest peak in the Northeastern United States. It is located in the White Mountains of New Hampshire and is known for its extreme weather conditions, including high winds and low temperatures. The mountain's summit reaches an elevation of approximately 6,288 feet (1,917 meters) above sea level.
</code></pre>
<h2 id="http-server-6"><a class="header" href="#http-server-6">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/llama_vision.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666"></p>
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is shown in this image? Write a detailed response analyzing the scene.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image shows Mount Washington, the highest peak in the Northeastern United States, located in the White Mountains of New Hampshire. The scene captures the mountain's rugged terrain and varied landscape features. 

In the foreground, there are dense forests of coniferous trees, primarily spruce and fir, which are typical of the region's boreal forest ecosystem. The trees are densely packed, indicating a high level of vegetation cover and biodiversity.

Moving upwards, the image reveals rocky outcroppings and boulders scattered across the slope, indicating the mountain's geological history of glacial activity. The presence of these rocks suggests that the area was once covered by ice sheets during the last ice age, which carved out the landscape and left behind a mix of boulders and talus slopes.

In the mid-ground, the image shows a series of ridges and valleys, which are characteristic of the mountain's glacially sculpted terrain. These features were formed by the movement of ice sheets that carved out U-shaped valleys and left behind a series of rounded hills and ridges.

At the summit, there is a prominent observation tower or weather station, which is likely used for scientific research and weather monitoring. The structure is situated at an elevation of approximately 6,288 feet (1,917 meters) above sea level, making it one of the highest points in the region.

The image also captures the atmospheric conditions on Mount Washington, with clouds and mist visible in the background. The mountain's unique location in a region where cold Arctic air meets warm moist air from the Gulf Stream creates a unique microclimate known as the "Home Rule," where extreme weather conditions can occur.

Overall, the image showcases the diverse geological and ecological features of Mount Washington, highlighting its role as a significant natural landmark in the Northeastern United States.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 --isq 4 -m lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-5"><a class="header" href="#rust-5">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

const MODEL_ID: &amp;str = "lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k";

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new(MODEL_ID)
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<hr>
<h2 id="python-5"><a class="header" href="#python-5">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/llama_vision.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

MODEL_ID = "lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k"

runner = Runner(
    which=Which.VisionPlain(
        model_id=MODEL_ID,
        arch=VisionArchitecture.VLlama,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<h2 id="uqff-models-1"><a class="header" href="#uqff-models-1">UQFF models</a></h2>
<p><a href="#universal-quantized-file-format-uqff">UQFF</a> is a quantized file format similar to GGUF based on ISQ. It removes the memory and compute requirements that come with ISQ by providing ready-made quantizations! The key advantage over GGUF is the flexibility to store multiple quantizations in one file.</p>
<p>We provide UQFF files (<a href="https://huggingface.co/EricB/Llama-3.2-11B-Vision-Instruct-UQFF">EricB/Llama-3.2-11B-Vision-Instruct-UQFF</a>) for this Llama 3.2 Vision model.</p>
<p>You can use these UQFF files to easily use quantized versions of Llama 3.2 Vision.</p>
<p>For example:</p>
<pre><code class="language-bash">mistralrs run -m meta-llama/Llama-3.2-11B-Vision-Instruct --from-uqff EricB/Llama-3.2-11B-Vision-Instruct-UQFF/llama-3.2-11b-vision-q4k.uqff
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="llama-4-series-meta-llamallama-4-scout-17b-16e-instruct"><a class="header" href="#llama-4-series-meta-llamallama-4-scout-17b-16e-instruct">Llama 4 Series: <a href="https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct"><code>meta-llama/Llama-4-Scout-17B-16E-Instruct</code></a></a></h1>
<p><strong>🚧 We are preparing a collection of UQFF quantized models! 🚧</strong></p>
<hr>
<p>The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences.</p>
<p><strong>Architecture:</strong></p>
<ul>
<li>Efficient inference: 17B activated parameters</li>
<li>Very sparse: 1 activated expert for both Scout (of 16), and Maverick (of 128)</li>
<li>RoPE enhancement: iRoPE enables high context-length functionality</li>
</ul>
<p><strong>Integration in mistral.rs:</strong></p>
<ul>
<li>Tool calling + <a href="#web-search-tool-in-mistralrs">Automatic web search</a></li>
<li>ISQ</li>
<li>Rust, Python and HTTP APIs</li>
</ul>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<h2 id="http-server-7"><a class="header" href="#http-server-7">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/llama4.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong></p>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg">
<h6><a href="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>Please describe this image in detail.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image presents a breathtaking mountain landscape, with a snow-capped peak dominating the scene. The mountain's rugged terrain is characterized by numerous ridges and valleys, while its summit is adorned with several structures that appear to be communication towers or antennas.

**Key Features:**

* **Mountain:** The mountain is the central focus of the image, showcasing a mix of snow-covered and bare areas.
* **Sky:** The sky above the mountain features a dramatic display of clouds, with dark grey clouds at the top gradually giving way to lighter blue skies towards the bottom.
* **Valley:** In the foreground, a valley stretches out, covered in trees that are mostly bare, suggesting a winter setting.
* **Lighting:** The lighting in the image is striking, with the sun casting a warm glow on the mountain's snow-covered slopes while leaving the surrounding areas in shadow.

**Overall Impression:**

The image exudes a sense of serenity and majesty, capturing the beauty of nature in a dramatic and awe-inspiring way. The contrast between the snow-covered mountain and the bare trees in the valley creates a visually appealing scene that invites the viewer to appreciate the natural world.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 --isq 4 -m meta-llama/Llama-4-Scout-17B-16E-Instruct
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI
import httpx
import textwrap
import json


client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")


completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "Please describe this image in detail.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-6"><a class="header" href="#rust-6">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the Llama 4 model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new(
        "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    )
    .with_isq(IsqType::Q4K)
    .with_logging()
    .build()
    .await?;

    let bytes = match reqwest::blocking::get(
        "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is this?",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python-6"><a class="header" href="#python-6">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/llama4.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="meta-llama/Llama-4-Scout-17B-16E-Instruct",
        arch=VisionArchitecture.Llama4,
    ),
    in_situ_quant="4",
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is this?",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="minicpm-o-26-model-openbmbminicpm-o-2_6"><a class="header" href="#minicpm-o-26-model-openbmbminicpm-o-2_6">MiniCPM-O 2.6 Model: <a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"><code>openbmb/MiniCPM-o-2_6</code></a></a></h1>
<p>Mistral.rs supports the MiniCPM-O 2.6 model, with examples in the Rust, Python, and HTTP APIs. ISQ quantization is supported to allow running the model with less memory requirements.</p>
<p>UQFF quantizations are coming soon.</p>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>Only the vision portion of this model has been implemented. No audio features are supported yet.</p>
</blockquote>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<h2 id="toc-2"><a class="header" href="#toc-2">ToC</a></h2>
<ul>
<li><a href="#minicpm-o-26-model-openbmbminicpm-o-2_6">MiniCPM-O 2.6 Model: <code>openbmb/MiniCPM-o-2_6</code></a>
<ul>
<li><a href="#toc-2">ToC</a></li>
<li><a href="#interactive-mode-2">Interactive mode</a></li>
<li><a href="#http-server-8">HTTP server</a></li>
<li><a href="#rust-7">Rust</a></li>
<li><a href="#python-7">Python</a></li>
</ul>
</li>
</ul>
<h2 id="interactive-mode-2"><a class="header" href="#interactive-mode-2">Interactive mode</a></h2>
<p>Mistral.rs supports interactive mode for vision models! It is an easy way to interact with the model.</p>
<ol>
<li>Start up interactive mode with the MiniCPM-O 2.6 Model model</li>
</ol>
<pre><code>mistralrs run vision --isq 4 -m openbmb/MiniCPM-o-2_6
</code></pre>
<ol start="2">
<li>Say hello!</li>
</ol>
<pre><code>&gt; Hello!
How can I assist you today?
</code></pre>
<ol start="3">
<li>Pass the model an image and ask a question.</li>
</ol>
<pre><code>&gt; Hello!
How can I assist you today?
&gt; \image https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Rosa_Precious_platinum.jpg/220px-Rosa_Precious_platinum.jpg What is this image?
The image shows a close-up view of a rose flower with dew drops on its petals. The rose is in full bloom, with its petals unfolding and displaying vibrant pink coloration. The dew drops on the petals create a delicate, glistening effect, adding to the overall visual appeal of the flower. The background is blurred, focusing attention on the intricate details of the rose.
</code></pre>
<h2 id="http-server-8"><a class="header" href="#http-server-8">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/minicpmo_2_6.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666"></p>
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is shown in this image? Write a detailed response analyzing the scene.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image shows Mount Washington, the highest peak in the Northeastern United States, located in the White Mountains of New Hampshire. The scene captures the mountain's rugged terrain and varied landscape features. 

In the foreground, there are dense forests of coniferous trees, primarily spruce and fir, which are typical of the region's boreal forest ecosystem. The trees are densely packed, indicating a high level of vegetation cover and biodiversity.

Moving upwards, the image reveals rocky outcroppings and boulders scattered across the slope, indicating the mountain's geological history of glacial activity. The presence of these rocks suggests that the area was once covered by ice sheets during the last ice age, which carved out the landscape and left behind a mix of boulders and talus slopes.

In the mid-ground, the image shows a series of ridges and valleys, which are characteristic of the mountain's glacially sculpted terrain. These features were formed by the movement of ice sheets that carved out U-shaped valleys and left behind a series of rounded hills and ridges.

At the summit, there is a prominent observation tower or weather station, which is likely used for scientific research and weather monitoring. The structure is situated at an elevation of approximately 6,288 feet (1,917 meters) above sea level, making it one of the highest points in the region.

The image also captures the atmospheric conditions on Mount Washington, with clouds and mist visible in the background. The mountain's unique location in a region where cold Arctic air meets warm moist air from the Gulf Stream creates a unique microclimate known as the "Home Rule," where extreme weather conditions can occur.

Overall, the image showcases the diverse geological and ecological features of Mount Washington, highlighting its role as a significant natural landmark in the Northeastern United States.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 --isq 4 -m openbmb/MiniCPM-o-2_6
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-7"><a class="header" href="#rust-7">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

const MODEL_ID: &amp;str = "openbmb/MiniCPM-o-2_6";

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new(MODEL_ID)
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<hr>
<h2 id="python-7"><a class="header" href="#python-7">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/minicpmo_2_6.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

MODEL_ID = "openbmb/MiniCPM-o-2_6"

runner = Runner(
    which=Which.VisionPlain(
        model_id=MODEL_ID,
        arch=VisionArchitecture.MiniCpmO,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mistral-small-31-model-mistralaimistral-small-31-24b-instruct-2503"><a class="header" href="#mistral-small-31-model-mistralaimistral-small-31-24b-instruct-2503">Mistral Small 3.1 Model: <a href="https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"><code>mistralai/Mistral-Small-3.1-24B-Instruct-2503</code></a></a></h1>
<p>The Mistral Small 3.1 model is a strong multimodal (text+vision) model with 128k context length, function calling, and strong visual understanding.</p>
<p>We support the Mistral 3 Model in the Rust, Python, and HTTP APIs, including ISQ for increased performance.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<h2 id="tool-calling-with-mistral-small-31"><a class="header" href="#tool-calling-with-mistral-small-31">Tool calling with Mistral Small 3.1</a></h2>
<p>The Mistral Small 3.1 model itself does not come with the correct JINJA chat template to enable tool calling. We provide a chat template for
tool calling with Mistral Small 3.1, and you can use it by specifying the <code>jinja_explicit</code> parameter in the various APIs. For example:</p>
<pre><code class="language-bash">mistralrs serve -p 1234 --isq 4 --jinja-explicit chat_templates/mistral_small_tool_call.jinja -m mistralai/Mistral-Small-3.1-24B-Instruct-2503
</code></pre>
<h2 id="http-server-9"><a class="header" href="#http-server-9">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/mistral3.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong></p>
<img src="https://upload.wikimedia.org/wikipedia/commons/f/fd/Pink_flower.jpg">
<h6><a href="https://upload.wikimedia.org/wikipedia/commons/f/fd/Pink_flower.jpg">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is this?
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image shows a close-up of a vibrant flower with pink petals and a central cluster of yellowish-brown stamens. This flower appears to be from the genus *Gazania*, commonly known as treasure flowers or gazanias. These flowers are known for their daisy-like appearance and bright colors.

Gazania flowers typically have ray florets (the petal-like structures) that can change color based on light conditions—often appearing more vibrant in direct sunlight. They are popular in gardens for their hardiness and ability to thrive in sunny locations with well-drained soil.

If there's anything specific about this flower or its care that interests you further, feel free to ask!
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 -m mistralai/Mistral-Small-3.1-24B-Instruct-2503
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI
import httpx
import textwrap
import json


client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")


completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/f/fd/Pink_flower.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is this?",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-8"><a class="header" href="#rust-8">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the Mistral 3 model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new("mistralai/Mistral-Small-3.1-24B-Instruct-2503")
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python-8"><a class="header" href="#python-8">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/mistral3.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="mistralai/Mistral-Small-3.1-24B-Instruct-2503",
        arch=VisionArchitecture.Mistral3,
    ),
    in_situ_quant="4"
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is this?",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="phi-35-model-microsoftphi-35-moe-instruct"><a class="header" href="#phi-35-model-microsoftphi-35-moe-instruct">Phi 3.5 Model: <a href="https://huggingface.co/microsoft/Phi-3.5-MoE-instruct"><code>microsoft/Phi-3.5-MoE-instruct</code></a></a></h1>
<p>The Phi 3.5 MoE model is a 16x3.8B parameter decoder-only text-to-text mixture of expert LLM.</p>
<ul>
<li>Context length of <strong>128k tokens</strong></li>
<li>Trained on <strong>4.9T tokens</strong></li>
<li>16 experts (16x3.8B parameters) with <strong>6.6B active parameters</strong></li>
<li>Expect inference performance of a 7B model</li>
</ul>
<h2 id="about-the-moe-mechanism"><a class="header" href="#about-the-moe-mechanism">About the MoE mechanism</a></h2>
<ol>
<li>Compute router gating logits</li>
<li>From the router gating logits, select the top-2 selected experts and the associated weights</li>
<li>The hidden states for each token in the sequence is computed by (if selected) applying the expert output to that token, and then weighting it.
<ul>
<li>If multiple experts are selected for the token, then this becomes a weighted sum</li>
<li>The design is flexible: 2 or 1 experts can be selected, enabling dense or sparse gating</li>
</ul>
</li>
</ol>
<pre><code class="language-bash">mistralrs run --isq 4 -m microsoft/Phi-3.5-MoE-instruct
</code></pre>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>This models supports MoQE which can be activated in the ISQ organization parameter within the various APIs, as demonstrated below:</p>
</blockquote>
<pre><code class="language-bash">mistralrs run --isq 4 -m microsoft/Phi-3.5-MoE-instruct --isq-organization moqe
</code></pre>
<h2 id="http-api-9"><a class="header" href="#http-api-9">HTTP API</a></h2>
<pre><code class="language-bash">mistralrs serve --isq 4 -p 1234 -m microsoft/Phi-3.5-MoE-instruct
</code></pre>
<pre><code class="language-py">import openai

messages = []
prompt = input("Enter system prompt &gt;&gt;&gt; ")
if len(prompt) &gt; 0:
    messages.append({"role": "system", "content": prompt})


while True:
    prompt = input("&gt;&gt;&gt; ")
    messages.append({"role": "user", "content": prompt})
    completion = client.chat.completions.create(
        model="default",
        messages=messages,
        max_tokens=256,
        frequency_penalty=1.0,
        top_p=0.1,
        temperature=0,
    )
    resp = completion.choices[0].message.content
    print(resp)
    messages.append({"role": "assistant", "content": resp})
</code></pre>
<h2 id="python-sdk-10"><a class="header" href="#python-sdk-10">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="microsoft/Phi-3.5-MoE-instruct",
        arch=Architecture.Phi3_5MoE ,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-8"><a class="header" href="#rust-sdk-8">Rust SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/text_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, PagedAttentionMetaBuilder, TextMessageRole, TextMessages, TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("microsoft/Phi-3.5-MoE-instruct")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .with_paged_attn(|| PagedAttentionMetaBuilder::default().build())?
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="phi-3-vision-model-microsoftphi-35-vision-instruct"><a class="header" href="#phi-3-vision-model-microsoftphi-35-vision-instruct">Phi 3 Vision Model: <a href="https://huggingface.co/microsoft/Phi-3-vision-128k-instruct"><code>microsoft/Phi-3.5-vision-instruct</code></a></a></h1>
<p>The Phi 3 Vision Model has support in the Rust, Python, and HTTP APIs. The Phi 3 Vision Model supports ISQ for increased performance.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<blockquote>
<p>Note: The Phi 3 Vision model works best with one image although it is supported to send multiple images.</p>
</blockquote>
<blockquote>
<p>Note: when sending multiple images, they will be resized to the minimum dimension by which all will fit without cropping.
Aspect ratio is not preserved in that case.</p>
</blockquote>
<h2 id="http-server-10"><a class="header" href="#http-server-10">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666"></p>
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is shown in this image? Write a detailed response analyzing the scene.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>The image captures a breathtaking view of a mountain peak, bathed in the soft glow of sunlight. The peak, dusted with a layer of snow, stands tall against the backdrop of a clear blue sky. A trail, etched into the mountain's side by countless hikers before it, winds its way up to the summit. The trail's white color contrasts sharply with the surrounding landscape, drawing attention to its path and inviting exploration.

The perspective from which this photo is taken offers an expansive view of the mountain and its surroundings. It seems as if one could look down from this vantage point and see miles upon miles of untouched wilderness stretching out into the distance. The colors in the image are predominantly blue and white, reflecting both sky and snow-covered mountains respectively. However, there are also hints of green from trees dotting lower parts of mountainsides or valleys below them - adding another layer to this picturesque scene. This serene landscape evokes feelings of tranquility and adventure at once - an invitation to explore nature's grandeur while respecting its majesty at all times!
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 -m microsoft/Phi-3.5-vision-instruct
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-9"><a class="header" href="#rust-9">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the Phi 3 Vision model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new("microsoft/Phi-3.5-vision-instruct")
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_phiv_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        image,
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python-9"><a class="header" href="#python-9">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="microsoft/Phi-3.5-vision-instruct",
        arch=VisionArchitecture.Phi3V,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://upload.wikimedia.org/wikipedia/commons/e/e7/ Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="phi-4-multimodal-model-microsoftphi-4-multimodal-instruct"><a class="header" href="#phi-4-multimodal-model-microsoftphi-4-multimodal-instruct">Phi 4 Multimodal Model: <a href="https://huggingface.co/microsoft/Phi-4-multimodal-instruct"><code>microsoft/Phi-4-multimodal-instruct</code></a></a></h1>
<p>The Phi 4 Multimodal Model has support in the Rust, Python, and HTTP APIs. The Phi 4 Multimodal Model supports ISQ for increased performance.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<blockquote>
<p>Note: The Phi 4 Multimodal model works best with one image although it is supported to send multiple images.</p>
</blockquote>
<blockquote>
<p>Note: when sending multiple images, they will be resized to the minimum dimension by which all will fit without cropping.
Aspect ratio is not preserved in that case.</p>
</blockquote>
<p><a href="#audio-input"><strong>Phi 4 multimodal supports audio inputs!</strong></a>.</p>
<h2 id="http-server-11"><a class="header" href="#http-server-11">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg" alt="Mount Washington" width="1000" height="666"></p>
<h6><a href="https://www.nhmagazine.com/mount-washington/">Credit</a></h6>

<p><strong>Prompt:</strong></p>
<pre><code>What is shown in this image? Write a detailed response analyzing the scene.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>A mountain with snow on it.
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 -m microsoft/Phi-4-multimodal-instruct
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-10"><a class="header" href="#rust-10">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<p>This is a minimal example of running the Phi 4 Multimodal model with a dummy image.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new("microsoft/Phi-4-multimodal-instruct")
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://cdn.britannica.com/45/5645-050-B9EC0205/head-treasure-flower-disk-flowers-inflorescence-ray.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is depicted here? Please describe the scene in detail.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python-10"><a class="header" href="#python-10">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="microsoft/Phi-4-multimodal-instruct",
        arch=VisionArchitecture.Phi4MM,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://upload.wikimedia.org/wikipedia/commons/e/e7/Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What is shown in this image? Write a detailed response analyzing the scene.",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="audio-input"><a class="header" href="#audio-input">Audio input</a></h2>
<p>Alongside vision, Phi 4 Multimodal in <code>mistral.rs</code> can accept <strong>audio</strong> as an additional modality.  This unlocks fully-local pipelines such as <strong>text + speech + vision -&gt; text</strong> where the model can reason jointly over what it <em>hears</em> and what it <em>sees</em>.</p>
<p><code>mistral.rs</code> automatically decodes the supplied audio (WAV/MP3/FLAC/OGG/… – anything <a href="https://github.com/pdeljanov/Symphonia">Symphonia</a> can handle) into 16-bit PCM.</p>
<h3 id="openai-http-api-1"><a class="header" href="#openai-http-api-1">OpenAI HTTP API</a></h3>
<p>Audio is delivered with the <code>audio_url</code> content-type that mirrors OpenAIʼs official specification:</p>
<pre><code class="language-json">{
  "role": "user",
  "content": [
    {
      "type": "audio_url",
      "audio_url": { "url": "https://upload.wikimedia.org/wikipedia/commons/4/42/Bird_singing.ogg" }
    },
    {
      "type": "image_url",
      "image_url": { "url": "https://www.allaboutbirds.org/guide/assets/og/528129121-1200px.jpg" }
    },
    {
      "type": "text",
      "text": "Describe what is happening in this clip in as much detail as possible."
    }
  ]
}
</code></pre>
<h3 id="rust-sdk-9"><a class="header" href="#rust-sdk-9">Rust SDK</a></h3>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{AudioInput, IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new("microsoft/Phi-4-multimodal-instruct")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .build()
        .await?;

    let audio_bytes = reqwest::blocking::get(
        "https://upload.wikimedia.org/wikipedia/commons/4/42/Bird_singing.ogg",
    )?
    .bytes()?
    .to_vec();
    let audio = AudioInput::from_bytes(&amp;audio_bytes)?;

    let image_bytes = reqwest::blocking::get(
        "https://www.allaboutbirds.org/guide/assets/og/528129121-1200px.jpg",
    )?
    .bytes()?
    .to_vec();
    let image = image::load_from_memory(&amp;image_bytes)?;

    let messages = VisionMessages::new()
        .add_multimodal_message(
            TextMessageRole::User,
            "Describe in detail what is happening.",
            vec![image],
            vec![audio],
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    Ok(())
}</code></pre>
<p>With this, you now have a single-call pipeline that fuses <em>sound</em>, <em>vision</em>, and <em>text</em> – all running locally through <code>mistral.rs</code>! 🔥</p>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="qwen-2-vision-model-qwen2-vl-collection"><a class="header" href="#qwen-2-vision-model-qwen2-vl-collection">Qwen 2 Vision Model: <a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d"><code>Qwen2-VL Collection</code></a></a></h1>
<p>Mistral.rs supports the Qwen2-VL vision model family, with examples in the Rust, Python, and HTTP APIs. ISQ quantization is supported to allow running the model with less memory requirements.</p>
<p>UQFF quantizations are also available.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<blockquote>
<p>Note: When using device mapping or model topology, only the text model and its layers will be managed. This is because it contains most of the model parameters. <em>The text model has 28 layers</em>.</p>
</blockquote>
<h2 id="toc-3"><a class="header" href="#toc-3">ToC</a></h2>
<ul>
<li><a href="#qwen-2-vision-model-qwen2-vl-collection">Qwen 2 Vision Model: <code>Qwen2-VL Collection</code></a>
<ul>
<li><a href="#toc-3">ToC</a></li>
<li><a href="#interactive-mode-3">Interactive mode</a></li>
<li><a href="#http-server-12">HTTP server</a></li>
<li><a href="#rust-11">Rust</a></li>
<li><a href="#python-11">Python</a></li>
</ul>
</li>
</ul>
<h2 id="interactive-mode-3"><a class="header" href="#interactive-mode-3">Interactive mode</a></h2>
<p>Mistral.rs supports interactive mode for vision models! It is an easy way to interact with the model.</p>
<ol>
<li>Start up interactive mode with the Qwen2-VL model</li>
</ol>
<pre><code>mistralrs run vision -m Qwen/Qwen2-VL-2B-Instruct
</code></pre>
<ol start="2">
<li>Say hello!</li>
</ol>
<pre><code>&gt; Hello!
Hello! How can I assist you today?
</code></pre>
<ol start="3">
<li>Pass the model an image and ask a question.</li>
</ol>
<pre><code>&gt; Hello!
Hello! How can I assist you today?
&gt; \image https://www.garden-treasures.com/cdn/shop/products/IMG_6245.jpg What type of flower is this? Give some fun facts.
flowers are a type of flowering plant that produce flowers that are typically used for decoration, pollination, and reproduction. there are many different types of flowers, each with its own unique characteristics and uses. here are some fun facts about camellias:

  * camellias are native to china and have been cultivated for over 2,000 years.
  * camellias are known for their long blooming season, with some varieties blooming continuously for months.
  * camellias come in a wide variety of colors, including red, pink, white, and yellow.
  * camellias are also known for their fragrant blooms, which can be enjoyed by both humans and animals.
  * camellias are often used in gardens and parks as a decorative element, and are also popular in landscaping and horticulture.

camellias are also known for their resilience and ability to thrive in a variety of conditions, making them a popular choice for gardeners and landscapers. they require well-draining soil and full sun or partial shade, and can be grown in containers or in the ground. overall, camellias are a beautiful and versatile flower that can add beauty and interest to any garden or landscape.
</code></pre>
<h2 id="http-server-12"><a class="header" href="#http-server-12">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/qwen2vl.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<hr>
<p><strong>Image:</strong>
<img src="https://www.garden-treasures.com/cdn/shop/products/IMG_6245.jpg" alt="Mount Washington"></p>
<p><strong>Prompt:</strong></p>
<pre><code>What type of flower is this? Give some fun facts.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>flowers are a beautiful addition to any garden or outdoor space. They come in many different colors and shapes, and can be used for both decorative purposes and as sources of pollination for bees and other insects.

One fun fact about camellias is that they are native to Japan, but were introduced to Europe in the 17th century by Portuguese sailors who brought them back from their voyages around the world. Camellias have been popular as ornamental plants since then, with many varieties available for cultivation.

Camellias also have interesting cultural significance in Japan, where they are often associated with good fortune and prosperity. In Chinese culture, camellias symbolize longevity and immortality.
In conclusion, camellias are beautiful flowers that add color and interest to gardens or outdoor spaces. They come in many different colors and shapes, making them a popular choice for gardeners everywhere!
</code></pre>
<hr>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 -m Qwen/Qwen2-VL-2B-Instruct
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.garden-treasures.com/cdn/shop/products/IMG_6245.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What type of flower is this? Give some fun facts.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-11"><a class="header" href="#rust-11">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

const MODEL_ID: &amp;str = "Qwen/Qwen2-VL-2B-Instruct";

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model =
        VisionModelBuilder::new(MODEL_ID)
            .with_isq(IsqType::Q4K)
            .with_logging()
            .build()
            .await?;

    let bytes = match reqwest::blocking::get(
        "https://www.garden-treasures.com/cdn/shop/products/IMG_6245.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What type of flower is this? Give some fun facts.",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<hr>
<h2 id="python-11"><a class="header" href="#python-11">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/qwen2vl.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

MODEL_ID = "Qwen/Qwen2-VL-2B-Instruct"

runner = Runner(
    which=Which.VisionPlain(
        model_id=MODEL_ID,
        arch=VisionArchitecture.Qwen2VL,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.garden-treasures.com/cdn/shop/products/IMG_6245.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What type of flower is this? Give some fun facts.",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="qwen-3-vision-model-qwen3-vl-collection"><a class="header" href="#qwen-3-vision-model-qwen3-vl-collection">Qwen 3 Vision Model: <a href="https://huggingface.co/collections/Qwen/qwen3-vl"><code>Qwen3 VL Collection</code></a></a></h1>
<p>The Qwen 3 VL models are the successors to the Qwen 2.5 VL models, featuring a diverse lineup of increased performance, flexible sizes, and reasoning-capable models.</p>
<p>Mistral.rs supports the Qwen 3 VL vision model family (including MoE variants), with examples in the Rust, Python, and HTTP APIs. ISQ quantization is supported to allow running the model with less memory requirements. MoE variants also support <a href="#in-situ-quantization-isq">MoQE</a> via the <code>--organization moqe</code> flag.</p>
<p>UQFF quantizations are also available.</p>
<p>The Python and HTTP APIs support sending images as:</p>
<ul>
<li>URL</li>
<li>Path to a local image</li>
<li><a href="https://en.wikipedia.org/wiki/Base64">Base64</a> encoded string</li>
</ul>
<p>The Rust SDK takes an image from the <a href="https://docs.rs/image/latest/image/index.html">image</a> crate.</p>
<blockquote>
<p>Note: When using device mapping or model topology, only the text model and its layers will be managed. This is because it contains most of the model parameters.</p>
</blockquote>
<h2 id="toc-4"><a class="header" href="#toc-4">ToC</a></h2>
<ul>
<li><a href="#qwen-3-vision-model-qwen3-vl-collection">Qwen 3 Vision Model: <code>Qwen3 VL Collection</code></a>
<ul>
<li><a href="#toc-4">ToC</a></li>
<li><a href="#interactive-mode-4">Interactive mode</a></li>
<li><a href="#http-server-13">HTTP server</a></li>
<li><a href="#rust-12">Rust</a></li>
<li><a href="#python-12">Python</a></li>
</ul>
</li>
</ul>
<h2 id="interactive-mode-4"><a class="header" href="#interactive-mode-4">Interactive mode</a></h2>
<p>Mistral.rs supports interactive mode for vision models! It is an easy way to interact with the model.</p>
<p>Start up interactive mode with the Qwen3 VL model:</p>
<pre><code>mistralrs run vision -m Qwen/Qwen3-VL-4B-Instruct
</code></pre>
<h2 id="http-server-13"><a class="header" href="#http-server-13">HTTP server</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/qwen3_vl.py">here</a>.</p>
<p>We support an OpenAI compatible HTTP API for vision models. This example demonstrates sending a chat completion request with an image.</p>
<blockquote>
<p>Note: The image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -p 1234 -m Qwen/Qwen3-VL-4B-Instruct
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

completion = client.chat.completions.create(
    model="default",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://www.garden-treasures.com/cdn/shop/products/IMG_6245.jpg"
                    },
                },
                {
                    "type": "text",
                    "text": "What type of flower is this? Give some fun facts.",
                },
            ],
        },
    ],
    max_tokens=256,
    frequency_penalty=1.0,
    top_p=0.1,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)

</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<hr>
<h2 id="rust-12"><a class="header" href="#rust-12">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/vision_models/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{IsqType, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new("Qwen/Qwen3-VL-4B-Instruct")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .build()
        .await?;

    let bytes = match reqwest::blocking::get(
        "https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg",
    ) {
        Ok(http_resp) =&gt; http_resp.bytes()?.to_vec(),
        Err(e) =&gt; anyhow::bail!(e),
    };
    let image = image::load_from_memory(&amp;bytes)?;

    let messages = VisionMessages::new().add_image_message(
        TextMessageRole::User,
        "What is this?",
        vec![image],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<hr>
<h2 id="python-12"><a class="header" href="#python-12">Python</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/qwen3_vl.py">here</a>.</p>
<p>This example demonstrates loading and sending a chat completion request with an image.</p>
<blockquote>
<p>Note: the image_url may be either a path, URL, or a base64 encoded string.</p>
</blockquote>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

MODEL_ID = "Qwen/Qwen3-VL-4B-Thinking"

runner = Runner(
    which=Which.VisionPlain(
        model_id=MODEL_ID,
        arch=VisionArchitecture.Qwen3VL,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://www.garden-treasures.com/cdn/shop/products/IMG_6245.jpg"
                        },
                    },
                    {
                        "type": "text",
                        "text": "What type of flower is this? Give some fun facts.",
                    },
                ],
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<ul>
<li>You can find an example of encoding the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_base64.py">image via base64 here</a>.</li>
<li>You can find an example of loading an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/phi3v_local_img.py">image locally here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="flux1-model-black-forest-labsflux1-schnell"><a class="header" href="#flux1-model-black-forest-labsflux1-schnell">FLUX.1 Model: <a href="https://huggingface.co/black-forest-labs/FLUX.1-schnell"><code>black-forest-labs/FLUX.1-schnell</code></a></a></h1>
<p>The FLUX model is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.</p>
<p>We support both the <code>-schnell</code> and <code>-dev</code> versions of the model.</p>
<img src="https://github.com/user-attachments/assets/82bf5009-e3e9-402b-acf9-c48a52c7721b" width="400" height="267">
<h2 id="memory-usage"><a class="header" href="#memory-usage">Memory usage</a></h2>
<p>The FLUX model itself is 12 billion parameters (~24GB), and the T5 XXL encoder model it uses requires ~9GB. We support loading the models fully onto the GPU, which allows much faster inference. If you do not have enough memory, try the offloaded (<code>-offloaded</code> or <code>-Offloaded</code>) model types. These will load the model on the CPU but perform computations on the GPU.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Memory requirement</th><th>Generation Time (s), A100</th></tr>
</thead>
<tbody>
<tr><td>Normal</td><td>~33GB</td><td>9.4</td></tr>
<tr><td>Offloaded</td><td>~4GB</td><td>92.7</td></tr>
</tbody>
</table>
</div>
<h2 id="http-server-14"><a class="header" href="#http-server-14">HTTP server</a></h2>
<p>The OpenAI HTTP server provides a compatible way to easily use this implementation. As per the specification, output images can be returned as local paths to images or be encoded to base64.</p>
<pre><code>mistralrs serve diffusion -p 1234 -m black-forest-labs/FLUX.1-schnell -a flux
</code></pre>
<p>After this, you can send requests via the HTTP server:</p>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

result = client.images.generate(
    model="default",
    prompt="A vibrant sunset in the mountains, 4k, high quality.",
    n=1,
)
print(result.data[0].url)
</code></pre>
<h2 id="rust-example"><a class="header" href="#rust-example">Rust example</a></h2>
<pre class="playground"><code class="language-rust">use std::time::Instant;
use anyhow::Result;
use mistralrs::{DiffusionGenerationParams, DiffusionLoaderType, DiffusionModelBuilder, ImageGenerationResponseFormat};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = DiffusionModelBuilder::new(
        "black-forest-labs/FLUX.1-schnell",
        DiffusionLoaderType::FluxOffloaded,
    )
    .with_logging()
    .build()
    .await?;

    let start = Instant::now();

    let response = model
        .generate_image(
            "A vibrant sunset in the mountains, 4k, high quality.".to_string(),
            ImageGenerationResponseFormat::Url,
            DiffusionGenerationParams::default(),
        )
        .await?;

    let finished = Instant::now();

    println!(
        "Done! Took {} s. Image saved at: {}",
        finished.duration_since(start).as_secs_f32(),
        response.data[0].url.as_ref().unwrap()
    );

    Ok(())
}
</code></pre>
<h2 id="python-example"><a class="header" href="#python-example">Python example</a></h2>
<pre><code class="language-py">from mistralrs import (
    Runner,
    Which,
    DiffusionArchitecture,
    ImageGenerationResponseFormat,
)

runner = Runner(
    which=Which.DiffusionPlain(
        model_id="black-forest-labs/FLUX.1-schnell",
        arch=DiffusionArchitecture.FluxOffloaded,
    ),
)

res = runner.generate_image(
    "A vibrant sunset in the mountains, 4k, high quality.",
    ImageGenerationResponseFormat.Url,
)
print(res.choices[0].url)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="voxtral-model-mistralaivoxtral-mini-4b-realtime-2602"><a class="header" href="#voxtral-model-mistralaivoxtral-mini-4b-realtime-2602">Voxtral Model: <a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"><code>mistralai/Voxtral-Mini-4B-Realtime-2602</code></a></a></h1>
<p>Voxtral Mini is a 4.4B parameter real-time automatic speech recognition (ASR) model created by Mistral AI. It features a causal Whisper-based audio encoder, a temporal adapter, and a Mistral decoder. The model accepts audio input and produces text output (speech-to-text).</p>
<p>The Voxtral Model has support in the Rust, Python, and HTTP APIs. Additionally, the Voxtral Model supports ISQ for increased performance.</p>
<blockquote>
<p>Note: Voxtral uses Mistral’s native format (<code>params.json</code>, <code>consolidated.safetensors</code>, <code>tekken.json</code>), which mistral.rs handles automatically.</p>
</blockquote>
<h2 id="http-server-15"><a class="header" href="#http-server-15">HTTP server</a></h2>
<p>We support an OpenAI compatible HTTP API for audio models.</p>
<ol>
<li>Start the server</li>
</ol>
<pre><code>mistralrs serve vision -m mistralai/Voxtral-Mini-4B-Realtime-2602
</code></pre>
<ol start="2">
<li>Send a request</li>
</ol>
<pre><code class="language-py">import base64
from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

# Load a local audio file
with open("audio.wav", "rb") as f:
    audio_b64 = base64.b64encode(f.read()).decode("utf-8")

completion = client.chat.completions.create(
    model="ignore",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "audio_url",
                    "audio_url": {
                        "url": f"data:audio/wav;base64,{audio_b64}"
                    },
                },
                {
                    "type": "text",
                    "text": "Transcribe this audio.",
                },
            ],
        },
    ],
    max_tokens=256,
    temperature=0,
)
resp = completion.choices[0].message.content
print(resp)
</code></pre>
<h2 id="rust-13"><a class="header" href="#rust-13">Rust</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/models/asr/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{AudioInput, TextMessageRole, VisionMessages, VisionModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = VisionModelBuilder::new("mistralai/Voxtral-Mini-4B-Realtime-2602")
        .with_logging()
        .build()
        .await?;

    let audio_bytes = std::fs::read("sample_audio.wav")?;
    let audio = AudioInput::from_bytes(&amp;audio_bytes)?;

    let messages = VisionMessages::new().add_multimodal_message(
        TextMessageRole::User,
        "Transcribe this audio.",
        vec![],
        vec![audio],
    );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="python-13"><a class="header" href="#python-13">Python</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="mistralai/Voxtral-Mini-4B-Realtime-2602",
        arch=VisionArchitecture.Voxtral,
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="ignore",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "audio_url",
                        "audio_url": {
                            "url": "path/to/audio.wav"
                        },
                    },
                    {
                        "type": "text",
                        "text": "Transcribe this audio.",
                    },
                ],
            }
        ],
        max_tokens=256,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dia-16b-model-nari-labsdia-16b"><a class="header" href="#dia-16b-model-nari-labsdia-16b">Dia 1.6b Model: <a href="https://huggingface.co/nari-labs/Dia-1.6B"><code>nari-labs/Dia-1.6B</code></a></a></h1>
<p>Dia is a 1.6B parameter text to speech model created by Nari Labs. You can condition the output on audio, enabling emotion and tone control. The model can also produce nonverbal communications like laughter, coughing, clearing throat, etc.</p>
<ul>
<li>Generate dialogue via the [S1] and [S2] tags</li>
<li>Generate non-verbal like (laughs), (coughs), etc.</li>
<li>Below verbal tags will be recognized, but might result in unexpected output. (laughs), (clears throat), (sighs), (gasps), (coughs), (singing), (sings), (mumbles), (beep), (groans), (sniffs), (claps), (screams), (inhales), (exhales), (applause), (burps), (humming), (sneezes), (chuckle), (whistles)</li>
</ul>
<blockquote>
<p>Note: voice cloning support is coming!</p>
</blockquote>
<h2 id="http-server-16"><a class="header" href="#http-server-16">HTTP server</a></h2>
<p>The OpenAI HTTP server provides a drop-in compatible way to easily use Dia locally!</p>
<blockquote>
<p>Note: we only support <code>pcm</code> and <code>wav</code> outputs.</p>
</blockquote>
<pre><code>mistralrs run speech -m nari-labs/Dia-1.6B -a dia
</code></pre>
<p>After this, you can send requests via the HTTP server:</p>
<pre><code class="language-py">from pathlib import Path
from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

# text_to_speak = "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face."
text_to_speak = "[S1] mistral r s is a local LLM inference engine. [S2] You can run text and vision models, and also image generation and speech generation. [S1] There is agentic web search, tool calling, and a convenient Python SDK. [S2] Check it out on github."

response = client.audio.speech.create(
    model="default", voice="N/A", input=text_to_speak, response_format="wav"
)

output_path = Path("output.wav")
output_path.write_bytes(response.read())
print(f"WAV audio written to {output_path.resolve()}")
</code></pre>
<h2 id="rust-example-1"><a class="header" href="#rust-example-1">Rust example</a></h2>
<pre class="playground"><code class="language-rust">use std::time::Instant;

use anyhow::Result;
use mistralrs::{speech_utils, SpeechLoaderType, SpeechModelBuilder};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = SpeechModelBuilder::new("nari-labs/Dia-1.6B", SpeechLoaderType::Dia)
        .with_logging()
        .build()
        .await?;

    let start = Instant::now();

    // let text_to_speak = "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.";
    let text_to_speak = "[S1] mistral r s is a local LLM inference engine. [S2] You can run text and vision models, and also image generation and speech generation. [S1] There is agentic web search, tool calling, and a convenient Python SDK. [S2] Check it out on github.";

    let (pcm, rate, channels) = model.generate_speech(text_to_speak).await?;

    let finished = Instant::now();

    let mut output = std::fs::File::create("out.wav").unwrap();
    speech_utils::write_pcm_as_wav(&amp;mut output, &amp;pcm, rate as u32, channels as u16).unwrap();

    println!(
        "Done! Took {} s. Audio saved at `out.wav`.",
        finished.duration_since(start).as_secs_f32(),
    );

    Ok(())
}</code></pre>
<h2 id="python-example-1"><a class="header" href="#python-example-1">Python example</a></h2>
<pre><code class="language-py">from mistralrs import (
    Runner,
    Which,
    SpeechLoaderType,
)
from pathlib import Path
import wave, struct

# text_to_speak = "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face."
text_to_speak = "[S1] mistral r s is a local LLM inference engine. [S2] You can run text and vision models, and also image generation and speech generation. [S1] There is agentic web search, tool calling, and a convenient Python SDK. [S2] Check it out on github."

runner = Runner(
    which=Which.Speech(
        model_id="nari-labs/Dia-1.6B",
        arch=SpeechLoaderType.Dia,
    ),
)

res = runner.generate_speech(text_to_speak)
print(res.choices[0].url)

pcm_data = res.pcm  # list of floats between -1.0 and 1.0
output_path = Path("output.wav")

# convert floats to 16-bit PCM ints
pcm_ints = [int(max(-32768, min(32767, int(sample * 32767)))) for sample in pcm_data]
with wave.open(output_path, "wb") as wf:
    wf.setnchannels(1)  # mono
    wf.setsampwidth(2 * res.channels)  # 2 bytes per sample (16-bit)
    wf.setframerate(res.rate)  # set sample rate (adjust if needed)
    wf.writeframes(b"".join(struct.pack("&lt;h", s) for s in pcm_ints))

print(f"WAV audio written to {output_path.resolve()}")
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="embeddinggemma"><a class="header" href="#embeddinggemma">EmbeddingGemma</a></h1>
<p>EmbeddingGemma was the first embedding model supported by mistral.rs. This guide walks through serving the
model via the OpenAI-compatible HTTP server, running it from Python, and embedding text directly in Rust.</p>
<p>For a catalog of available embedding models and general usage tips, see <a href="#embeddings-overview">EMBEDDINGS.md</a>.</p>
<h2 id="prompt-instructions"><a class="header" href="#prompt-instructions">Prompt instructions</a></h2>
<p>EmbeddingGemma can generate optimized embeddings for various use cases-such as document retrieval, question answering, and fact verification-or for specific input types, either, a query or a document-using prompts that are prepended to the input strings.</p>
<ul>
<li>Query prompts follow the form <code>task: {task description} | query: </code> where the task description varies by the use case, with the default task description being search result.</li>
<li>Document-style prompts follow the form <code>title: {title | "none"} | text: </code> where the title is either none (the default) or the actual title of the document. Note that providing a title, if available, will improve model performance for document prompts but may require manual formatting.</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th><strong>Use Case (task type enum)</strong></th><th><strong>Descriptions</strong></th><th><strong>Recommended Prompt</strong></th></tr>
</thead>
<tbody>
<tr><td><strong>Retrieval (Query)</strong></td><td>Used to generate embeddings that are optimized for document search or information retrieval.</td><td><code>task: search result | query: {content}</code></td></tr>
<tr><td><strong>Retrieval (Document)</strong></td><td>Used to generate embeddings that are optimized for document search or information retrieval (document side).</td><td><code>title: {title | "none"} | text: {content}</code></td></tr>
<tr><td><strong>Question Answering</strong></td><td>Used to generate embeddings that are optimized for answering natural language questions.</td><td><code>task: question answering | query: {content}</code></td></tr>
<tr><td><strong>Fact Verification</strong></td><td>Used to generate embeddings that are optimized for verifying factual correctness.</td><td><code>task: fact checking | query: {content}</code></td></tr>
<tr><td><strong>Classification</strong></td><td>Used to generate embeddings that are optimized to classify texts according to preset labels.</td><td><code>task: classification | query: {content}</code></td></tr>
<tr><td><strong>Clustering</strong></td><td>Used to generate embeddings that are optimized to cluster texts based on their similarities.</td><td><code>task: clustering | query: {content}</code></td></tr>
<tr><td><strong>Semantic Similarity</strong></td><td>Used to generate embeddings that are optimized to assess text similarity. This is not intended for retrieval use cases.</td><td><code>task: sentence similarity | query: {content}</code></td></tr>
<tr><td><strong>Code Retrieval</strong></td><td>Used to retrieve a code block based on a natural language query, such as <em>sort an array</em> or <em>reverse a linked list</em>. Embeddings of code blocks are computed using <code>retrieval_document</code>.</td><td><code>task: code retrieval | query: {content}</code></td></tr>
</tbody>
</table>
</div>
<h2 id="http-server-17"><a class="header" href="#http-server-17">HTTP server</a></h2>
<p>Launch the server in embedding mode to expose an OpenAI-compatible <code>/v1/embeddings</code> endpoint:</p>
<pre><code class="language-bash">mistralrs serve -p 1234 -m google/embeddinggemma-300m
</code></pre>
<p>Once running, call the endpoint with an OpenAI client or raw <code>curl</code>:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/embeddings \
  -H "Authorization: Bearer EMPTY" \
  -H "Content-Type: application/json" \
  -d '{"model": "default", "input": ["task: search result | query: What is graphene?", "task: search result | query: What is an apple?"]}'
</code></pre>
<p>An example with the OpenAI client can be found <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/embedding.py">here</a>.</p>
<p>By default the server registers the model as <code>default</code>. To expose it under a custom name or alongside chat
models, run in multi-model mode and assign an identifier in the selector configuration:</p>
<pre><code class="language-json">{
  "embed-gemma": {
    "Embedding": {
      "model_id": "google/embeddinggemma-300m",
      "arch": "embeddinggemma"
    }
  }
}
</code></pre>
<p>See <a href="#post-v1embeddings">docs/HTTP.md</a> for the full request schema and response layout.</p>
<h2 id="python-sdk-11"><a class="header" href="#python-sdk-11">Python SDK</a></h2>
<p>Instantiate <code>Runner</code> with the <code>Which.Embedding</code> selector and request EmbeddingGemma explicitly. The helper method
<code>send_embedding_request</code> returns batched embeddings as Python lists.</p>
<pre><code class="language-python">from mistralrs import EmbeddingArchitecture, EmbeddingRequest, Runner, Which

runner = Runner(
    which=Which.Embedding(
        model_id="google/embeddinggemma-300m",
        arch=EmbeddingArchitecture.EmbeddingGemma,
    )
)

request = EmbeddingRequest(
    input=["task: search result | query: What is graphene?", "task: search result | query: What is an apple?"],
    truncate_sequence=True,
)

embeddings = runner.send_embedding_request(request)
print(len(embeddings), len(embeddings[0]))
</code></pre>
<p>Refer to <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/embedding_gemma.py">this example</a> for a complete runnable script.</p>
<h2 id="rust-sdk-10"><a class="header" href="#rust-sdk-10">Rust SDK</a></h2>
<p>Use the <code>EmbeddingModelBuilder</code> helper from the <code>mistralrs</code> crate to create the model and submit an
<code>EmbeddingRequest</code>:</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{EmbeddingModelBuilder, EmbeddingRequest};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = EmbeddingModelBuilder::new("google/embeddinggemma-300m")
        .with_logging()
        .build()
        .await?;

    let embeddings = model
        .generate_embeddings(
            EmbeddingRequest::builder()
                .add_prompt("task: search result | query: What is graphene?")
        )
        .await?;

    println!("Returned {} vectors", embeddings.len());
    Ok(())
}</code></pre>
<p>This example lives <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/getting_started/embedding/main.rs">here</a>, and can be run with:</p>
<pre><code class="language-bash">cargo run --package mistralrs --example embedding_gemma
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="qwen3-embedding"><a class="header" href="#qwen3-embedding">Qwen3 Embedding</a></h1>
<p>The Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks.</p>
<p>For a catalog of all embedding backends, see <a href="#embeddings-overview">EMBEDDINGS.md</a>.</p>
<h2 id="http-server-18"><a class="header" href="#http-server-18">HTTP server</a></h2>
<p>Serve the model with the OpenAI-compatible endpoint enabled:</p>
<pre><code class="language-bash">mistralrs serve -p 1234 -m Qwen/Qwen3-Embedding-0.6B
</code></pre>
<p>Call the endpoint via <code>curl</code> or the OpenAI SDK:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/embeddings \
  -H "Authorization: Bearer EMPTY" \
  -H "Content-Type: application/json" \
  -d '{"model": "default", "input": ["Graphene conductivity", "Explain superconductors in simple terms."]}'
</code></pre>
<p>An example with the OpenAI client can be found <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/embedding.py">here</a>.</p>
<p>To expose the model alongside chat models, register it in your selector configuration using the
<code>qwen3embedding</code> architecture tag:</p>
<pre><code class="language-json">{
  "embed-qwen3": {
    "Embedding": {
      "model_id": "Qwen/Qwen3-Embedding-0.6B",
      "arch": "qwen3embedding"
    }
  }
}
</code></pre>
<p>See <a href="#post-v1embeddings">docs/HTTP.md</a> for the full request schema.</p>
<h2 id="python-sdk-12"><a class="header" href="#python-sdk-12">Python SDK</a></h2>
<p>Instantiate <code>Runner</code> with the embedding selector and request Qwen3 explicitly. The output mirrors the
OpenAI embeddings array shape:</p>
<pre><code class="language-python">from mistralrs import EmbeddingArchitecture, EmbeddingRequest, Runner, Which

runner = Runner(
    which=Which.Embedding(
        model_id="Qwen/Qwen3-Embedding-0.6B",
        arch=EmbeddingArchitecture.Qwen3Embedding,
    )
)

request = EmbeddingRequest(
    input=["Graphene conductivity", "Explain superconductors in simple terms."],
    truncate_sequence=True,
)

embeddings = runner.send_embedding_request(request)
print(len(embeddings), len(embeddings[0]))
</code></pre>
<p>A ready-to-run version can be found at <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/qwen3_embedding.py"><code>examples/python/qwen3_embedding.py</code></a>.</p>
<h2 id="rust-sdk-11"><a class="header" href="#rust-sdk-11">Rust SDK</a></h2>
<p>Use the <code>EmbeddingModelBuilder</code> helper just like with EmbeddingGemma. The example below mirrors the
repository sample:</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{EmbeddingModelBuilder, EmbeddingRequest};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = EmbeddingModelBuilder::new("Qwen/Qwen3-Embedding-0.6B")
        .with_logging()
        .build()
        .await?;

    let embeddings = model
        .generate_embeddings(
            EmbeddingRequest::builder()
                .add_prompt("What is graphene?")
                .add_prompt("Explain superconductors in simple terms.")
        )
        .await?;

    println!("Returned {} vectors", embeddings.len());
    Ok(())
}</code></pre>
<p>You can find the full example at <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/embeddings/main.rs"><code>mistralrs/examples/advanced/embeddings/main.rs</code></a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="quantization-in-mistralrs"><a class="header" href="#quantization-in-mistralrs">Quantization in mistral.rs</a></h1>
<p>Mistral.rs supports the following quantization:</p>
<ul>
<li>⭐ ISQ (<a href="#in-situ-quantization-isq">read more detail</a>)
<ul>
<li>Supported in all plain/vision and adapter models</li>
<li>Works on all supported devices</li>
<li>Automatic selection to use the fastest and most accurate method</li>
<li>Supports:
<ul>
<li>Q, K type GGUF quants</li>
<li>AFQ</li>
<li>HQQ</li>
<li>FP8</li>
<li>F8Q8</li>
</ul>
</li>
</ul>
</li>
<li>GGUF/GGML
<ul>
<li>Q, K type</li>
<li>Supported in GGUF/GGML and GGUF/GGML adapter models</li>
<li>Supported in all plain/vision and adapter models</li>
<li>Imatrix quantization is supported</li>
<li>I quants coming!</li>
<li>CPU, CUDA, Metal (all supported devices)</li>
<li>2, 3, 4, 5, 6, 8 bit</li>
</ul>
</li>
<li>GPTQ (convert with <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/scripts/convert_to_gptq.py">this script</a>)
<ul>
<li>Supported in all plain/vision and adapter models</li>
<li>CUDA only</li>
<li>2, 3, 4, 8 bit</li>
<li><a href="https://github.com/IST-DASLab/marlin">Marlin</a> kernel support in 4-bit and 8-bit.</li>
</ul>
</li>
<li>AWQ (convert with <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/scripts/convert_awq_marlin.py">this script</a>)
<ul>
<li>Supported in all plain/vision and adapter models</li>
<li>CUDA only</li>
<li>4 and 8 bit</li>
<li><a href="https://github.com/IST-DASLab/marlin">Marlin</a> kernel support in 4-bit and 8-bit.</li>
</ul>
</li>
<li>HQQ
<ul>
<li>Supported in all plain/vision and adapter models via ISQ</li>
<li>4, 8 bit</li>
<li>CPU, CUDA, Metal (all supported devices)</li>
</ul>
</li>
<li>FP8
<ul>
<li>Supported in all plain/vision and adapter models</li>
<li>CPU, CUDA, Metal (all supported devices)</li>
</ul>
</li>
<li>BNB
<ul>
<li>Supported in all plain/vision and adapter models</li>
<li>bitsandbytes int8, fp4, nf4 support</li>
</ul>
</li>
<li>AFQ
<ul>
<li>2, 3, 4, 6, 8 bit</li>
<li>🔥 Designed to be fast on <strong>Metal</strong>!</li>
<li>Only supported on Metal.</li>
</ul>
</li>
<li>MLX prequantized
<ul>
<li>Supported in all plain/vision and adapter models</li>
</ul>
</li>
</ul>
<h2 id="using-a-gguf-quantized-model"><a class="header" href="#using-a-gguf-quantized-model">Using a GGUF quantized model</a></h2>
<ul>
<li>Use the <code>gguf</code> (cli) / <code>GGUF</code> (Python) model selector</li>
<li>Provide the GGUF file</li>
</ul>
<pre><code>mistralrs run --format gguf -f my-gguf-file.gguf
</code></pre>
<h2 id="using-isq"><a class="header" href="#using-isq">Using ISQ</a></h2>
<p>See the <a href="#in-situ-quantization-isq">docs</a></p>
<pre><code>mistralrs run --isq 4 -m microsoft/Phi-3-mini-4k-instruct
</code></pre>
<h2 id="using-a-gptq-quantized-model"><a class="header" href="#using-a-gptq-quantized-model">Using a GPTQ quantized model</a></h2>
<ul>
<li>Provide the model ID for the GPTQ model</li>
<li>Mistral.rs will automatically detect and use GPTQ quantization for plain and vision models!</li>
<li>The <a href="https://github.com/IST-DASLab/marlin">Marlin</a> kernel will automatically be used for 4-bit and 8-bit.</li>
</ul>
<pre><code>mistralrs run -m kaitchup/Phi-3-mini-4k-instruct-gptq-4bit
</code></pre>
<p>You can create your own GPTQ model using [<code>scripts/convert_to_gptq.py</code>][../scripts/convert_to_gptq.py]:</p>
<pre><code>pip install gptqmodel transformers datasets

python3 scripts/convert_to_gptq.py --src path/to/model --dst output/model/path --bits 4
</code></pre>
<h2 id="using-a-mlx-prequantized-model-on-metal"><a class="header" href="#using-a-mlx-prequantized-model-on-metal">Using a MLX prequantized model (on Metal)</a></h2>
<ul>
<li>Provide the model ID for the MLX prequantized model</li>
<li>Mistral.rs will automatically detect and use quantization for plain and vision models!</li>
<li>Specialized kernels will be used to accelerate inference!</li>
</ul>
<pre><code>mistralrs run -m mlx-community/Llama-3.8-1B-8bit
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="in-situ-quantization-isq"><a class="header" href="#in-situ-quantization-isq">In situ quantization (ISQ)</a></h1>
<p>In situ quantization (ISQ) quantizes model weights in place as they are loaded, so the full unquantized model never needs to fit in memory. Using with I/O and parallel pipelining, this means you can load and run a model that is larger than the total amount of RAM (CPU or GPU) on your system.</p>
<p>If the quantized weights are small enough to fit even though the original weights would not, you can still run the model! Like all quantization, ISQ may also increase inference performance due to reduced memory bandwidth pressure.</p>
<p><strong>Quick start</strong>: Just use <code>--isq 4</code> (or 2, 3, 5, 6, 8) and mistral.rs will pick the best quantization for your hardware:</p>
<pre><code>mistralrs run --isq 4 -m meta-llama/Llama-3.2-3B-Instruct
</code></pre>
<p>An API is exposed on the Python and Rust SDKs which provides the ability to dynamically re-ISQ models at runtime.</p>
<p>To set the ISQ type for individual layers, use a model <a href="#model-topology-configuration"><code>topology</code></a>.</p>
<blockquote>
<p>Note: 🔥 AFQ (affine) quantization is designed to be fast on <strong>Metal</strong> but is only supported on Metal.</p>
</blockquote>
<h2 id="automatic-isq-just-use-a-number"><a class="header" href="#automatic-isq-just-use-a-number">Automatic ISQ (just use a number!)</a></h2>
<p>Instead of specifying a quantization type like <code>Q4K</code>, you can just pass an integer (2, 3, 4, 5, 6, or 8) and mistral.rs will automatically select the best quantization method for your platform.</p>
<p>On Metal, this uses fast AFQ quantization (for 2, 3, 4, 6, or 8 bits). On other platforms, it falls back to Q/K quantization.</p>
<pre><code>mistralrs run --isq 4 -m meta-llama/Llama-3.2-3B-Instruct
</code></pre>
<h2 id="isq-quantization-types"><a class="header" href="#isq-quantization-types">ISQ quantization types</a></h2>
<ul>
<li>AFQ2 (<em>AFQ is only available on Metal</em>)</li>
<li>AFQ3</li>
<li>AFQ4</li>
<li>AFQ6</li>
<li>AFQ8</li>
<li>Q4_0</li>
<li>Q4_1</li>
<li>Q5_0</li>
<li>Q5_1</li>
<li>Q8_0</li>
<li>Q8_1 (<em>not available on CUDA</em>)</li>
<li>Q2K</li>
<li>Q3K</li>
<li>Q4K</li>
<li>Q5K</li>
<li>Q6K</li>
<li>Q8K  (<em>not available on CUDA</em>)</li>
<li>HQQ4</li>
<li>HQQ8</li>
<li>FP8</li>
<li>F8Q8</li>
</ul>
<pre><code>mistralrs run --isq 4 -m meta-llama/Llama-3.2-3B-Instruct
</code></pre>
<p>For Mixture of Expert models, a method called <a href="https://arxiv.org/abs/2310.02410">MoQE</a> can be applied to only quantize MoE layers. This is configured via the ISQ “organization” parameter in all APIs. The following models support MoQE:</p>
<ul>
<li><a href="#phi-35-model-microsoftphi-35-moe-instruct">Phi 3.5 MoE</a></li>
<li><a href="#deepseek-v2-deepseek-aideepseek-v2-lite">DeepSeek V2</a></li>
<li><a href="#deepseek-v3-deepseek-aideepseek-v3-deepseek-aideepseek-r1">DeepSeek V3 / DeepSeek R1</a></li>
<li><a href="#glm-47-moe-zai-orgglm-47">GLM4-MoE</a></li>
<li><a href="#glm-47-flash-moe-zai-orgglm-47-flash">GLM4-MoE-Lite</a></li>
<li><a href="#qwen-3-collection">Qwen 3 (MoE variants)</a></li>
<li><a href="#qwen-3-vision-model-qwen3-vl-collection">Qwen3-VL-MoE (MoE variants)</a></li>
</ul>
<h2 id="quantization-strategies"><a class="header" href="#quantization-strategies">Quantization strategies</a></h2>
<p>ISQ supports two quantization strategies, selected automatically based on your configuration:</p>
<h3 id="immediate-isq-default"><a class="header" href="#immediate-isq-default">Immediate ISQ (default)</a></h3>
<p>Immediate ISQ quantizes each weight as it is loaded during model construction rather than loading all weights first, then quantizing. This means only a small number of unquantized weight tensors need to be in CPU memory at any given time, enabling ISQ for models that would not otherwise fit in memory.</p>
<p>Quantization is parallelized across a thread pool on all devices. Multiple weights are quantized concurrently on CPU during loading, then moved to the target device. The number of threads depends on the ISQ type: GGML types (Q2K-Q8K) use all available CPU threads, while GPU-quantized types (HQQ, AFQ) use a single thread since the GPU work is serialized by a guard.</p>
<p>Set <code>MISTRALRS_ISQ_SINGLETHREAD=1</code> to force single-threaded quantization.</p>
<h3 id="deferred-isq"><a class="header" href="#deferred-isq">Deferred ISQ</a></h3>
<p>Deferred ISQ loads the full unquantized model into CPU memory first, then quantizes all weights in parallel in a post-processing pass. This path is used when an imatrix file (<code>--imatrix</code>) or calibration file (<code>--calibration-file</code>) is provided, since these require access to the full model or a forward pass before quantization can begin. Peak CPU memory usage is higher than immediate ISQ because the entire unquantized model must fit in memory during the quantization pass.</p>
<h2 id="accuracy"><a class="header" href="#accuracy">Accuracy</a></h2>
<p>Accuracy of ISQ can be measured by the performance degradation versus the unquantized model.
This is commonly measured with perplexity. Please see the <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/perplexity/main.rs">perplexity</a> example.</p>
<p>To improve the accuracy of a model with ISQ, use an imatrix file. These can be found online (for example, on Hugging Face), and should be passed with the <code>--imatrix</code> flag for <code>plain</code> models. This will increase the accuracy of the quantization significantly and bring the ISQ quantization up to par with the GGUF counterpart.</p>
<p>Check out the <a href="#enhancing-isq-with-an-imatrix">imatrix docs</a>.</p>
<h2 id="python-example-2"><a class="header" href="#python-example-2">Python Example</a></h2>
<pre><code class="language-python">runner = Runner(
    which=Which.Plain(
        model_id="Qwen/Qwen3-0.6B",
    ),
    in_situ_quant="4",
)
</code></pre>
<h2 id="rust-example-2"><a class="header" href="#rust-example-2">Rust Example</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/isq/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let model = TextModelBuilder::new("microsoft/Phi-3.5-mini-instruct")
    .with_isq(IsqType::Q8_0)
    .with_logging()
    .with_paged_attn(|| PagedAttentionMetaBuilder::default().build())?
    .build()
    .await?;
<span class="boring">}</span></code></pre>
<h2 id="server-example"><a class="header" href="#server-example">Server example</a></h2>
<pre><code>mistralrs serve --port 1234 --isq 4 -m mistralai/Mistral-7B-Instruct-v0.1
</code></pre>
<p>Or with a specific quantization type:</p>
<pre><code>mistralrs serve --port 1234 --isq Q4K -m mistralai/Mistral-7B-Instruct-v0.1
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="universal-quantized-file-format-uqff"><a class="header" href="#universal-quantized-file-format-uqff">Universal Quantized File Format: UQFF</a></h1>
<h3 align="left">
The uniquely powerful quantized file format.
</h3>

<ol>
<li><strong>Flexible</strong> 🌀: Multiple quantization formats in <em>one</em> file format with <em>one</em> framework to run them all.</li>
<li><strong>Reliable</strong> 🔒: Compatibility ensured with <em>embedded</em> and <em>checked</em> semantic versioning information from day 1.</li>
<li><strong>Easy</strong> 🤗: Download UQFF models <em>easily</em> and <em>quickly</em> from Hugging Face, or use a local file.</li>
<li><strong>Customizable</strong> 🛠️: Make and publish your own UQFF files in minutes.</li>
</ol>
<h1 id="toc-5"><a class="header" href="#toc-5">ToC</a></h1>
<ul>
<li><a href="#universal-quantized-file-format-uqff">Universal Quantized File Format: UQFF</a></li>
<li><a href="#toc-5">ToC</a>
<ul>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#loading-a-uqff-model">Loading a UQFF model</a>
<ul>
<li><a href="#shard-auto-discovery">Shard auto-discovery</a></li>
<li><a href="#running-with-the-cli">Running with the CLI</a></li>
<li><a href="#using-with-the-rust-sdk">Using with the Rust SDK</a></li>
<li><a href="#using-the-python-sdk">Using the Python SDK</a></li>
<li><a href="#using-topology-for-device-mapping-with-uqff">Using topology for device mapping with UQFF</a></li>
</ul>
</li>
<li><a href="#creating-a-uqff-model">Creating a UQFF model</a>
<ul>
<li><a href="#model-card-generation">Model card generation</a></li>
<li><a href="#uploading-to-hugging-face">Uploading to Hugging Face</a></li>
</ul>
</li>
<li><a href="#list-of-models">List of models</a></li>
</ul>
</li>
</ul>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>UQFF builds on our ISQ feature by allowing serialization and deserialization for models.</p>
<p>While ISQ is a powerful feature enabling easy quantization of models, the key limitation has been the time required for requantization. While the process is relatively fast with parallelization and other techniques, multiple runs can make the experience slow.</p>
<p><strong>Comparting UQFF to GGUF:</strong></p>
<p>In contrast to GGUF, which only supports the GGUF quantizations, UQFF is designed with flexibiliuty in mind. At its code, it extends the power and flexibility of ISQ. The ability to support multiple quantization types (more to come!) in one simple, easy-to-use file is a critical feature.</p>
<p>Additionally, users will no longer need to wait for GGUF support to begin using post-training quantized models. As we add new models and quantization schemes to mistral.rs, the feature set of UQFF will grow.</p>
<h2 id="support"><a class="header" href="#support">Support</a></h2>
<p>The following quantization formats are supported in UQFF. One can, of course, be combined arbitrarily during UQFF generation or ISQ using a <a href="#model-topology-configuration">model topology</a>. When loading a UQFF model, only the per-layer device mapping feature of the topology applies.</p>
<ul>
<li>
<p>GGUF quantized:</p>
<ul>
<li>Q4_0</li>
<li>Q4_1</li>
<li>Q5_0</li>
<li>Q5_1</li>
<li>Q8_0</li>
<li>Q8_1 (<em>not available on CUDA</em>)</li>
<li>Q2K</li>
<li>Q3K</li>
<li>Q4K</li>
<li>Q5K</li>
<li>Q6K</li>
<li>Q8K  (<em>not available on CUDA</em>)</li>
</ul>
</li>
<li>
<p>HQQ quantized:</p>
<ul>
<li>HQQ4</li>
<li>HQQ8</li>
</ul>
</li>
<li>
<p>FP8:</p>
<ul>
<li>FP8 E4M3 (4-bit exponent, 3-bit mantissa)</li>
</ul>
</li>
<li>
<p>AFQ quantized (🔥 AFQ is fast on <strong>Metal</strong>):</p>
<ul>
<li>AFQ2</li>
<li>AFQ3</li>
<li>AFQ4</li>
<li>AFQ6</li>
<li>AFQ8</li>
</ul>
</li>
<li>
<p>F8Q8:</p>
<ul>
<li>F8Q8</li>
</ul>
</li>
</ul>
<h2 id="loading-a-uqff-model"><a class="header" href="#loading-a-uqff-model">Loading a UQFF model</a></h2>
<p>To load a UQFF model, specify the filename of the first (or only) UQFF shard. This will be located based on the model ID, and can
be loaded locally or from Hugging Face based on the model ID.</p>
<ul>
<li><code>phi3.5-mini-instruct-q4k-0.uqff</code></li>
<li><code>../UQFF/phi3.5-mini-instruct-q4k-0.uqff</code></li>
</ul>
<p>You can find a <a href="https://huggingface.co/collections/EricB/uqff-670e4a49d56ecdd3f7f0fd4c">collection of UQFF models here</a>, which each include a simple
command to get started.</p>
<blockquote>
<p>Note: when loading an UQFF model, <em>any</em> ISQ setting will be ignored.</p>
</blockquote>
<h3 id="shard-auto-discovery"><a class="header" href="#shard-auto-discovery">Shard auto-discovery</a></h3>
<p>Large models produce multiple shard files (e.g., <code>q4k-0.uqff</code>, <code>q4k-1.uqff</code>, <code>q4k-2.uqff</code>). You only need to specify <strong>one</strong> shard file – the remaining shards are auto-discovered from the same directory or Hugging Face repository.</p>
<p>For example, if a model has shards <code>q4k-0.uqff</code>, <code>q4k-1.uqff</code>, and <code>q4k-2.uqff</code>:</p>
<pre><code class="language-bash"># Just specify the first shard -- the rest are found automatically
mistralrs run -m EricB/MyModel-UQFF --from-uqff q4k-0.uqff
</code></pre>
<p>This also works when multiple quantizations exist in the same repo (e.g., <code>q4k-*</code> and <code>q8_0-*</code>). Only the shards matching the specified prefix are loaded.</p>
<h3 id="running-with-the-cli"><a class="header" href="#running-with-the-cli">Running with the CLI</a></h3>
<pre><code class="language-bash">mistralrs run -m EricB/Phi-3.5-mini-instruct-UQFF --from-uqff phi3.5-mini-instruct-f8e4m3-0.uqff
</code></pre>
<h3 id="using-with-the-rust-sdk"><a class="header" href="#using-with-the-rust-sdk">Using with the Rust SDK</a></h3>
<p>Check out the following examples:</p>
<ul>
<li>Normal: <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/uqff/main.rs">uqff/main.rs</a></li>
<li>Vision: <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/uqff_vision/main.rs">uqff_vision/main.rs</a></li>
</ul>
<h3 id="using-the-python-sdk"><a class="header" href="#using-the-python-sdk">Using the Python SDK</a></h3>
<p>Modify the <code>Which</code> instantiation as follows:</p>
<pre><code class="language-diff">Which.Plain(
    model_id="EricB/Phi-3.5-mini-instruct-UQFF",
+   from_uqff="phi3.5-mini-instruct-q4k-0.uqff"
),
</code></pre>
<h3 id="using-topology-for-device-mapping-with-uqff"><a class="header" href="#using-topology-for-device-mapping-with-uqff">Using topology for device mapping with UQFF</a></h3>
<p>When loading a UQFF model, the quantization is already baked in, so ISQ settings in the topology are ignored. However, <strong>device mapping</strong> from a topology file still applies. This is useful for splitting a pre-quantized model across multiple GPUs or offloading layers to CPU.</p>
<p><strong>CLI example:</strong></p>
<pre><code class="language-bash">mistralrs run -m EricB/Phi-3.5-mini-instruct-UQFF --from-uqff phi3.5-mini-instruct-q4k.uqff --topology device_map.yml
</code></pre>
<p><strong>Topology file for device mapping only (<code>device_map.yml</code>):</strong></p>
<pre><code class="language-yaml">0-16:
  device: cuda[0]
16-32:
  device: cuda[1]
</code></pre>
<p><strong>Rust SDK example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mistralrs::{UqffTextModelBuilder, Topology, LayerTopology, Device};

let model = UqffTextModelBuilder::new(
    "EricB/Phi-3.5-mini-instruct-UQFF",
    vec!["phi3.5-mini-instruct-q4k.uqff".into()],
)
.into_inner()
.with_topology(
    Topology::empty()
        .with_range(0..16, LayerTopology { isq: None, device: Some(Device::Cuda(0)) })
        .with_range(16..32, LayerTopology { isq: None, device: Some(Device::Cuda(1)) })
)
.build()
.await?;
<span class="boring">}</span></code></pre>
<p><strong>Python SDK example:</strong></p>
<pre><code class="language-python">runner = Runner(
    which=Which.Plain(
        model_id="EricB/Phi-3.5-mini-instruct-UQFF",
        from_uqff="phi3.5-mini-instruct-q4k.uqff",
        topology="device_map.yml",
    ),
)
</code></pre>
<blockquote>
<p>Note: The <code>isq</code> field in topology entries is ignored when loading UQFF models since quantization is pre-applied.</p>
</blockquote>
<h2 id="creating-a-uqff-model"><a class="header" href="#creating-a-uqff-model">Creating a UQFF model</a></h2>
<p>Creating a UQFF model requires you to generate the UQFF file.</p>
<ul>
<li>Specify an output path: either a <code>.uqff</code> file path or a directory where files will be auto-named.</li>
<li>The quantization of a UQFF model is determined from the ISQ or model topology (see the <a href="#model-topology-configuration">topology docs</a> for more details on how ISQ and the topology mix).</li>
</ul>
<p>Along with the UQFF file, the generation process will also output several <code>.json</code> configuration files and <code>residual.safetensors</code>. All of these files are considered the
UQFF model, and should be kept together or uploaded.</p>
<blockquote>
<p>Note: Only the <code>.uqff</code> files are unique to the quantization level(s). If you are generating multiple UQFF files, it is OK for the others to be overwritten.</p>
</blockquote>
<p><strong>Single quantization (file output):</strong></p>
<pre><code class="language-bash">mistralrs quantize -m microsoft/Phi-3.5-mini-instruct --isq q4k -o phi3.5-uqff/phi3.5-mini-instruct-q4k.uqff
</code></pre>
<p><strong>Single quantization (directory output):</strong></p>
<pre><code class="language-bash">mistralrs quantize -m microsoft/Phi-3.5-mini-instruct --isq q4k -o phi3.5-uqff/
</code></pre>
<p><strong>Multiple quantizations at once (directory output):</strong></p>
<p>Generate multiple UQFF files by specifying multiple <code>--isq</code> types. All quantizations go to the same output directory.</p>
<pre><code class="language-bash"># Comma-separated ISQ types
mistralrs quantize -m microsoft/Phi-3.5-mini-instruct --isq q4k,q8_0 -o phi3.5-uqff/

# Equivalent: repeated --isq flags
mistralrs quantize -m microsoft/Phi-3.5-mini-instruct --isq q4k --isq q8_0 -o phi3.5-uqff/
</code></pre>
<p>This produces the following in <code>phi3.5-uqff/</code>:</p>
<ul>
<li><code>q4k-0.uqff</code> (and additional shards <code>q4k-1.uqff</code>, … if the model is large)</li>
<li><code>q8_0-0.uqff</code> (and additional shards if needed)</li>
<li><code>README.md</code> (auto-generated model card for Hugging Face)</li>
<li>Shared files: <code>config.json</code>, <code>tokenizer.json</code>, <code>residual.safetensors</code>, etc.</li>
</ul>
<blockquote>
<p>Note: Multiple <code>--isq</code> values require a directory output path (not a <code>.uqff</code> file path).</p>
</blockquote>
<h3 id="model-card-generation"><a class="header" href="#model-card-generation">Model card generation</a></h3>
<p>When using directory output mode, the <code>quantize</code> command automatically generates a <code>README.md</code> model card in the output directory. This model card includes Hugging Face YAML frontmatter, a description, and an examples table with the appropriate <code>--from-uqff</code> commands for each quantization.</p>
<p>To skip model card generation, use <code>--no-readme</code>:</p>
<pre><code class="language-bash">mistralrs quantize -m microsoft/Phi-3.5-mini-instruct --isq q4k -o phi3.5-uqff/ --no-readme
</code></pre>
<h3 id="uploading-to-hugging-face"><a class="header" href="#uploading-to-hugging-face">Uploading to Hugging Face</a></h3>
<p>After quantization completes in directory mode, the <code>quantize</code> command prints the <code>huggingface-cli</code> upload command you can use. The general form is:</p>
<pre><code class="language-bash">huggingface-cli upload &lt;YOUR_USERNAME&gt;/&lt;MODEL_NAME&gt;-UQFF &lt;output_dir&gt; --repo-type model --private
</code></pre>
<p>Alternatively, you can upload with Git LFS:</p>
<ol>
<li>Install <a href="https://github.com/git-lfs/git-lfs?tab=readme-ov-file#installing">git-lfs</a></li>
<li>Run <code>git lfs install</code></li>
<li>(If the files are larger than <strong>5GB</strong>) Run <code>huggingface-cli lfs-enable-largefiles .</code> (you will need to <code>pip install huggingface_hub</code>)</li>
</ol>
<p>After this, you can use Git to track, commit, and push files.</p>
<h2 id="list-of-models"><a class="header" href="#list-of-models">List of models</a></h2>
<p>You can find a list of models in the <a href="https://huggingface.co/collections/EricB/uqff-670e4a49d56ecdd3f7f0fd4c">Hugging Face model collection</a>.</p>
<p>Have you created a UQFF model on Hugging Face? If so, please <a href="https://github.com/EricLBuehler/mistral.rs/issues/new">create an issue</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="uqff-internal-structure"><a class="header" href="#uqff-internal-structure">UQFF internal structure</a></h1>
<p>The following describes the exact memory layout of UQFF tensors of version 0.1.0.</p>
<h2 id="toc-6"><a class="header" href="#toc-6">ToC</a></h2>
<ul>
<li><a href="#gguf-quantization">GGUF quantization</a></li>
<li><a href="#hqq-quantization">HQQ quantization</a></li>
<li><a href="#unquantized-layers">Uquantized layers</a></li>
<li><a href="#fp8-layers">FP8 layers</a></li>
<li><a href="#standard-tensors">Standard tensors</a></li>
</ul>
<h2 id="gguf-quantization"><a class="header" href="#gguf-quantization">GGUF quantization</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Element type</th><th>Endianness</th></tr>
</thead>
<tbody>
<tr><td>UQFF version</td><td>u32</td><td>little endian</td></tr>
<tr><td>ISQ type (0)</td><td>u8</td><td>little endian</td></tr>
<tr><td>Tensor data length in bytes</td><td>u32</td><td>little endian</td></tr>
<tr><td>Whether bias data is included (boolean)</td><td>u8</td><td>little endian</td></tr>
<tr><td>Quantized dtype</td><td>u32</td><td>little endian</td></tr>
<tr><td>Num shape dims</td><td>u32</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> quantized weight shape dims</td><td>u32</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> quantized weight data</td><td>u8</td><td>little endian</td></tr>
<tr><td><strong>[Optional]</strong> <strong>Array</strong> Bias tensor data, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
</tbody>
</table>
</div>
<h2 id="unquantized-layers"><a class="header" href="#unquantized-layers">Unquantized layers</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Element type</th><th>Endianness</th></tr>
</thead>
<tbody>
<tr><td>UQFF version</td><td>u32</td><td>little endian</td></tr>
<tr><td>ISQ type (1)</td><td>u8</td><td>little endian</td></tr>
<tr><td>Whether bias data is included (boolean)</td><td>u8</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> Weight tensor data, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
<tr><td><strong>[Optional]</strong> <strong>Array</strong> Bias tensor data, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
</tbody>
</table>
</div>
<h2 id="fp8-layers"><a class="header" href="#fp8-layers">FP8 layers</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Element type</th><th>Endianness</th></tr>
</thead>
<tbody>
<tr><td>UQFF version</td><td>u32</td><td>little endian</td></tr>
<tr><td>ISQ type (1)</td><td>u8</td><td>little endian</td></tr>
<tr><td>Whether bias data is included (boolean)</td><td>u8</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> Weight tensor data, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
<tr><td>Dequant W scalar</td><td>f32</td><td>little endian</td></tr>
<tr><td>Dequant X scalar</td><td>f32</td><td>little endian</td></tr>
<tr><td>Quant scalar</td><td>f32</td><td>little endian</td></tr>
<tr><td>Quantization type</td><td>u32</td><td>little endian</td></tr>
<tr><td><strong>[Optional]</strong> <strong>Array</strong> Bias tensor data, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
</tbody>
</table>
</div>
<h2 id="hqq-quantization"><a class="header" href="#hqq-quantization">HQQ quantization</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Element type</th><th>Endianness</th></tr>
</thead>
<tbody>
<tr><td>UQFF version</td><td>u32</td><td>little endian</td></tr>
<tr><td>ISQ type (2)</td><td>u8</td><td>little endian</td></tr>
<tr><td>Whether bias data is included (boolean)</td><td>u8</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> Q weight, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
<tr><td><strong>Array</strong> Q scale, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
<tr><td><strong>Array</strong> Q zeroes, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
<tr><td>Dequant weight num shape dims</td><td>u32</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> dequant weight shape dims</td><td>u32</td><td>little endian</td></tr>
<tr><td>CFG bits</td><td>u8</td><td>little endian</td></tr>
<tr><td>CFG group size</td><td>u32</td><td>little endian</td></tr>
<tr><td>CFG axis</td><td>u8</td><td>little endian</td></tr>
<tr><td>CFG optimization steps (0 means <code>Option::None</code> for now)</td><td>u32</td><td>little endian</td></tr>
<tr><td>CFG round zeroes (boolean)</td><td>u8</td><td>little endian</td></tr>
<tr><td>CFG channel wise (boolean)</td><td>u8</td><td>little endian</td></tr>
</tbody>
</table>
</div>
<h2 id="fp8-layers-1"><a class="header" href="#fp8-layers-1">FP8 layers</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Element type</th><th>Endianness</th></tr>
</thead>
<tbody>
<tr><td>UQFF version</td><td>u32</td><td>little endian</td></tr>
<tr><td>ISQ type (3)</td><td>u8</td><td>little endian</td></tr>
<tr><td>Whether bias data is included (boolean)</td><td>u8</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> Weight tensor data, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
<tr><td>Dequant scale W</td><td>f32</td><td>little endian</td></tr>
<tr><td>Dequant scale X</td><td>f32</td><td>little endian</td></tr>
<tr><td>Quant scale</td><td>f32</td><td>little endian</td></tr>
<tr><td>Layer dtype</td><td>u32</td><td>little endian</td></tr>
<tr><td><strong>[Optional]</strong> <strong>Array</strong> Bias tensor data, see <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td><td>See <a href="#standard-tensors">docs</a></td></tr>
</tbody>
</table>
</div>
<h2 id="standard-tensors"><a class="header" href="#standard-tensors">Standard tensors</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Element type</th><th>Endianness</th></tr>
</thead>
<tbody>
<tr><td>Tensor data length in bytes</td><td>u32</td><td>little endian</td></tr>
<tr><td>Tensor dtype</td><td>u32</td><td>little endian</td></tr>
<tr><td>Num shape dims</td><td>u32</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> shape dims</td><td>u32</td><td>little endian</td></tr>
<tr><td><strong>Array</strong> flattened (contiguous) tensor data</td><td>u8</td><td>little endian</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="model-topology-configuration"><a class="header" href="#model-topology-configuration">Model topology configuration</a></h1>
<h3>Quantization and device mapping in one file.</h3>

<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>Manual device mapping flags are deprecated in favor of automatic placement because it is easy to misconfigure them. Topology files remain the preferred way to express per-layer quantization, and you can still provide <code>device</code> overrides here when you truly need to. Those overrides win over the automatic mapper, so apply them sparingly. See the <a href="#device-mapping-1">device mapping documentation</a> for guidance.</p>
</blockquote>
<p>Use a simple model topology to configure ISQ and device mapping for <em>per-layer</em> with a single <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/topologies/isq_and_device.yml">YAML file</a> (examples <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/topologies">here</a>)!</p>
<p>To support per-layer mix of ISQ, Mistral.rs supports loading a model topology YAML file. This YAML file is formatted as follows:</p>
<ol>
<li>Top-level keys are either:
<ul>
<li>A range of layers (<code>start-end</code>) where <code>start &lt; end</code>. <code>start</code> is inclusive and <code>end</code> is exclusive</li>
<li>A single layer number</li>
</ul>
<ol start="2">
<li>The topology for the range or layer:
<ul>
<li>An optional key (<code>isq</code>) which maps to a single value, which can be any <a href="#isq-quantization-types">ISQ type</a>. If not specified, there is no ISQ for this range of layers applied.</li>
<li>An optional key (<code>device</code>) which maps to a single value, which is one of the below. If not specified, the default loading deice will be used.
<ul>
<li><code>cpu</code></li>
<li><code>cuda[ORDINAL]</code></li>
<li><code>metal[ORDINAL]</code></li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<p>Note that:</p>
<ul>
<li>The topology for the range is expanded to fill the range</li>
<li>If ranges overlap, the range with the higher end layer takes precedence. When two ranges share the same end layer, the one that appears later in the topology file wins.</li>
<li>Any layers which are not covered will have no topology mapping. They will inherit any other ISQ (e.g. with <code>--isq</code>/<code>in_situ_quant</code>) set.</li>
<li>Unless the layer is not covered by the topology, the topology value will override any other ISQ (e.g. with <code>--isq</code>/<code>in_situ_quant</code>).</li>
<li>The topology device mapping will override any other device mapping.</li>
</ul>
<h3 id="using-topology-with-uqff-models"><a class="header" href="#using-topology-with-uqff-models">Using topology with UQFF models</a></h3>
<p>When loading a <a href="#universal-quantized-file-format-uqff">UQFF</a> model, the quantization is already applied during UQFF creation. Therefore:</p>
<ul>
<li><strong>ISQ settings in the topology are ignored</strong> - the pre-quantized weights are used as-is</li>
<li><strong>Device mapping still applies</strong> - you can split layers across GPUs or offload to CPU</li>
</ul>
<p>This is useful for deploying pre-quantized models across multiple devices without re-quantizing.</p>
<p>Example topology for UQFF device mapping:</p>
<pre><code class="language-yaml"># Only device mapping is used; isq would be ignored
0-16:
  device: cuda[0]
16-32:
  device: cuda[1]
</code></pre>
<p>See the <a href="#using-topology-for-device-mapping-with-uqff">UQFF documentation</a> for complete examples.</p>
<h3 id="regex-selectors"><a class="header" href="#regex-selectors">Regex selectors</a></h3>
<p>Layer ranges are convenient when you know the numeric index, but you can also target weights by name. Keys wrapped in <code>/.../</code> are interpreted as regular expressions that are matched against the fully qualified tensor name (for example, <code>model.layers.3.attn.q_proj.weight</code>). Regex selectors may override both <code>isq</code> and <code>device</code>.</p>
<pre><code class="language-yml">'/attn\.q_proj$/':
  isq: Q4K
'/ffn_.*\.weight$/':
  isq: Q3K
</code></pre>
<p>Regex-based ISQ overrides are applied through the immediate ISQ system, so they quantize weights as they are loaded. Numeric layer ranges continue to be handled by the post-load topology pass. Regex selectors are evaluated top-to-bottom as they appear in the YAML file, so a selector that comes later in the file overrides earlier matches.</p>
<pre><code class="language-yml">0-8:
  isq: Q3K
  device: cuda[0]
8-16:
  isq: Q4K
  device: cpu
16-24:
  isq: Q6K
# Skip 24-28
28-32:
  isq: Q8_0
  device: cuda[0]
</code></pre>
<p>Model topologies may be applied to all model types.</p>
<h2 id="cli-example"><a class="header" href="#cli-example">CLI example</a></h2>
<pre><code>mistralrs run -m microsoft/Phi-3-mini-128k-instruct --topology topologies/isq.yml
</code></pre>
<h2 id="http-server-example"><a class="header" href="#http-server-example">HTTP server example</a></h2>
<pre><code>mistralrs serve -p 1234 -m microsoft/Phi-3-mini-128k-instruct --topology topologies/isq.yml
</code></pre>
<h2 id="rust-example-3"><a class="header" href="#rust-example-3">Rust example</a></h2>
<p>Example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/topology/main.rs">here</a>.</p>
<h2 id="python-example-3"><a class="header" href="#python-example-3">Python example</a></h2>
<p>Example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/topology.py">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enhancing-isq-with-an-imatrix"><a class="header" href="#enhancing-isq-with-an-imatrix">Enhancing ISQ with an imatrix</a></h1>
<p>Mistral.rs supports enhancing the performance of models quantized with ISQ by collecting an imatix from <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/calibration_data/">calibration data</a>. The following quantizations are supported with an imatrix:</p>
<ul>
<li><code>Q2K</code></li>
<li><code>Q3K</code></li>
<li><code>Q4K</code></li>
<li><code>Q5K</code></li>
<li><code>Q6K</code></li>
</ul>
<blockquote>
<p><strong>What is an imatrix?</strong> An imatrix (importance matrix) is generated from data collected during the execution of the model on calibration data. This data is used to enhance the performance of the model by enabling a weighted RMSE minimization when quantizing the tensor. For more information, see the <a href="https://github.com/ggerganov/llama.cpp/pull/4861">original PR</a>.</p>
</blockquote>
<p>Using an imatrix causes the quantization process to take longer as the data must be collected, but there is no inference-time performance decrease.</p>
<blockquote>
<p>Note: mistral.rs will automatically generate a <strong>.cimatrix</strong> file which can be used within mistral.rs as a replacement for a .imatrix file. The primary advantage is the in-situ generation within mistral.rs. The format is incompatible with llama.cpp.</p>
</blockquote>
<p>To use this, simply specify the calibration data file in the various APIs as detailed below.</p>
<h2 id="with-the-cli"><a class="header" href="#with-the-cli">With the CLI</a></h2>
<pre><code class="language-bash">mistralrs run --isq 4 -m meta-llama/Llama-3.2-3B-Instruct --calibration-file calibration_data/calibration_datav3_small.txt
</code></pre>
<h2 id="with-the-rust-sdk"><a class="header" href="#with-the-rust-sdk">With the Rust SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/mistralrs/examples/imatrix/">here</a>.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let model = TextModelBuilder::new("meta-llama/Llama-3.2-3B-Instruct")
    .with_isq(IsqType::Q4K)
    .with_calibration_file("calibration_data/calibration_datav3_small.txt".into())
    .with_logging()
    .with_paged_attn(|| PagedAttentionMetaBuilder::default().build())?
    .build()
    .await?;
<span class="boring">}</span></code></pre>
<h2 id="with-the-python-sdk"><a class="header" href="#with-the-python-sdk">With the Python SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/imatrix.py">here</a>.</p>
<pre><code class="language-python">runner = Runner(
    which=Which.Plain(
        model_id="meta-llama/Llama-3.2-3B-Instruct",
        calibration_file="calibration_data/calibration_datav3_small.txt"
    ),
    in_situ_quant="4",
)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="adapter-model-support"><a class="header" href="#adapter-model-support">Adapter model support</a></h1>
<p>An adapter model is a model with X-LoRA or LoRA. X-LoRA support is provided by selecting an <code>XLora*</code> architecture, and LoRA support by selecting the <code>Lora*</code> architecture. For both X-LoRA and LoRA, an ordering file (see <a href="#adapter-ordering-file">this section</a> for preparing the ordering file) must be provided. The ordering file describes the ordering of layers and which adapters to use (and what order to use them in for X-LoRA).</p>
<p>When using an adapter model with a quantized base model, if the ordering file specifies unsupported layers you will receive an error.</p>
<h2 id="supported-x-lora-or-lora-quantized-layers"><a class="header" href="#supported-x-lora-or-lora-quantized-layers">Supported X-LoRA or LoRA quantized layers**</a></h2>
<p><strong>Llama architecture:</strong></p>
<ul>
<li>model.layers.{layer_idx}.self_attn.q_proj</li>
<li>model.layers.{layer_idx}.self_attn.k_proj</li>
<li>model.layers.{layer_idx}.self_attn.v_proj</li>
<li>model.layers.{layer_idx}.self_attn.o_proj</li>
<li>model.layers.{layer_idx}.mlp.up_proj</li>
<li>model.layers.{layer_idx}.mlp.down_proj</li>
<li>model.layers.{layer_idx}.mlp.gate_proj</li>
<li>lm_head</li>
</ul>
<p><strong>Phi 3 architecture:</strong></p>
<ul>
<li>model.layers.{layer_idx}.self_attn.qkv_proj</li>
<li>model.layers.{layer_idx}.self_attn.o_proj</li>
<li>model.layers.{layer_idx}.mlp.gate_up_proj</li>
<li>model.layers.{layer_idx}.mlp.down_proj</li>
<li>lm_head</li>
</ul>
<h2 id="adapter-ordering-file"><a class="header" href="#adapter-ordering-file">Adapter ordering file</a></h2>
<p><strong>Preparing the X-LoRA/LoRA Ordering File</strong>
The X-LoRA/LoRA ordering file is necessary to prepare before inference with an X-LoRA model. However, it is easy with a provided <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/scripts/create_ordering.py"><code>script</code></a>!</p>
<h3 id="x-lora-case"><a class="header" href="#x-lora-case">X-LoRA case</a></h3>
<p>An ordering JSON file for X-LoRA contains 2 major parts.</p>
<ol>
<li>The adapter names <code>order</code>
<ul>
<li>The order matters!</li>
<li>Should be an array of strings which are the adapter names corresponding to the order the adapters were specified during training. For example, if the adapters were specified as a dictionary:</li>
</ul>
</li>
<li>The layer ordering <code>layers</code>
<ul>
<li>Automatically generated and should not be manipulated as it controls the application of scalings.</li>
</ul>
</li>
</ol>
<pre><code class="language-python">adapters = {
    "math": ...,
    "reasoning": ...,
    "biology": ...
}
</code></pre>
<p>The specified order would be <code>["math", "reasoning", "biology"]</code>.</p>
<p>We provide an <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/orderings/xlora-paper-ordering.json">ordering file</a> which contains the ordering for the X-LoRA model associated with <a href="https://arxiv.org/abs/2402.07148">the paper</a> and the Huggingface repository: https://huggingface.co/lamm-mit/x-lora.</p>
<h3 id="lora-case"><a class="header" href="#lora-case">LoRA case</a></h3>
<p>An ordering JSON file for LoRA contains 2 major parts:</p>
<ol>
<li>The adapter names <code>order</code> (optional):
<ul>
<li>The order does not matter</li>
<li>Come controls which adapters will be initially activated</li>
<li>If this key is not specified, then no adapters will be activated initially</li>
</ul>
</li>
<li>Preload adapter section <code>preload_adapters</code> (optional): <a href="#adapter-model-dynamic-adapter-activation">see this section</a>
<ul>
<li>Order does not matter</li>
<li>Specifies the adapter name and the model ID to find them, which may be a local path.</li>
</ul>
</li>
</ol>
<h3 id="preparing-the-ordering-file-lora-or-x-lora-cases"><a class="header" href="#preparing-the-ordering-file-lora-or-x-lora-cases">Preparing the ordering file (LoRA or X-LoRA cases)</a></h3>
<p>There are 2 scripts to prepare the ordering file and which work for both X-LoRA and LoRA. The ordering file is specific to each architecture and set of target modules. Therefore, if either are changed, it is necessary to create a new ordering file using the first option. If only the adapter order or adapters changed, then the second option should be used.</p>
<ol>
<li>
<p>From scratch: No ordering file for the architecture and target modules</p>
<p>A script <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/scripts/create_ordering.py"><code>create_ordering.py</code></a> is provided which prompts the user for the model ID, target modules, and adapter names. The user is prompted for an output file location, relative to the working directory.</p>
</li>
<li>
<p>Create a new ordering file from an existing ordering file for an architecture and target modules</p>
<p>A script <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/scripts/set_names.py"><code>set_names.py</code></a> is provided which prompts the user for the adapter names and the old ordering file. The user is prompted for an output file location, relative to the working directory.</p>
</li>
</ol>
<h3 id="quantized-x-lora-or-lora-models"><a class="header" href="#quantized-x-lora-or-lora-models">Quantized X-LoRA or LoRA models</a></h3>
<p>Mistral.rs supports running quantized models with X-LoRA or LoRA. The X-LoRA or LoRA adapter layers will not be quantized, only the base model. P</p>
<p>In the X-LoRA case, please note that using a high quantization level (eg., 4-bit) can distort the signal and prevent the classifier from acting properly. Therefore, it is better to use slightly lower levels such as 8-bit.</p>
<h2 id="avoiding-the-scaling-pass-with-non-granular-scalings"><a class="header" href="#avoiding-the-scaling-pass-with-non-granular-scalings">Avoiding the scaling pass with non-granular scalings</a></h2>
<p>The X-LoRA implementation supports non-granular scalings. This caches the scalings after <code>k</code> completion tokens are generated and they will be used for the remaining passes avoiding the scaling pass. The number of tokens to generate before caching is defined by setting <code>tgt_non_granular_index</code>. Setting <code>tgt_non_granular_index</code> will restrict the maximum running sequences to 1.</p>
<p>Please see <a href="#x-lora-non-granular-scalings">this page</a> for more details and examples.</p>
<h2 id="adapter-model-dynamic-adapter-activation"><a class="header" href="#adapter-model-dynamic-adapter-activation">Adapter model dynamic adapter activation</a></h2>
<p>We support dynamic adapter activation for LoRA models, allowing you to activate a set of adapters at runtime. There is a Python, Rust and HTTP API:</p>
<ul>
<li>Rust: <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/lora/main.rs">example</a></li>
<li>Python: <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/lora_zephyr.py">example</a></li>
<li>HTTP: <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/adapter_chat.py">example</a></li>
</ul>
<p>To use this feature, you should add a <code>preload_adapters</code> key to your ordering file:</p>
<pre><code class="language-diff">{
    "order": ["..."],
    "layers": {"...": "123"},
    "base_model_id": "...",
+    "preload_adapters": [{"name": "...", "adapter_model_id": "..."}] # New field here
}
</code></pre>
<p>This allows mistral.rs to preload the adapter and enable runtime activation.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="examples-of-lora-and-x-lora-models"><a class="header" href="#examples-of-lora-and-x-lora-models">Examples of LoRA and X-LoRA models</a></h1>
<ul>
<li>X-LoRA with no quantization</li>
</ul>
<p>To start an X-LoRA server with the exactly as presented in <a href="https://arxiv.org/abs/2402.07148">the paper</a>:</p>
<pre><code class="language-bash">mistralrs serve -p 1234 --xlora lamm-mit/x-lora --xlora-order orderings/xlora-paper-ordering.json
</code></pre>
<ul>
<li>LoRA with a model from GGUF</li>
</ul>
<p>To start a LoRA server with adapters from the X-LoRA paper (you should modify the ordering file to use only one adapter, as the adapter static scalings are all 1 and so the signal will become distorted):</p>
<pre><code class="language-bash">mistralrs serve -p 1234 --format gguf -m TheBloke/zephyr-7B-beta-GGUF -f zephyr-7b-beta.Q8_0.gguf --lora lamm-mit/x-lora
</code></pre>
<p>Normally with a LoRA model you would use a custom ordering file. However, for this example we use the ordering from the X-LoRA paper because we are using the adapters from the X-LoRA paper.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="x-lora-non-granular-scalings"><a class="header" href="#x-lora-non-granular-scalings">X-LoRA non-granular scalings</a></h1>
<p>A key limitation of the X-LoRA architecture is the need for 2 forward passes of the model per generation step. To trade off model performance for speed, mistral.rs allows the user to reduce the granularity of the scalings by caching them in a technique we call Non Granular Scalings.</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h2>
<p>For the first $k$ generation steps, the scalings are calculated normally for each token. However, for the rest of the tokens, it is cached and re-used. In this way, we are able to avoid the second forward pass and the performance is increased significantly. To maintain correctness, enabling non-granular scalings will restrict the engine to processing one sequence at a time.</p>
<h2 id="how-to-use-it"><a class="header" href="#how-to-use-it">How to use it</a></h2>
<h3 id="command-line"><a class="header" href="#command-line">Command line</a></h3>
<p>This can be enabled by passing <code>--tgt-non-granular-index</code> followed by $k$:</p>
<pre><code class="language-bash">mistralrs serve -p 1234 --xlora lamm-mit/x-lora --xlora-order orderings/xlora-paper-ordering.json --tgt-non-granular-index 5
</code></pre>
<h3 id="python-14"><a class="header" href="#python-14">Python</a></h3>
<p>Set the <code>tgt_non_granular_index</code> attribute to a non-<code>None</code> value in the <code>Which</code> selection:</p>
<pre><code class="language-py">from mistralrs import Runner, Which

runner = Runner(
    which=Which.XLoraGGUF(
        tok_model_id=None,  # Automatically determine from ordering file
        quantized_model_id="TheBloke/zephyr-7B-beta-GGUF",
        quantized_filename="zephyr-7b-beta.Q4_0.gguf",
        xlora_model_id="lamm-mit/x-lora",
        order="orderings/xlora-paper-ordering.json",
        tgt_non_granular_index=5,
    )
)

...
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="build-a-memory-efficient-moe-model-from-anything-in-seconds"><a class="header" href="#build-a-memory-efficient-moe-model-from-anything-in-seconds">Build a memory-efficient MoE model from anything, in seconds</a></h1>
<p>AnyMoE is technique to dynamically and efficiently create MoE models. By providing a set of experts and a small pretraining dataset, you can create an MoE locally!</p>
<p>It has the following features:</p>
<ul>
<li>Apply AnyMoE to any supported model
<ul>
<li><code>plain</code></li>
<li><code>vision-plain</code></li>
</ul>
</li>
<li>Specify the layers to apply AnyMoE to for efficient training</li>
</ul>
<p>Paper: https://arxiv.org/abs/2405.19076</p>
<p>https://github.com/EricLBuehler/mistral.rs/assets/65165915/33593903-d907-4c08-a0ac-d349d7bf33de</p>
<blockquote>
<p>Note: By default, this has the capability to create an csv loss image. When building from source (for Python or CLI), you may use <code>--no-default-features</code> command line to disable this. This may be necessary if networking is unavailable.</p>
</blockquote>
<h2 id="dataset"><a class="header" href="#dataset">Dataset</a></h2>
<p>Currently, AnyMoE expects a JSON dataset with one top-level key <code>row</code>, which is an array of objects with keys <code>prompt</code> (string), <code>expert</code> (integer), and <code>image_urls</code> (optional array of strings). For example:</p>
<pre><code class="language-json">{
    "rows": [
        {
            "prompt": "Discuss the impact of Renaissance art on modern aesthetics",
            "expert": 0
        },
        {
            "prompt": "Explain the significance of the theory of relativity in modern physics",
            "expert": 1
        },
    ]
}
  
</code></pre>
<p>For a vision model, <code>image_urls</code> may contain an array of image URLs/local paths or Base64 encoded images.</p>
<h2 id="experts"><a class="header" href="#experts">Experts</a></h2>
<p>AnyMoE experts can be either fine-tuned models or LoRA adapter models. Only the mlp layers will be loaded from each. The experts must be homogeneous: they must be all fine-tuned or all adapter. Additionally, certain layers can be specified to apply AnyMoE.</p>
<blockquote>
<p>Note: When using LoRA adapter experts, it may not be necessary to set the layers where AnyMoE will be applied due to the lower memory usage.</p>
</blockquote>
<h3 id="example-of-toml-selector-with-fine-tuned-experts"><a class="header" href="#example-of-toml-selector-with-fine-tuned-experts">Example of TOML selector with fine-tuned experts</a></h3>
<pre><code class="language-toml">[model]
model_id = "mistralai/Mistral-7B-Instruct-v0.1"
arch = "mistral"

[anymoe]
dataset_json = "examples/amoe.json"
prefix = "model.layers"
mlp = "mlp"
model_ids = ["HuggingFaceH4/zephyr-7b-beta"]
layers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

[anymoe.config]
hidden_size = 4096
expert_type = "fine_tuned"
</code></pre>
<h3 id="example-of-toml-selector-with-lora-adapter-experts"><a class="header" href="#example-of-toml-selector-with-lora-adapter-experts">Example of TOML selector with LoRA adapter experts</a></h3>
<pre><code class="language-toml">[model]
model_id = "HuggingFaceH4/zephyr-7b-beta"
arch = "mistral"

[anymoe]
dataset_json = "examples/amoe.json"
prefix = "model.layers"
mlp = "mlp"
model_ids = ["EricB/example_adapter"]
layers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

[anymoe.config]
hidden_size = 4096

[anymoe.config.expert_type.lora_adapter]
rank = 16
alpha = 16
target_modules = ["gate_proj"]
</code></pre>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<h2 id="cli"><a class="header" href="#cli">CLI</a></h2>
<p>CLI usage is via the <a href="#anymoe">TOML selector</a> where you can also find docs on the required fields.</p>
<p>For example, to use the demo fine-tuned expert:</p>
<pre><code class="language-bash">mistralrs from-config --file toml-selectors/anymoe.toml
</code></pre>
<p>To use the demo LoRA expert:</p>
<pre><code class="language-bash">mistralrs from-config --file toml-selectors/anymoe_lora.toml
</code></pre>
<h2 id="python-example-4"><a class="header" href="#python-example-4">Python example</a></h2>
<pre><code class="language-py">from mistralrs import (
    Runner,
    Which,
    ChatCompletionRequest,
    Architecture,
    AnyMoeConfig,
    AnyMoeExpertType,
)

runner = Runner(
    which=Which.Plain(
        model_id="mistralai/Mistral-7B-Instruct-v0.1",
        arch=Architecture.Mistral,
    ),
    anymoe_config=AnyMoeConfig(
        hidden_size=4096,
        dataset_json="examples/amoe.json",
        prefix="model.layers",
        mlp="mlp",
        expert_type=AnyMoeExpertType.FineTuned(),
        lr=1e-3,
        epochs=100,
        batch_size=4,
        model_ids=["HuggingFaceH4/zephyr-7b-beta"],
    ),
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-12"><a class="header" href="#rust-sdk-12">Rust SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/anymoe/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    AnyMoeConfig, AnyMoeExpertType, AnyMoeModelBuilder, IsqType, PagedAttentionMetaBuilder,
    TextMessageRole, TextMessages, TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let text_builder = TextModelBuilder::new("mistralai/Mistral-7B-Instruct-v0.1")
        .with_isq(IsqType::Q8_0)
        .with_logging()
        .with_paged_attn(PagedAttentionMetaBuilder::default().build()?);

    let model = AnyMoeModelBuilder::from_text_builder(
        text_builder,
        AnyMoeConfig {
            hidden_size: 4096,
            lr: 1e-3,
            epochs: 100,
            batch_size: 4,
            expert_type: AnyMoeExpertType::LoraAdapter {
                rank: 64,
                alpha: 16.,
                target_modules: vec!["gate_proj".to_string()],
            },
            gate_model_id: None, // Set this to Some("path/to/model/id") for the pretrained gating model id
            training: true,
            loss_csv_path: None,
        },
        "model.layers",
        "mlp",
        "examples/amoe.json",
        vec!["HuggingFaceH4/zephyr-7b-beta"],
        vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
    )
    .build()
    .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="matformer-matryoshka-transformer-support"><a class="header" href="#matformer-matryoshka-transformer-support">Matformer (Matryoshka Transformer) Support</a></h1>
<p>Matformer allows you to dynamically resize transformer models at runtime, trading compute/memory for quality. This enables deploying the same model across devices with different resource constraints - from edge devices to powerful GPUs.</p>
<h2 id="quick-start-1"><a class="header" href="#quick-start-1">Quick Start</a></h2>
<h3 id="command-line-1"><a class="header" href="#command-line-1">Command Line</a></h3>
<pre><code class="language-bash"># Run Gemma 3n with the E2.49B configuration (2.49B params instead of 3.98B)
mistralrs run -m google/gemma-3n-E4B-it \
  --matformer-config-path matformer_configs/gemma3n.csv \
  --matformer-slice-name "Config for E2.49B (block-level)"
</code></pre>
<h3 id="python-15"><a class="header" href="#python-15">Python</a></h3>
<pre><code class="language-python">from mistralrs import Runner, Which, VisionArchitecture

runner = Runner(
    which=Which.VisionPlain(
        model_id="google/gemma-3n-E4B-it",
        arch=VisionArchitecture.Gemma3n,
        matformer_config_path="matformer_configs/gemma3n.csv",
        matformer_slice_name="Config for E2.49B (block-level)",
    ),
)
</code></pre>
<h3 id="rust-14"><a class="header" href="#rust-14">Rust</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mistralrs::VisionModelBuilder;
use std::path::PathBuf;

let model = VisionModelBuilder::new("google/gemma-3n-E4B-it")
    .with_matformer_config_path(PathBuf::from("matformer_configs/gemma3n.csv"))
    .with_matformer_slice_name("Config for E2.49B (block-level)".to_string())
    .build()
    .await?;
<span class="boring">}</span></code></pre>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h2>
<p>Matformer models are pre-trained with a special architecture that allows certain layers to be skipped at inference time while maintaining reasonable quality. When you select a “slice”:</p>
<ol>
<li><strong>Layer Skipping</strong>: Specified layers are completely removed from computation</li>
<li><strong>FFN Resizing</strong>: Feed-forward network dimensions can be adjusted per layer</li>
<li><strong>Automatic Remapping</strong>: Remaining layers are renumbered sequentially</li>
</ol>
<p>For example, the Gemma 3n E2.49B (block-level) slice:</p>
<ul>
<li>Keeps all 35 layers (no layer skipping)</li>
<li>Uses mixed FFN dimensions: 8192 for layers 0-19, 16384 for layers 20-24, 8192 for layers 25-34</li>
<li>Cuts parameters from 3.98B to 2.49B (~37% reduction)</li>
<li>Maintains ~87% of the full model’s quality</li>
</ul>
<h2 id="configuration-files"><a class="header" href="#configuration-files">Configuration Files</a></h2>
<p>Matformer configurations are CSV files with these columns:</p>
<pre><code class="language-csv">name,# Layers,# Effective Params (B),MMLU PT accuracy,FFN Hidden Dims,Layers Skipped
Main model,35,3.98,62.30%,"[16384, 16384, ...]",
Config for E2.49B (block-level),35,2.49,54.50%,"[8192, 8192, ..., 16384, 16384, ..., 8192, 8192, ...]",
</code></pre>
<ul>
<li><strong>name</strong>: Slice identifier used in <code>matformer_slice_name</code></li>
<li><strong># Layers</strong>: Number of active layers after skipping</li>
<li><strong># Effective Params (B)</strong>: Approximate parameter count in billions</li>
<li><strong>MMLU PT accuracy</strong>: Benchmark score (informational)</li>
<li><strong>FFN Hidden Dims</strong>: List of FFN dimensions for each layer</li>
<li><strong>Layers Skipped</strong>: Which layers to remove (0-indexed)</li>
</ul>
<h2 id="supported-models-2"><a class="header" href="#supported-models-2">Supported Models</a></h2>
<p>Currently supported:</p>
<ul>
<li><strong>Gemma 3n</strong> (<code>google/gemma-3n-E4B-it</code>) - Multimodal model with vision and audio</li>
</ul>
<p>See <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/matformer_configs/"><code>matformer_configs/</code></a> for available configurations.</p>
<h2 id="performance-guide"><a class="header" href="#performance-guide">Performance Guide</a></h2>
<h3 id="memory-usage-1"><a class="header" href="#memory-usage-1">Memory Usage</a></h3>
<p>Memory scales approximately with parameter count:</p>
<ul>
<li>Full model (3.98B): ~8GB VRAM</li>
<li>E2.49B slice: ~5GB VRAM</li>
<li>E2B slice (1.91B): ~4GB VRAM</li>
<li>Smaller slices: Proportionally less</li>
</ul>
<h3 id="inference-speed"><a class="header" href="#inference-speed">Inference Speed</a></h3>
<p>Speed improvement is roughly linear with layer count:</p>
<ul>
<li>30 layers vs 35 layers = ~14% faster</li>
<li>20 layers vs 35 layers = ~43% faster</li>
</ul>
<h3 id="quality-trade-offs"><a class="header" href="#quality-trade-offs">Quality Trade-offs</a></h3>
<p>Example accuracy on MMLU benchmark:</p>
<ul>
<li>Full model: 62.3%</li>
<li>E2.98B: 59.5% (-4.5%)</li>
<li>E2.49B: 54.5% (-12.5%)</li>
<li>E2B: 50.9% (-18.3%)</li>
</ul>
<p>Choose based on your requirements:</p>
<ul>
<li><strong>Maximum quality</strong>: Use full model (omit matformer args)</li>
<li><strong>Balanced</strong>: E2.49B to E2.98B configurations (block-level configs recommended)</li>
<li><strong>Resource-constrained</strong>: E2B configuration (1.91B params)</li>
<li><strong>Extreme efficiency</strong>: E1.96B configuration</li>
</ul>
<h2 id="advanced-usage"><a class="header" href="#advanced-usage">Advanced Usage</a></h2>
<h3 id="with-quantization"><a class="header" href="#with-quantization">With Quantization</a></h3>
<p>Combine Matformer with ISQ for maximum efficiency:</p>
<pre><code class="language-python">runner = Runner(
    which=Which.VisionPlain(
        model_id="google/gemma-3n-E4B-it",
        arch=VisionArchitecture.Gemma3n,
        matformer_config_path="matformer_configs/gemma3n.csv",
        matformer_slice_name="Config for E2.49B (block-level)",
    ),
    in_situ_quant="Q4K"  # 4-bit quantization
)
</code></pre>
<h3 id="with-device-mapping"><a class="header" href="#with-device-mapping">With Device Mapping</a></h3>
<p>Matformer works seamlessly with automatic device mapping:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mistralrs::{VisionModelBuilder, DeviceMapSetting, AutoDeviceMapParams};

let model = VisionModelBuilder::new("google/gemma-3n-E4B-it")
    .with_matformer_config_path(PathBuf::from("matformer_configs/gemma3n.csv"))
    .with_matformer_slice_name("Config for E2.49B (block-level)".to_string())
    .with_device_mapping(DeviceMapSetting::Auto(
        AutoDeviceMapParams::default_vision()
    ))
    .build()
    .await?;
<span class="boring">}</span></code></pre>
<p>Only active layers are loaded to GPU, saving memory.</p>
<h2 id="creating-custom-configurations"><a class="header" href="#creating-custom-configurations">Creating Custom Configurations</a></h2>
<p>To create your own Matformer configuration:</p>
<ol>
<li><strong>Start with the full model</strong> as baseline</li>
<li><strong>Identify skippable layers</strong>:
<ul>
<li>Middle layers (10-30) are often good candidates</li>
<li>Avoid early layers (feature extraction) and late layers (final representations)</li>
<li>Never skip special layers (KV-sharing, attention patterns)</li>
</ul>
</li>
<li><strong>Test quality degradation</strong> at each configuration</li>
<li><strong>Create CSV file</strong> with your configurations</li>
</ol>
<p>Example minimal configuration:</p>
<pre><code class="language-csv">name,# Layers,# Effective Params (B),FFN Hidden Dims,Layers Skipped
Tiny,15,0.8,"[4096, 4096, ...]","[5,6,7,10,11,12,15,16,17,20,21,22,25,26,27,30,31,32,33,34]"
</code></pre>
<h2 id="api-reference"><a class="header" href="#api-reference">API Reference</a></h2>
<h3 id="command-line-arguments"><a class="header" href="#command-line-arguments">Command Line Arguments</a></h3>
<ul>
<li><code>--matformer-config-path PATH</code>: Path to CSV configuration file</li>
<li><code>--matformer-slice-name NAME</code>: Exact name of slice from CSV</li>
</ul>
<h3 id="python-parameters"><a class="header" href="#python-parameters">Python Parameters</a></h3>
<pre><code class="language-python">Which.VisionPlain(
    model_id: str,
    arch: VisionArchitecture,
    matformer_config_path: str = None,  # Path to CSV
    matformer_slice_name: str = None,   # Slice name
    # ... other parameters
)
</code></pre>
<h3 id="rust-methods"><a class="header" href="#rust-methods">Rust Methods</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// For VisionModelBuilder
.with_matformer_config_path(path: PathBuf)
.with_matformer_slice_name(name: String)

// For TextModelBuilder (when supported)
.with_matformer_config_path(path: PathBuf)  
.with_matformer_slice_name(name: String)
<span class="boring">}</span></code></pre>
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="common-issues-1"><a class="header" href="#common-issues-1">Common Issues</a></h3>
<p><strong>“Matformer slice ‘X’ not found”</strong></p>
<ul>
<li>Check slice name matches exactly (case-sensitive)</li>
<li>Verify CSV file path is correct</li>
</ul>
<p><strong>“Layers X and Y are reserved and cannot be skipped”</strong></p>
<ul>
<li>Some models have special layers that must not be skipped</li>
<li>Try different layer combinations</li>
</ul>
<p><strong>Memory not reduced as expected</strong></p>
<ul>
<li>Ensure you’re using the slice (check logs)</li>
<li>Skipped layers still need to be loaded initially</li>
<li>Consider combining with quantization</li>
</ul>
<h3 id="debugging"><a class="header" href="#debugging">Debugging</a></h3>
<p>Enable logging to see Matformer details:</p>
<pre><code class="language-bash">RUST_LOG=mistralrs_core=info mistralrs ...
</code></pre>
<p>This shows:</p>
<ul>
<li>Configuration file loaded</li>
<li>Selected slice details</li>
<li>Layers being skipped</li>
<li>Final layer count</li>
</ul>
<h2 id="future-plans"><a class="header" href="#future-plans">Future Plans</a></h2>
<ul>
<li>Support for more model architectures</li>
<li>Dynamic slice switching during runtime</li>
<li>Automatic slice selection based on available resources</li>
<li>Fine-tuning tools for creating new Matformer models</li>
</ul>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/2410.23265">Matryoshka Transformer Paper</a></li>
<li><a href="https://github.com/EricLBuehler/mistral.rs/tree/master/matformer_configs/">Example Configurations</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="device-mapping-1"><a class="header" href="#device-mapping-1">Device mapping</a></h1>
<p>In mistral.rs, device mapping is <strong>automatically managed</strong> to be as performant and easy as possible. Automatic device mapping is enabled
by default in the CLI/server and Python SDK and does not make any changes when the model fits entirely on the GPU.</p>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>If your system has more than one CUDA device, mistral.rs will automatically use <a href="#distributed-inference-in-mistralrs">tensor parallelism</a>. If the model does not
completely fit on the available GPUs, or you wish to use automatic device mapping, you can disable tensor parallelism by setting <code>MISTRALRS_NO_NCCL=1</code>.</p>
</blockquote>
<p>Automatic device mapping works by prioritizing loading models into GPU memory, and any remaining parts are loaded into CPU memory.
Models architectures such as vision models which greatly benefit from GPU acceleration also automatically prioritize keeping those
components on the GPU.</p>
<p>To control the mapping across devices, you can set the following maximum parameters which the model should expect in a prompt.</p>
<ul>
<li>maximum sequence length (default: 4096)</li>
<li>maximum batch size (default: 1)</li>
<li>(vision models) maximum image length (length refers to the edge length) (default: 1024)</li>
<li>(vision models) maximum number of images (default: 1)</li>
</ul>
<p>These parameters do not translate to hard limits during runtime, they only control the mapping.</p>
<h3 id="unified-memory-systems"><a class="header" href="#unified-memory-systems">Unified memory systems</a></h3>
<p>On integrated GPU systems (e.g. Apple Silicon, NVIDIA Grace Blackwell, Jetson) where GPU and CPU share the same physical RAM, the auto device mapper caps the GPU memory budget to a fraction of system RAM (75% by default for CUDA iGPUs, configurable via <code>MISTRALRS_IGPU_MEMORY_FRACTION</code>; Metal uses the <code>iogpu.wired_limit_mb</code> sysctl). CPU offload capacity is limited to the remaining fraction to prevent over-subscription of shared memory. Use <code>mistralrs doctor</code> to check whether your device is detected as unified memory.</p>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>The maximum sequence length is also used to ensure that a KV cache will fit for with and without PagedAttention.</p>
</blockquote>
<h2 id="examples-2"><a class="header" href="#examples-2">Examples</a></h2>
<ul>
<li>Python
<ul>
<li>Text models <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/text_auto_device_map.py">text_auto_device_map.py</a></li>
<li>Vision models <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/vision_auto_device_map.py">vision_auto_device_map.py</a></li>
</ul>
</li>
<li>Rust
<ul>
<li>Text models <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/auto_device_map/main.rs">text_auto_device_map/main.rs</a></li>
<li>Vision models <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/auto_device_map/main.rs">vision_auto_device_map/main.rs</a></li>
</ul>
</li>
<li>Server
<ul>
<li>Text models:</li>
</ul>
<pre><code class="language-bash">mistralrs run --isq 4 -m meta-llama/Llama-3.3-70B-Instruct --max-seq-len 4096 --max-batch-size 2
</code></pre>
<ul>
<li>Vision models:</li>
</ul>
<pre><code class="language-bash">mistralrs run --isq 4 -m meta-llama/Llama-3.2-11B-Vision-Instruct --max-seq-len 4096 --max-batch-size 2 --max-num-images 2 --max-image-length 1024
</code></pre>
</li>
</ul>
<hr>
<p>If you want to manually device map the model (not recommended), please continue reading.</p>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>Manual device mapping is deprecated in favor of automatic device mapping due to the possibility for user error in manual.</p>
</blockquote>
<h2 id="manual-device-mapping"><a class="header" href="#manual-device-mapping">Manual device mapping</a></h2>
<p>There are 2 ways to do device mapping:</p>
<ol>
<li>Specify the number of layers to put on the GPU - this uses the GPU with ordinal 0.</li>
<li>Specify the ordinals and number of layers - this allows for cross-GPU device mapping.</li>
</ol>
<p>The format for the ordinals and number of layers is <code>ORD:NUM;...</code> where ORD is the unique ordinal and NUM is the number of layers for that GPU. This may be repeated as many times as necessary.</p>
<blockquote>
<p>Note: We refer to GPU layers as “device layers” throughout mistral.rs.</p>
</blockquote>
<h2 id="example-of-specifying-ordinals"><a class="header" href="#example-of-specifying-ordinals">Example of specifying ordinals</a></h2>
<pre><code>mistralrs run -n "0:16;1:16" -m gradientai/Llama-3-8B-Instruct-262k
</code></pre>
<blockquote>
<p>Note: In the Python SDK, the “0:16;1:16” string is passed as the list <code>["0:16", "1:16"]</code>.</p>
</blockquote>
<h2 id="example-of-specifying-the-number-of-gpu-layers"><a class="header" href="#example-of-specifying-the-number-of-gpu-layers">Example of specifying the number of GPU layers</a></h2>
<pre><code>mistralrs run -n 16 -m gradientai/Llama-3-8B-Instruct-262k
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pagedattention-in-mistralrs"><a class="header" href="#pagedattention-in-mistralrs">PagedAttention in mistral.rs</a></h1>
<p>Mistral.rs supports PagedAttention (<a href="https://arxiv.org/abs/2309.06180">paper here</a>) to accelerate both normal inference and batched inference on:</p>
<ul>
<li>CUDA (Unix-like platforms such as WSL, Linux)</li>
<li>Metal</li>
</ul>
<p>Our PagedAttention implementation has 2 inputs: GPU KV cache memory size, and block size. This enables you to have fine-tuned control over the available context length, by configuring the available memory for KV cache. When using a CUDA device, PagedAttention is actiated by default but can be disabled with <code>no_paged_attn</code> for Python or <code>no-paged-attn</code> for the CLI tools.</p>
<h2 id="kv-cache-quantization"><a class="header" href="#kv-cache-quantization">KV Cache Quantization</a></h2>
<p>PagedAttention now supports KV cache quantization to reduce memory usage and potentially improve performance. The KV cache can be quantized to FP8 (F8E4M3 format) instead of using the model’s native dtype, significantly reducing memory requirements while maintaining model quality.</p>
<p><strong>Available cache types:</strong></p>
<ul>
<li><code>auto</code> (default): Uses the model’s native dtype for KV cache</li>
<li><code>f8e4m3</code>: Quantizes KV cache to 8-bit floating point (E4M3 format)</li>
</ul>
<p>When using FP8 quantization, the memory usage for KV cache is approximately halved compared to FP16, allowing for longer context lengths with the same GPU memory allocation.</p>
<blockquote>
<p>Note: The default block size if not specified is 32.</p>
</blockquote>
<blockquote>
<p>Note: if OOM occurs (this can be caused by a variety of factors including adapter activation, re-ISQ, and others), it is likely because the PagedAttention KV cache has already been allocated. To counter this, either set the KV cache memory to a lower amount or usage percentage (recommended) or disable paged attention entirely for a dynamically allocated cache.</p>
</blockquote>
<blockquote>
<p>Note: Paged Attention is not enabled on Windows platforms, only Unix-based platforms.</p>
</blockquote>
<blockquote>
<p>Note: In the CLI and Python SDK, Paged Attention is disabled by default for Metal. It can be enabled with the <code>--paged-attn</code>/<code>paged_attn</code> flags.</p>
</blockquote>
<p><strong>There are more features being added to this:</strong></p>
<ul>
<li>GGML model support</li>
<li>Adapter model support</li>
<li>Speculative decoding</li>
</ul>
<p><strong>Prefix caching is now supported with PagedAttention.</strong> PagedAttention can leverage the prefix cacher to cache KV prefix states across iterations for faster multi-turn inference.</p>
<h2 id="block-level-prefix-caching"><a class="header" href="#block-level-prefix-caching">Block-Level Prefix Caching</a></h2>
<p>Prefix caching is a technique to reuse computed KV cache blocks across requests that share common prefixes (like system prompts). This can significantly speed up inference when multiple requests use the same prefix.</p>
<h3 id="how-it-works-2"><a class="header" href="#how-it-works-2">How It Works</a></h3>
<ol>
<li>
<p><strong>Block Hashing</strong>: Each block of tokens is assigned a unique hash based on its contents and the hash of its parent block:</p>
<pre><code>hash(block) = hash(parent_hash, block_tokens)
</code></pre>
<p>This creates a hash chain that uniquely identifies any prefix sequence.</p>
</li>
<li>
<p><strong>Cache Lookup</strong>: When allocating blocks for a new request, the scheduler checks if any full blocks match existing cached blocks by comparing hashes.</p>
</li>
<li>
<p><strong>Block Reuse</strong>: Matched blocks are reused directly - their pre-computed KV cache values are used without recomputation. Only the non-matching suffix tokens need to be processed.</p>
</li>
<li>
<p><strong>LRU Eviction</strong>: When memory is needed, least recently used cached blocks are evicted first.</p>
</li>
</ol>
<h3 id="benefits"><a class="header" href="#benefits">Benefits</a></h3>
<ul>
<li><strong>Multi-turn conversations</strong>: System prompts and conversation history are cached and reused</li>
<li><strong>Batched requests</strong>: Multiple requests with shared prefixes (e.g., same system prompt) benefit from caching</li>
<li><strong>Reduced TTFT</strong>: Time-to-first-token is reduced by skipping prefix computation</li>
</ul>
<h3 id="how-its-enabled"><a class="header" href="#how-its-enabled">How It’s Enabled</a></h3>
<p>Prefix caching is <strong>enabled by default</strong> when using PagedAttention and controlled by the same <code>prefix_cache_n</code> setting that controls the sequence-level prefix cacher:</p>
<ul>
<li><strong>CLI</strong>: <code>--prefix-cache-n &lt;N&gt;</code> (default 16). Set to 0 to disable prefix caching.</li>
<li><strong>Python SDK</strong>: <code>prefix_cache_n=&lt;N&gt;</code> (default 16). Set to <code>None</code> or <code>0</code> to disable.</li>
<li><strong>Rust SDK</strong>: <code>.with_prefix_cache_n(Some(N))</code> (default 16). Pass <code>None</code> to disable.</li>
</ul>
<p><strong>Important:</strong> The two prefix caching systems are mutually exclusive:</p>
<ul>
<li><strong>PagedAttention</strong> uses block-level prefix caching (handled by <code>PrefixCacher</code> in <code>BlockEngine</code>)</li>
<li><strong>Non-PagedAttention</strong> uses sequence-level prefix caching (handled by <code>PrefixCacheManagerV2</code>)</li>
</ul>
<p>The <code>prefix_cache_n</code> setting controls both systems, but only one is active depending on whether PagedAttention is enabled. You’ll see one of these log messages at startup indicating which system is active:</p>
<ul>
<li><code>Prefix caching enabled (block-level, PagedAttention).</code></li>
<li><code>Prefix caching enabled (sequence-level, non-paged attention).</code></li>
</ul>
<h3 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h3>
<p>The prefix cache operates at the block level (not token level) for efficiency:</p>
<ol>
<li>
<p><strong>Full blocks only</strong>: Only complete blocks (block_size tokens) are cached. Partial blocks at the end of a sequence are not cached.</p>
</li>
<li>
<p><strong>Hash chain</strong>: The hash for each block depends on all preceding blocks, ensuring the entire prefix matches.</p>
</li>
<li>
<p><strong>Copy-on-Write</strong>: Cached blocks use reference counting. When a cached block needs modification, it’s copied first (CoW).</p>
</li>
<li>
<p><strong>Memory management</strong>: The cache uses LRU eviction when allocating new blocks. Evicted blocks are returned to the free pool.</p>
</li>
</ol>
<h3 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h3>
<ul>
<li>Block size affects cache granularity: larger blocks = fewer cache entries but coarser matching</li>
<li>Cache hit rate improves with more repeated prefixes</li>
<li>Memory overhead is minimal (just hash-to-block mappings)</li>
</ul>
<p><strong>Supported models:</strong></p>
<ul>
<li>Normal models</li>
<li>GGUF models</li>
<li>Vision models</li>
</ul>
<blockquote>
<p>Note: Prefix caching is supported when using PagedAttention. Configure the number of sequences to cache on the device with:</p>
<ul>
<li>CLI: <code>--prefix-cache-n &lt;N&gt;</code> (default 16)</li>
<li>Python SDK: <code>prefix_cache_n=&lt;N&gt;</code> (default 16)</li>
<li>Rust SDK: <code>.with_prefix_cache_n(Some(N))</code> (default 16)</li>
</ul>
</blockquote>
<h2 id="metal-memory-behavior"><a class="header" href="#metal-memory-behavior">Metal Memory Behavior</a></h2>
<p>On Metal (macOS Apple Silicon), the GPU and CPU share the same physical RAM (unified memory). Unlike CUDA GPUs with dedicated VRAM where unused memory would otherwise be wasted, allocating large KV caches on Metal wires physical RAM away from the OS and CPU, which can cause system-wide memory pressure and thrashing.</p>
<p>To avoid this, mistral.rs automatically caps the PagedAttention KV cache on Metal to <code>max_seq_len * max_batch_size</code> tokens — just enough for the configured context length. On CUDA, the full available memory is used for maximum request concurrency (following the vLLM approach).</p>
<p>You can override this behavior on any platform with <code>--pa-memory-mb</code> to set an explicit KV cache budget in megabytes.</p>
<h2 id="flashattention-v2v3--pagedattention-in-mistralrs"><a class="header" href="#flashattention-v2v3--pagedattention-in-mistralrs">FlashAttention V2/V3 + PagedAttention in mistral.rs</a></h2>
<p>If mistral.rs is compiled with <a href="#flashattention-in-mistralrs">FlashAttention</a> and PagedAttention is enabled, then FlashAttention will be used in tandem to accelerate
the prefill phase.</p>
<h2 id="using-the-cli"><a class="header" href="#using-the-cli">Using the CLI</a></h2>
<p>Add the <code>--pa-gpu-mem</code>/<code>--pa-gpu-mem-usage</code> and <code>--pa-blk-size</code> parameters before the model kind selector. The GPU memory is in MBs and the block size means the number of tokens per block. These parameters may be passed on any supported model type.</p>
<p>To enable KV cache quantization, use the <code>--pa-cache-type</code> parameter with either <code>auto</code> (default) or <code>f8e4m3</code>.</p>
<pre><code>mistralrs run --pa-memory-mb 8192 --pa-block-size 32 --isq 4 -m microsoft/Phi-3-mini-128k-instruct
</code></pre>
<pre><code>mistralrs run --pa-memory-fraction 0.95 --pa-block-size 32 --format gguf -t mistralai/Mistral-7B-Instruct-v0.1 -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF -f mistral-7b-instruct-v0.1.Q4_K_M.gguf
</code></pre>
<p>Example with FP8 KV cache quantization:</p>
<pre><code>mistralrs run --paged-attn on --pa-memory-mb 4096 --pa-block-size 32 --pa-cache-type f8e4m3 -m microsoft/Phi-3-mini-128k-instruct
</code></pre>
<h2 id="using-the-rust-sdk"><a class="header" href="#using-the-rust-sdk">Using the Rust SDK</a></h2>
<p>You can find this example <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/paged_attn/main.rs">here</a>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, MemoryGpuConfig, PagedAttentionMetaBuilder, TextMessageRole, TextMessages,
    TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("microsoft/Phi-3.5-mini-instruct")
        .with_isq(IsqType::Q8_0)
        .with_logging()
        .with_paged_attn(
            PagedAttentionMetaBuilder::default()
                .with_block_size(32)
                .with_gpu_memory(MemoryGpuConfig::ContextSize(1024))
                .build()?,
        )
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<p>Example with FP8 KV cache quantization:</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, MemoryGpuConfig, PagedAttentionMetaBuilder, PagedCacheType, 
    TextMessageRole, TextMessages, TextModelBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("microsoft/Phi-3.5-mini-instruct")
        .with_isq(IsqType::Q8_0)
        .with_logging()
        .with_paged_attn(
            PagedAttentionMetaBuilder::default()
                .with_block_size(32)
                .with_gpu_memory(MemoryGpuConfig::ContextSize(1024))
                .with_cache_type(PagedCacheType::F8E4M3)
                .build()?,
        )
        .build()
        .await?;

    // ... rest of the code remains the same
}</code></pre>
<h2 id="using-the-python-sdk-1"><a class="header" href="#using-the-python-sdk-1">Using the Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="mistralai/Mistral-7B-Instruct-v0.1",
        arch=Architecture.Mistral,
    ),
    pa_gpu_mem = 4096,
    pa_blk_size = 32,
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<p>Example with FP8 KV cache quantization:</p>
<pre><code class="language-py">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture, PagedCacheType

runner = Runner(
    which=Which.Plain(
        model_id="mistralai/Mistral-7B-Instruct-v0.1",
        arch=Architecture.Mistral,
    ),
    pa_gpu_mem = 4096,
    pa_blk_size = 32,
    pa_cache_type = PagedCacheType.F8E4M3,
)

# ... rest of the code remains the same
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="speculative-decoding"><a class="header" href="#speculative-decoding">Speculative Decoding</a></h1>
<p>Speculative decoding is an inference acceleration technique that uses a smaller “draft” model to propose tokens, which are then validated in parallel by the larger “target” model. This can significantly speed up generation when the draft model frequently predicts tokens the target model would also choose.</p>
<p>Mistral.rs implements speculative decoding based on the paper: <a href="https://arxiv.org/pdf/2211.17192">Fast Inference from Transformers via Speculative Decoding</a>.</p>
<h2 id="how-it-works-3"><a class="header" href="#how-it-works-3">How It Works</a></h2>
<ol>
<li>The draft model generates <code>gamma</code> candidate tokens autoregressively</li>
<li>The target model evaluates all candidate tokens in a single forward pass</li>
<li>Using rejection sampling, tokens are accepted or rejected:
<ul>
<li>Accept if the target model’s probability &gt;= draft model’s probability</li>
<li>Otherwise, accept with probability <code>p_target(x) / p_draft(x)</code></li>
<li>If rejected, sample from the normalized difference distribution</li>
</ul>
</li>
</ol>
<p>This approach guarantees the same output distribution as running the target model alone, while often achieving significant speedups.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>The key parameter is <code>gamma</code> - the number of draft tokens to generate per speculation step. Higher values can increase throughput when the draft model is accurate, but waste computation when predictions are frequently rejected.</p>
<p><strong>Recommended values:</strong> Start with <code>gamma = 12-32</code> and tune based on your models and workload.</p>
<h2 id="requirements"><a class="header" href="#requirements">Requirements</a></h2>
<ul>
<li><strong>Same tokenizer:</strong> Both target and draft models must share the same tokenizer vocabulary</li>
<li><strong>Same model category:</strong> Both must be the same type (e.g., both text models or both vision models)</li>
<li><strong>KV cache enabled:</strong> Both models must have KV caching enabled (default behavior)</li>
</ul>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<blockquote>
<p>Note: PagedAttention is not currently supported with speculative decoding.</p>
</blockquote>
<blockquote>
<p>Note: Prefix caching is not supported with speculative decoding.</p>
</blockquote>
<blockquote>
<p>Note: Hybrid KV caches are not supported with speculative decoding.</p>
</blockquote>
<h2 id="using-toml-configuration"><a class="header" href="#using-toml-configuration">Using TOML Configuration</a></h2>
<p>The recommended way to configure speculative decoding is via TOML. Create a config file (e.g., <code>speculative.toml</code>):</p>
<pre><code class="language-toml">[model]
model_id = "meta-llama/Llama-3.1-8B-Instruct"

[speculative]
gamma = 12

[speculative.draft_model]
model_id = "meta-llama/Llama-3.2-1B-Instruct"
</code></pre>
<p>Then run with:</p>
<pre><code class="language-bash">mistralrs run --from-toml speculative.toml
</code></pre>
<p>The draft model can use any supported format (Plain, GGUF, etc.) and can have different quantization than the target model.</p>
<h3 id="toml-with-gguf-draft-model"><a class="header" href="#toml-with-gguf-draft-model">TOML with GGUF Draft Model</a></h3>
<pre><code class="language-toml">[model]
model_id = "mistralai/Mistral-7B-Instruct-v0.1"

[speculative]
gamma = 16

[speculative.draft_model]
model_id = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
model_file = "mistral-7b-instruct-v0.1.Q4_K_M.gguf"
tok_model_id = "mistralai/Mistral-7B-Instruct-v0.1"
</code></pre>
<h3 id="toml-with-isq-quantization"><a class="header" href="#toml-with-isq-quantization">TOML with ISQ Quantization</a></h3>
<pre><code class="language-toml">[model]
model_id = "meta-llama/Llama-3.1-8B-Instruct"

[speculative]
gamma = 16

[speculative.draft_model]
model_id = "meta-llama/Llama-3.2-1B-Instruct"
isq = "Q8_0"
</code></pre>
<h2 id="using-the-python-sdk-2"><a class="header" href="#using-the-python-sdk-2">Using the Python SDK</a></h2>
<pre><code class="language-python">from mistralrs import Runner, Which, ChatCompletionRequest, Architecture

runner = Runner(
    which=Which.Plain(
        model_id="mistralai/Mistral-7B-Instruct-v0.1",
        arch=Architecture.Mistral,
    ),
    which_draft=Which.GGUF(
        tok_model_id="mistralai/Mistral-7B-Instruct-v0.1",
        quantized_model_id="TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
        quantized_filename="mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    ),
    speculative_gamma=32,
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "Tell me a story about the Rust type system."}
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h3 id="python-sdk-parameters"><a class="header" href="#python-sdk-parameters">Python SDK Parameters</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>which_draft</code></td><td><code>Which</code></td><td>Draft model specification (Plain, GGUF, etc.)</td></tr>
<tr><td><code>speculative_gamma</code></td><td><code>int</code></td><td>Number of draft tokens per step (default: 32)</td></tr>
</tbody>
</table>
</div>
<h2 id="using-the-rust-sdk-1"><a class="header" href="#using-the-rust-sdk-1">Using the Rust SDK</a></h2>
<p>You can find this example at <code>mistralrs/examples/advanced/speculative/main.rs</code>.</p>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    IsqType, RequestBuilder, SpeculativeConfig, TextMessageRole, TextMessages,
    TextModelBuilder, TextSpeculativeBuilder,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let target = TextModelBuilder::new("meta-llama/Llama-3.1-8B-Instruct")
        .with_logging();
    let draft = TextModelBuilder::new("meta-llama/Llama-3.2-1B-Instruct")
        .with_logging()
        .with_isq(IsqType::Q8_0);
    let spec_cfg = SpeculativeConfig { gamma: 16 };

    let model = TextSpeculativeBuilder::new(target, draft, spec_cfg)?
        .build()
        .await?;

    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::System,
            "You are an AI agent with a specialty in programming.",
        )
        .add_message(
            TextMessageRole::User,
            "Hello! How are you? Please write generic binary search function in Rust.",
        );

    let response = model.send_chat_request(messages).await?;

    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<h2 id="choosing-draft-and-target-models"><a class="header" href="#choosing-draft-and-target-models">Choosing Draft and Target Models</a></h2>
<p>For best performance:</p>
<ol>
<li><strong>Use the same model family</strong> - Draft models from the same family as the target (e.g., Llama 3.2-1B with Llama 3.1-8B) typically have higher acceptance rates</li>
<li><strong>Smaller is better for draft</strong> - The draft model should be significantly smaller than the target for meaningful speedup</li>
<li><strong>Quantize the draft model</strong> - Using ISQ or GGUF quantization on the draft model reduces memory and improves draft generation speed</li>
<li><strong>Tune gamma</strong> - Monitor acceptance rates and adjust gamma accordingly</li>
</ol>
<h3 id="example-model-pairings"><a class="header" href="#example-model-pairings">Example Model Pairings</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Target Model</th><th>Draft Model</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>Llama 3.1-8B</td><td>Llama 3.2-1B</td><td>Same family, good acceptance</td></tr>
<tr><td>Llama 3.1-70B</td><td>Llama 3.1-8B</td><td>Large speedup potential</td></tr>
<tr><td>Mistral-7B</td><td>Mistral-7B (Q4_K_M GGUF)</td><td>Same model, quantized draft</td></tr>
</tbody>
</table>
</div>
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<ul>
<li><strong>Acceptance rate:</strong> Higher acceptance rates lead to better speedups. Monitor your logs for rejection statistics.</li>
<li><strong>Draft model overhead:</strong> If the draft model is too large relative to the target, the overhead may negate speedup benefits.</li>
<li><strong>Batch size:</strong> Speculative decoding is most beneficial for single-request scenarios. For high-throughput batch inference, standard decoding may be more efficient.</li>
<li><strong>Memory usage:</strong> Both models must fit in memory simultaneously. Consider quantizing one or both models.</li>
</ul>
<h2 id="combining-with-other-features"><a class="header" href="#combining-with-other-features">Combining with Other Features</a></h2>
<p>Speculative decoding can be combined with:</p>
<ul>
<li><strong>ISQ quantization</strong> - Quantize target, draft, or both models</li>
<li><strong>X-LoRA adapters</strong> - Use adapters on the target model</li>
<li><strong>Device mapping</strong> - Distribute models across multiple GPUs</li>
</ul>
<p>See <code>examples/python/speculative_xlora.py</code> for an example combining speculative decoding with X-LoRA.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="flashattention-in-mistralrs"><a class="header" href="#flashattention-in-mistralrs">FlashAttention in mistral.rs</a></h1>
<p>Mistral.rs supports FlashAttention V2 and V3 on CUDA devices (V3 is only supported when CC &gt;= 9.0).</p>
<blockquote>
<p>Note: If compiled with FlashAttention and <a href="#pagedattention-in-mistralrs">PagedAttention</a> is enabled, then FlashAttention will be used in tandem to accelerate
the prefill phase.</p>
</blockquote>
<h2 id="gpu-architecture-compatibility"><a class="header" href="#gpu-architecture-compatibility">GPU Architecture Compatibility</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Architecture</th><th>Compute Capability</th><th>Example GPUs</th><th>Feature Flag</th></tr>
</thead>
<tbody>
<tr><td>Ampere</td><td>8.0, 8.6</td><td>RTX 30*, A100, A40</td><td><code>--features flash-attn</code></td></tr>
<tr><td>Ada Lovelace</td><td>8.9</td><td>RTX 40*, L40S</td><td><code>--features flash-attn</code></td></tr>
<tr><td>Hopper</td><td>9.0</td><td>H100, H800</td><td><code>--features flash-attn-v3</code></td></tr>
<tr><td>Blackwell</td><td>10.0, 12.0</td><td>RTX 50*</td><td><code>--features flash-attn</code></td></tr>
</tbody>
</table>
</div>
<blockquote>
<p>Note: FlashAttention V2 and V3 are mutually exclusive
Note: To use FlashAttention in the Python SDK, <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs-pyo3/README.md">compile from source</a>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="multi-head-latent-attention-mla-in-mistralrs"><a class="header" href="#multi-head-latent-attention-mla-in-mistralrs">Multi-head Latent Attention (MLA) in mistral.rs</a></h1>
<p>Multi-head Latent Attention (MLA) is an efficient attention mechanism that reduces KV cache memory usage by compressing key-value states into a low-rank latent space. This technique was introduced in DeepSeek V2 and is also used in DeepSeek V3 and GLM-4.7-Flash models.</p>
<h2 id="how-it-works-4"><a class="header" href="#how-it-works-4">How It Works</a></h2>
<p>MLA compresses the key-value cache by:</p>
<ol>
<li>Projecting KV states into a compact latent representation (<code>kv_lora_rank</code> dimensions)</li>
<li>Storing only the compressed latent vectors and rotary position embeddings in the KV cache</li>
<li>Reconstructing full KV states on-the-fly during attention computation</li>
</ol>
<p>This results in significant memory savings compared to standard multi-head attention, enabling longer context lengths with the same GPU memory.</p>
<h2 id="supported-models-3"><a class="header" href="#supported-models-3">Supported Models</a></h2>
<p>MLA is automatically enabled for the following model architectures when using <a href="#pagedattention-in-mistralrs">PagedAttention</a> on CUDA:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>Architecture</th><th>MLA Dimensions</th></tr>
</thead>
<tbody>
<tr><td><a href="#deepseek-v2-deepseek-aideepseek-v2-lite">DeepSeek V2</a></td><td><code>deepseekv2</code></td><td>kv_lora_rank varies</td></tr>
<tr><td><a href="#deepseek-v3-deepseek-aideepseek-v3-deepseek-aideepseek-r1">DeepSeek V3</a></td><td><code>deepseekv3</code></td><td>kv_lora_rank=512, kpe_head_dim=64</td></tr>
<tr><td><a href="#glm-47-flash-moe-zai-orgglm-47-flash">GLM-4.7-Flash</a></td><td><code>glm4moelite</code></td><td>kv_lora_rank=512, kpe_head_dim=64</td></tr>
</tbody>
</table>
</div>
<h2 id="requirements-1"><a class="header" href="#requirements-1">Requirements</a></h2>
<p>MLA decode optimization requires:</p>
<ul>
<li><strong>CUDA</strong> on Unix-like platforms (Linux, WSL)</li>
<li><strong>PagedAttention</strong> enabled</li>
<li>Compatible model architecture (see table above)</li>
</ul>
<p>When these conditions are met, MLA is automatically used during the decode phase for optimal performance.</p>
<h2 id="performance-benefits"><a class="header" href="#performance-benefits">Performance Benefits</a></h2>
<p>MLA provides two key optimizations:</p>
<ol>
<li>
<p><strong>Reduced KV Cache Memory</strong>: The compressed latent representation uses significantly less memory than full key-value states, allowing for:</p>
<ul>
<li>Longer context lengths</li>
<li>Larger batch sizes</li>
<li>More efficient memory utilization</li>
</ul>
</li>
<li>
<p><strong>Optimized Decode Kernels</strong>: Custom FlashInfer-based MLA kernels accelerate single-token generation by:</p>
<ul>
<li>Operating directly on compressed latent states</li>
<li>Avoiding repeated KV decompression</li>
<li>Leveraging efficient memory access patterns</li>
</ul>
</li>
</ol>
<h2 id="disabling-mla"><a class="header" href="#disabling-mla">Disabling MLA</a></h2>
<p>If you encounter issues or want to compare performance, you can disable MLA by setting the environment variable:</p>
<pre><code class="language-bash">MISTRALRS_NO_MLA=1 mistralrs ...
</code></pre>
<p>When disabled, the model falls back to standard PagedAttention with full KV cache storage.</p>
<h2 id="technical-details-1"><a class="header" href="#technical-details-1">Technical Details</a></h2>
<h3 id="kv-cache-layout"><a class="header" href="#kv-cache-layout">KV Cache Layout</a></h3>
<p>When MLA is enabled, PagedAttention uses a specialized cache layout:</p>
<ul>
<li><strong>Key cache</strong>: Stores compressed latent vectors (<code>kv_lora_rank</code> dimensions) + rotary position embeddings (<code>kpe_head_dim</code> dimensions)</li>
<li><strong>Value cache</strong>: Shares the same block structure for efficient memory management</li>
</ul>
<h3 id="decode-path"><a class="header" href="#decode-path">Decode Path</a></h3>
<p>During single-token generation (decode phase):</p>
<ol>
<li>Query is projected to latent space</li>
<li>Attention is computed directly on compressed KV states using FlashInfer MLA kernels</li>
<li>Output is projected back from latent space</li>
</ol>
<h3 id="prefill-path"><a class="header" href="#prefill-path">Prefill Path</a></h3>
<p>During prompt processing (prefill phase):</p>
<ol>
<li>Full KV states are computed for the current chunk</li>
<li>Compressed latents are stored in the PagedAttention cache</li>
<li>For prefix-cached sequences, latents are retrieved and decompressed as needed</li>
</ol>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="#pagedattention-in-mistralrs">PagedAttention</a> - Required for MLA optimization</li>
<li><a href="#flashattention-in-mistralrs">FlashAttention</a> - Accelerates prefill phase</li>
<li><a href="#deepseek-v2-deepseek-aideepseek-v2-lite">DeepSeek V2</a> - Model documentation</li>
<li><a href="#deepseek-v3-deepseek-aideepseek-v3-deepseek-aideepseek-r1">DeepSeek V3</a> - Model documentation</li>
<li><a href="#glm-47-flash-moe-zai-orgglm-47-flash">GLM-4.7-Flash</a> - Model documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="distributed-inference-in-mistralrs"><a class="header" href="#distributed-inference-in-mistralrs">Distributed inference in mistral.rs</a></h1>
<p>Mistral.rs supports distributed inference with a few strategies</p>
<ul>
<li><a href="#nccl-in-mistralrs">NCCL</a> (recommended for CUDA)</li>
<li><a href="#ring-backend-in-mistralrs">Ring backend</a> (supported on all devices)</li>
</ul>
<p><strong>What backend is best?</strong></p>
<ul>
<li><strong>For CUDA-only system</strong>: NCCL</li>
<li><strong>Anything else</strong>: Ring backend</li>
</ul>
<p>The Ring backend is also <strong>heterogenous</strong>! This means that you can use the Ring backend on any set of multiple devices connected over TCP.
For example, you can connect 2 Metal systems, or 2 Metal and 1 CPU system with the Ring backend!</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nccl-in-mistralrs"><a class="header" href="#nccl-in-mistralrs">NCCL in mistral.rs</a></h1>
<p>Mistral.rs supports distributed inference on CUDA with Tensor Parallelism via NCCL.</p>
<blockquote>
<p>Note: Multi-node support is coming! Distributed inference on Apple hardware is also being investigated.</p>
</blockquote>
<p>Tensor Parallelism (TP) is automatically used to accelerate distributed inference when more than one CUDA GPUs are detected. The tensor parallelism size is always automatically set to the total number of GPUs.</p>
<p>TP splits the model into shards and benefits from fast single-node interconnects like NVLink. Both <code>normal</code> and <code>vision</code> models support tensor parallelism.</p>
<p><strong>Important</strong>: The world size (total number of GPUs) must be a power of 2 (e.g., 1, 2, 4, 8, 16, 32, etc.). This is a requirement for optimal performance and correct operation of the distributed algorithms.</p>
<blockquote>
<p>Note: In mistral.rs, if NCCL is enabled, then automatic device mapping <em>will not</em> be used.</p>
</blockquote>
<p><strong>Important</strong>: To build for NCCL, be sure to add the <code>nccl</code> feature flag (for example: <code>--features nccl,cuda</code>).</p>
<p>See the following environment variables:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Name</th><th>Function</th><th>Usage</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_NO_NCCL=1</code></td><td>Disable TP and NCCL</td><td>If the model does not fit on the available CUDA devices, disabling NCCL will re-enable automatic device mapping</td></tr>
</tbody>
</table>
</div>
<h2 id="single-node-support"><a class="header" href="#single-node-support">Single-Node Support</a></h2>
<p>Set the number of ranks using <code>MISTRALRS_MN_LOCAL_WORLD_SIZE</code>, e.g.,</p>
<pre><code class="language-bash">MISTRALRS_MN_LOCAL_WORLD_SIZE=2 mistralrs serve -p 8000 -m Qwen/Qwen3-30B-A3B-Instruct-2507
</code></pre>
<p>where, if no <code>MISTRALRS_MN_LOCAL_WORLD_SIZE</code> env given, mistral.rs will split the model across all available devices.</p>
<h2 id="multi-node-support"><a class="header" href="#multi-node-support">Multi-node support</a></h2>
<pre><code># Head node:
MISTRALRS_MN_GLOBAL_WORLD_SIZE=32 MISTRALRS_MN_HEAD_NUM_WORKERS=1 MISTRALRS_MN_HEAD_PORT=&lt;PORT&gt; mistralrs run -m ...

# For the worker nodes:
MISTRALRS_MN_GLOBAL_WORLD_SIZE=32 MISTRALRS_MN_WORKER_ID=0 MISTRALRS_WORKER_SERVER_ADDR=&lt;HEAD ADDR&gt;:&lt;PORT&gt; mistralrs run -m ...
MISTRALRS_MN_GLOBAL_WORLD_SIZE=32 MISTRALRS_MN_WORKER_ID=1 MISTRALRS_WORKER_SERVER_ADDR=&lt;HEAD ADDR&gt;:&lt;PORT&gt; mistralrs run -m ...
MISTRALRS_MN_GLOBAL_WORLD_SIZE=32 MISTRALRS_MN_WORKER_ID=2 MISTRALRS_WORKER_SERVER_ADDR=&lt;HEAD ADDR&gt;:&lt;PORT&gt; mistralrs run -m ...
</code></pre>
<p>Multi-node support in mistral.rs divides the nodes into two groups: a “head” node, and multiple “worker” nodes. Head node choice is arbitrary.
For example, if a system has 8 nodes, there will be 1 “head” node, and 7 “worker” nodes.</p>
<p>To enable multi-node, set the <code>MISTRALRS_MN_GLOBAL_WORLD_SIZE=&lt;number&gt;</code> environment variable to the total number of GPUs in all nodes, including “head” and “worker“s. <strong>Note</strong>: This number must be a power of 2.</p>
<p>It is recommended to use server mode with mistral.rs when in multi-node. <strong>Currently, you must send requests to every node!</strong></p>
<p>The following environment variables must be set for each node:</p>
<p><strong>Head node:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Name</th><th>Function</th><th>Usage</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_MN_HEAD_NUM_WORKERS=&lt;number&gt;</code></td><td>The number of worker nodes which will be connected.</td><td>This should be the number of nodes in the system, minus 1 for the head node.</td></tr>
<tr><td><code>MISTRALRS_MN_HEAD_PORT=&lt;PORT&gt;</code></td><td>The port on which to communicate with the worker nodes.</td><td>Worker nodes will connect to this port via TCP sockets</td></tr>
</tbody>
</table>
</div>
<p><strong>Worker node:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Name</th><th>Function</th><th>Usage</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_MN_WORKER_ID=&lt;number&gt;</code></td><td>The 0-indexed worker ID for this worker node.</td><td>If there are 4 nodes (1 head, 3 workers), then the worker ids will be 0, 1, and 2</td></tr>
<tr><td><code>MISTRALRS_MN_WORKER_SERVER_ADDR=&lt;ADDR&gt;:&lt;PORT&gt;</code></td><td>The IP address and port to connect to the server.</td><td>This is used to establish communication with the head node.</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ring-backend-in-mistralrs"><a class="header" href="#ring-backend-in-mistralrs">Ring backend in mistral.rs</a></h1>
<p>Mistral.rs provides a TCP-based ring backend for distributed tensor-parallel inference. This backend is enabled by compiling with the <code>ring</code> feature and implements collective operations over a ring topology using TCP sockets.</p>
<h2 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h2>
<ul>
<li>Build with the <code>ring</code> feature enable, <strong>in addition to any others</strong>:
<pre><code class="language-bash">cargo build --release --features ring
</code></pre>
</li>
<li>Ensure the specified TCP ports are open and reachable between processes.</li>
<li>The <code>world_size</code> must be a power of 2 (2, 4, 8, 16, etc.) for correct operation.</li>
</ul>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<p>Create one JSON configuration file per process with the following fields:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>master_ip</code></td><td>string</td><td>Optional. IP address for master node.</td></tr>
<tr><td><code>master_port</code></td><td>integer</td><td>Optional. Port for master node.</td></tr>
<tr><td><code>port</code></td><td>integer</td><td>Local port to bind for incoming connections from the left neighbor.</td></tr>
<tr><td><code>right_port</code></td><td>integer</td><td>Port on which the right neighbor is listening (used to connect outgoing to the right).</td></tr>
<tr><td><code>right_ip</code></td><td>string</td><td>Optional. IP address of the right neighbor (defaults to <code>0.0.0.0</code>).</td></tr>
<tr><td><code>rank</code></td><td>integer</td><td>Rank of this process in <code>[0..world_size)</code>.</td></tr>
<tr><td><code>world_size</code></td><td>integer</td><td>Total number of processes in the ring. <strong>Must be a power of 2</strong> (e.g., 2, 4, 8, 16, etc.).</td></tr>
</tbody>
</table>
</div>
<p><strong>This address and port should form a ring topology for each of the nodes.</strong> For example, the last node should point to the first node as its right neighbor.</p>
<p>Although all processes participate in collective communication, Rank 0 acts as the master node. For example, interactive mode or the server runs on Rank 0, while other ranks act as background workers.</p>
<p>Example ring topology:</p>
<pre><code class="language-text">+---------+         +---------+
| Rank 0  | -----&gt;  | Rank 1  |
| IP: A   |         | IP: B   |
| Port: X |         | Port: Y |
+----+----+         +----+----+
     ^                   |
     |                   v
+----+----+         +----+----+
| Rank 3  | &lt;-----  | Rank 2  |
| IP: D   |         | IP: C   |
| Port: W |         | Port: Z |
+---------+         +---------+
</code></pre>
<p>Each node connects to its right neighbor by IP and port, and the last node wraps around to the first.</p>
<p>Example for two processes:</p>
<ul>
<li>
<p><a href="https://github.com/EricLBuehler/mistral.rs/blob/master/ring_configs/ring_0.json"><code>ring_0.json</code></a>:</p>
<pre><code class="language-json">{
  "master_ip": "0.0.0.0",
  "master_port": 1234,
  "port": 12345,
  "right_port": 12346,
  "rank": 0,
  "world_size": 2
}
</code></pre>
</li>
<li>
<p><a href="https://github.com/EricLBuehler/mistral.rs/blob/master/ring_configs/ring_1.json"><code>ring_0.json</code></a>:</p>
<pre><code class="language-json">{
  "master_ip": "0.0.0.0",
  "master_port": 1234,
  "port": 12346,
  "right_port": 12345,
  "rank": 1,
  "world_size": 2
}
</code></pre>
</li>
</ul>
<h3 id="multi-machine-example"><a class="header" href="#multi-machine-example">Multi-Machine Example</a></h3>
<p>To run on different machines, update the <code>right_ip</code> field in each config to the actual IP address of the neighbor process. For example, if you have two machines with IPs <code>192.168.1.10</code> and <code>192.168.1.11</code>:</p>
<ul>
<li>
<p><code>ring_0.json</code> on Machine A (192.168.1.10):</p>
<pre><code class="language-json">{
  "port": 12345,
  "right_port": 12346,
  "right_ip": "192.168.1.11",
  "rank": 0,
  "world_size": 2
}
</code></pre>
</li>
<li>
<p><code>ring_1.json</code> on Machine B (192.168.1.11):</p>
<pre><code class="language-json">{
  "port": 12346,
  "right_port": 12345,
  "right_ip": "192.168.1.10",
  "rank": 1,
  "world_size": 2
}
</code></pre>
</li>
</ul>
<p>Make sure that the specified ports are open and that each machine can reach the other via TCP on those ports.</p>
<h2 id="usage-1"><a class="header" href="#usage-1">Usage</a></h2>
<p>Set the <code>RING_CONFIG</code> environment variable to point to the JSON file for each process, then run your application built with the <code>ring</code> feature:</p>
<pre><code class="language-bash"># Process 0 or computer 0
export RING_CONFIG=path/to/ring_0.json
cargo run --release --features ring -- ...

# Process 1 or computer 1
export RING_CONFIG=path/to/ring_1.json
cargo run --release --features ring -- ...
</code></pre>
<p>The ring backend will automatically handle collective communication for tensor-parallel inference.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tool-calling-1"><a class="header" href="#tool-calling-1">Tool calling</a></h1>
<p>Tool calling makes LLMs smarter.</p>
<p>LLMs use tool calling to interact with the outside world. Mistral.rs has OpenAI compatible support for tool calling in all APIs, HTTP, Python, and Rust.</p>
<p>Note that some models, such as Mistral Small/Nemo models, require a chat template to be specified. For example:</p>
<pre><code class="language-bash">mistralrs serve -p 1234 --isq 4 --jinja-explicit chat_templates/mistral_small_tool_call.jinja -m mistralai/Mistral-Small-3.1-24B-Instruct-2503
</code></pre>
<p>OpenAI docs: https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models</p>
<p>We support the following models’ tool calling in OpenAI-compatible and parse native tool calling:</p>
<ul>
<li>Llama 4</li>
<li>Llama 3.1/3.2/3.3</li>
<li>Mistral Small (including 3.1 + multimodal)</li>
<li>Mistral Nemo</li>
<li>Hermes 2 Pro</li>
<li>Hermes 3</li>
<li>DeepSeek V2/V3/R1</li>
<li>Qwen 3</li>
</ul>
<p>All models that support tool calling will respond according to the OpenAI tool calling API.</p>
<h2 id="openai-compatible-http-example"><a class="header" href="#openai-compatible-http-example">OpenAI compatible HTTP example</a></h2>
<p>Please see <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/server/tool_calling.py">our example here</a>.</p>
<blockquote>
<p>OpenAI docs: https://platform.openai.com/docs/api-reference/chat/create?lang=curl</p>
</blockquote>
<h2 id="rust-example-4"><a class="header" href="#rust-example-4">Rust example</a></h2>
<p>Please see <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/tools/main.rs">our example here</a>.</p>
<h2 id="python-example-5"><a class="header" href="#python-example-5">Python example</a></h2>
<p>Please see <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/tool_calling.ipynb">our notebook here</a>.</p>
<h2 id="tool-callbacks"><a class="header" href="#tool-callbacks">Tool callbacks</a></h2>
<p>You can override tool execution using a <strong>tool callback</strong>. The callback receives
the tool name and a dictionary of arguments and must return the tool output as a
string.</p>
<h3 id="python-16"><a class="header" href="#python-16">Python</a></h3>
<pre><code class="language-py">def tool_cb(name: str, args: dict) -&gt; str:
    if name == "local_search":
        return json.dumps(local_search(args.get("query", "")))
    return ""

runner = Runner(
    which=Which.Plain(model_id="YourModel/ID", arch=Architecture.Llama),
    tool_callback=tool_cb,
)
</code></pre>
<p>See <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/custom_search.py">custom_search.py</a> for a full
example. In Rust pass <code>.with_tool_callback(...)</code> to the builder as demonstrated
in <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs/examples/advanced/tool_callback/main.rs">tool_callback/main.rs</a>.</p>
<h2 id="search-callbacks"><a class="header" href="#search-callbacks">Search callbacks</a></h2>
<p>Web search uses a DuckDuckGo-based callback by default. Provide your own search
function with <code>search_callback</code> in Python or <code>.with_search_callback(...)</code> in
Rust. Each callback should return a list of results with <code>title</code>, <code>description</code>,
<code>url</code> and <code>content</code> fields. See <a href="#web-search-tool-in-mistralrs">WEB_SEARCH.md</a> for more details
and examples.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="web-search-tool-in-mistralrs"><a class="header" href="#web-search-tool-in-mistralrs">Web search tool in mistral.rs</a></h1>
<p>mistral.rs is compatible with OpenAI’s <code>web_search_options</code> parameter! Once enabled, this allows web searching for models.</p>
<p>This works with all models that support <a href="#tool-calling-1">tool calling</a>. However, your mileage may vary depending on the specific model. The following models work during testing and are recommended for usage:</p>
<ul>
<li>Hermes 3 3b/8b</li>
<li>Mistral 3 24b</li>
<li>Llama 4 Scout/Maverick</li>
<li>Qwen 3 (⭐ Recommended!)</li>
</ul>
<p>Web search is supported both in streaming and completion responses! This makes it easy to integrate and test out in interactive mode!</p>
<p>Besides tool calling and parsing of web content, we also use an embedding model to select the most relevant search results.</p>
<p>You can use the web search tool in all the APIs: Python, Rust, and server.</p>
<h2 id="selecting-a-search-embedding-model"><a class="header" href="#selecting-a-search-embedding-model">Selecting a search embedding model</a></h2>
<p>Internally, we now use <a href="https://huggingface.co/google/embeddinggemma-300m">google/embeddinggemma-300m</a> to embed documents for ranking. You can pick from the built-in reranker variants (currently just <code>embedding_gemma</code>) in every API:</p>
<ul>
<li>Rust: <code>with_search(SearchEmbeddingModel::EmbeddingGemma300M)</code> in the builder</li>
<li>Python: <code>search_embedding_model="embedding_gemma"</code> in the Runner</li>
<li>Server: <code>--search-embedding-model embedding_gemma</code> flag</li>
</ul>
<h2 id="specifying-a-custom-search-callback"><a class="header" href="#specifying-a-custom-search-callback">Specifying a custom search callback</a></h2>
<p>By default, mistral.rs uses a DuckDuckGo-based search callback. To override this, you can provide your own search function:</p>
<ul>
<li>Rust: use <code>.with_search_callback(...)</code> on the model builder with an <code>Arc&lt;dyn Fn(&amp;SearchFunctionParameters) -&gt; anyhow::Result&lt;Vec&lt;SearchResult&gt;&gt; + Send + Sync&gt;</code>.</li>
<li>Python: pass the <code>search_callback</code> keyword argument to <code>Runner</code>, which should be a function <code>def search_callback(query: str) -&gt; List[Dict[str, str]]</code> returning a list of results with keys <code>"title"</code>, <code>"description"</code>, <code>"url"</code>, and <code>"content"</code>.</li>
</ul>
<p>Example in Python:</p>
<pre><code class="language-py">def search_callback(query: str) -&gt; list[dict[str, str]]:
    # Implement your custom search logic here, returning a list of result dicts
    return [
        {
            "title": "Example Result",
            "description": "An example description",
            "url": "https://example.com",
            "content": "Full text content of the page",
        },
        # more results...
    ]

from mistralrs import Runner, Which, Architecture
runner = Runner(
    which=Which.Plain(model_id="YourModel/ID", arch=Architecture.Mistral),
    enable_search=True,
    search_callback=search_callback,
)
</code></pre>
<h2 id="http-server-19"><a class="header" href="#http-server-19">HTTP server</a></h2>
<p><strong>Be sure to add <code>--enable-search</code>!</strong></p>
<p>Here are some examples using various models. Note that this works for both streaming and completion requests, so interactive mode is featured here!</p>
<pre><code class="language-bash">mistralrs run --enable-search --isq 4 -m Qwen/Qwen3-4B
</code></pre>
<pre><code class="language-bash">mistralrs serve --enable-search -p 1234 --isq 4 --jinja-explicit chat_templates/mistral_small_tool_call.jinja -m mistralai/Mistral-Small-3.1-24B-Instruct-2503
</code></pre>
<pre><code class="language-bash">mistralrs run --enable-search --isq 4 -m NousResearch/Hermes-3-Llama-3.1-8B
</code></pre>
<pre><code class="language-py">from openai import OpenAI

client = OpenAI(api_key="foobar", base_url="http://localhost:1234/v1/")

messages = [
    {
        "role": "user",
        "content": "Can you show me some code using mistral.rs for running Llama 3.2 Vision?",
    }
]

completion = client.chat.completions.create(
    model="default",
    messages=messages,
    tool_choice="auto",
    max_tokens=1024,
    web_search_options={},
)

# print(completion.usage)
print(completion.choices[0].message.content)

if completion.choices[0].message.tool_calls is not None:
    # Should never happen.
    tool_called = completion.choices[0].message.tool_calls[0].function
    print(tool_called)
</code></pre>
<h2 id="python-sdk-13"><a class="header" href="#python-sdk-13">Python SDK</a></h2>
<pre><code class="language-py">from mistralrs import (
    Runner,
    Which,
    ChatCompletionRequest,
    Architecture,
    WebSearchOptions,
)

# Define a custom search callback if desired
def my_search_callback(query: str) -&gt; list[dict[str, str]]:
    # Fetch or compute search results here
    return [
        {
            "title": "Mistral.rs GitHub",
            "description": "Official mistral.rs repository",
            "url": "https://github.com/EricLBuehler/mistral.rs",
            "content": "mistral.rs is a Rust binding for Mistral models...",
        },
    ]

runner = Runner(
    which=Which.Plain(
        model_id="NousResearch/Hermes-3-Llama-3.1-8B",
        arch=Architecture.Llama,
    ),
    enable_search=True,
    search_callback=my_search_callback,
)

res = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[
            {
                "role": "user",
                "content": "Can you show me some code using mistral.rs for running Llama 3.2 Vision?",
            }
        ],
        max_tokens=256,
        presence_penalty=1.0,
        top_p=0.1,
        temperature=0.1,
        web_search_options=WebSearchOptions(
            search_context_size=None, user_location=None
        ),
    )
)
print(res.choices[0].message.content)
print(res.usage)
</code></pre>
<h2 id="rust-sdk-13"><a class="header" href="#rust-sdk-13">Rust SDK</a></h2>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use mistralrs::{
    SearchEmbeddingModel, IsqType, RequestBuilder, TextMessageRole, TextMessages, TextModelBuilder,
    WebSearchOptions,
};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let model = TextModelBuilder::new("NousResearch/Hermes-3-Llama-3.1-8B")
        .with_isq(IsqType::Q4K)
        .with_logging()
        .with_search(SearchEmbeddingModel::default())
        .build()
        .await?;

    let messages = TextMessages::new().add_message(
        TextMessageRole::User,
        "What is the weather forecast for Boston?",
    );
    let messages =
        RequestBuilder::from(messages).with_web_search_options(WebSearchOptions::default());

    let response = model.send_chat_request(messages).await?;

    println!("What is the weather forecast for Boston?\n\n");
    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    dbg!(
        response.usage.avg_prompt_tok_per_sec,
        response.usage.avg_compl_tok_per_sec
    );

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chat-templates-and-tokenizer-customization"><a class="header" href="#chat-templates-and-tokenizer-customization">Chat templates and tokenizer customization</a></h1>
<h2 id="jinja-chat-templates-recommended-method"><a class="header" href="#jinja-chat-templates-recommended-method">JINJA chat templates (recommended method)</a></h2>
<p>Some models do not come with support for tool calling or other features, and as such it might be necessary to specify your own chat template.</p>
<p>We provide some chat templates <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/chat_templates/">here</a>, and it is easy to modify or create others to customize chat template behavior.</p>
<p>To use this, add the <code>jinja-explicit</code> parameter to the various APIs</p>
<pre><code class="language-bash">mistralrs serve -p 1234 --isq 4 --jinja-explicit chat_templates/mistral_small_tool_call.jinja -m mistralai/Mistral-Small-3.1-24B-Instruct-2503
</code></pre>
<h2 id="chat-template-overrides"><a class="header" href="#chat-template-overrides">Chat template overrides</a></h2>
<p>Mistral.rs attempts to automatically load a chat template from the <code>tokenizer_config.json</code> file. This enables high flexibility across instruction-tuned models and ensures accurate chat templating. However, if the <code>chat_template</code> field is missing, then a JINJA chat template should be provided. The JINJA chat template may use <code>messages</code>, <code>add_generation_prompt</code>, <code>bos_token</code>, <code>eos_token</code>, and <code>unk_token</code> as inputs.</p>
<p>We provide some chat templates <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/chat_templates/">here</a>, and it is easy to modify or create others to customize chat template behavior.</p>
<p>For example, to use the <code>chatml</code> template, <code>--chat-template</code> is specified <em>before</em> the model architecture. For example:</p>
<pre><code class="language-bash">mistralrs serve -p 1234 --log output.log --chat-template ./chat_templates/chatml.json -m meta-llama/Llama-3.2-3B-Instruct
</code></pre>
<blockquote>
<p>Note: For GGUF models, the chat template may be loaded directly from the GGUF file by omitting any other chat template sources.</p>
</blockquote>
<h2 id="tokenizer"><a class="header" href="#tokenizer">Tokenizer</a></h2>
<p>Some models do not provide a <code>tokenizer.json</code> file although mistral.rs expects one. To solve this, please run <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/scripts/get_tokenizers_json.py">this</a> script. It will output the <code>tokenizer.json</code> file for your specific model. This may be used by passing the <code>--tokenizer-json</code> flag <em>after</em> the model architecture. For example:</p>
<pre><code class="language-bash">$ python3 scripts/get_tokenizers_json.py
Enter model ID: microsoft/Orca-2-13b
$ mistralrs serve -p 1234 --log output.log -m microsoft/Orca-2-13b --tokenizer-json tokenizer.json
</code></pre>
<p>Putting it all together, to run, for example, an <a href="https://huggingface.co/microsoft/Orca-2-13b">Orca</a> model (which does not come with a <code>tokenizer.json</code> or chat template):</p>
<ol>
<li>Generate the <code>tokenizer.json</code> by running the script at <code>scripts/get_tokenizers_json.py</code>. This will output some files including <code>tokenizer.json</code> in the working directory.</li>
<li>Find and copy the correct chat template from <code>chat-templates</code> to the working directory (eg., <code>cp chat_templates/chatml.json .</code>)</li>
<li>Run <code>mistralrs serve</code>, specifying the tokenizer and chat template: <code>mistralrs serve -p 1234 --log output.txt --chat-template chatml.json -m microsoft/Orca-2-13b -t tokenizer.json</code></li>
</ol>
<blockquote>
<p>Note: For GGUF models, the tokenizer may be loaded directly from the GGUF file by omitting the tokenizer model ID.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sampling-and-penalty-techniques-in-mistralrs"><a class="header" href="#sampling-and-penalty-techniques-in-mistralrs">Sampling and penalty techniques in mistral.rs</a></h1>
<p>mistral.rs supports a comprehensive set of sampling and penalty techniques to control text generation. These can be configured via the HTTP API, Python SDK, or Rust SDK.</p>
<h2 id="temperature"><a class="header" href="#temperature">Temperature</a></h2>
<p>Controls the randomness of token selection. Lower values make output more deterministic, higher values increase creativity and randomness.</p>
<ul>
<li><strong>Range</strong>: 0.0 to 2.0 (typically 0.0 to 1.0)</li>
<li><strong>Default</strong>: Model-dependent, usually around 0.7</li>
<li><strong>Effect</strong>: At 0.0, always selects the most likely token (greedy). At higher values, sampling becomes more diverse.</li>
</ul>
<h2 id="top-k"><a class="header" href="#top-k">Top K</a></h2>
<p>Limits token selection to the K most likely tokens.</p>
<ul>
<li><strong>Range</strong>: 1 to vocabulary size</li>
<li><strong>Effect</strong>: Lower values restrict choices to only the most probable tokens, reducing randomness.</li>
</ul>
<h2 id="top-p-nucleus-sampling"><a class="header" href="#top-p-nucleus-sampling">Top P (Nucleus Sampling)</a></h2>
<p>Limits token selection to the smallest set of tokens whose cumulative probability exceeds P.</p>
<ul>
<li><strong>Range</strong>: 0.0 to 1.0</li>
<li><strong>Effect</strong>: At 0.1, only tokens comprising the top 10% probability mass are considered. More adaptive than Top K as it adjusts based on the probability distribution.</li>
</ul>
<h2 id="min-p"><a class="header" href="#min-p">Min P</a></h2>
<p>Filters out tokens with probability less than <code>min_p * max_probability</code>.</p>
<ul>
<li><strong>Range</strong>: 0.0 to 1.0</li>
<li><strong>Effect</strong>: Removes low-probability tokens relative to the most likely token. Useful for preventing unlikely tokens from being selected.</li>
</ul>
<h2 id="stop-sequences"><a class="header" href="#stop-sequences">Stop Sequences</a></h2>
<p>Strings that, when generated, cause generation to stop immediately.</p>
<ul>
<li><strong>Type</strong>: Array of strings</li>
<li><strong>Effect</strong>: Generation terminates as soon as any stop sequence is produced. Useful for controlling output boundaries.</li>
</ul>
<h2 id="repetition-penalty"><a class="header" href="#repetition-penalty">Repetition Penalty</a></h2>
<p>Applies a multiplicative penalty to tokens that have already appeared in the context.</p>
<ul>
<li><strong>Range</strong>: Typically 1.0 to 2.0</li>
<li><strong>Effect</strong>: Values &gt; 1.0 make repeated tokens less likely. This is distinct from frequency and presence penalties.</li>
</ul>
<h2 id="frequency-penalty"><a class="header" href="#frequency-penalty">Frequency Penalty</a></h2>
<p>Penalizes tokens based on how many times they’ve appeared in the generated text so far.</p>
<ul>
<li><strong>Range</strong>: -2.0 to 2.0</li>
<li><strong>Effect</strong>: Positive values reduce repetition proportionally to token frequency. Negative values encourage repetition.</li>
</ul>
<h2 id="presence-penalty"><a class="header" href="#presence-penalty">Presence Penalty</a></h2>
<p>Penalizes tokens that have appeared at least once in the generated text.</p>
<ul>
<li><strong>Range</strong>: -2.0 to 2.0</li>
<li><strong>Effect</strong>: Positive values discourage any repetition (binary penalty). Negative values encourage reusing tokens.</li>
</ul>
<h2 id="dry-dont-repeat-yourself-penalty"><a class="header" href="#dry-dont-repeat-yourself-penalty">DRY (Don’t Repeat Yourself) Penalty</a></h2>
<p>An advanced anti-repetition technique that detects and penalizes repeated sequences of tokens, not just individual tokens. See the <a href="https://github.com/oobabooga/text-generation-webui/pull/5677">original implementation</a> for details.</p>
<h3 id="dry-parameters"><a class="header" href="#dry-parameters">DRY Parameters</a></h3>
<ul>
<li><strong><code>dry_multiplier</code></strong>: Controls the strength of the penalty. Higher values more strongly discourage repetition.</li>
<li><strong><code>dry_base</code></strong>: Base value for the exponential penalty calculation.</li>
<li><strong><code>dry_allowed_length</code></strong>: Minimum sequence length before the penalty applies. Sequences shorter than this are not penalized.</li>
<li><strong><code>dry_sequence_breakers</code></strong>: Array of tokens (like newlines, punctuation) that reset the sequence tracking. When these tokens appear, the DRY penalty starts fresh.</li>
</ul>
<h3 id="example-dry-configuration-http-api"><a class="header" href="#example-dry-configuration-http-api">Example DRY Configuration (HTTP API)</a></h3>
<pre><code class="language-json">{
  "dry_multiplier": 0.8,
  "dry_base": 1.75,
  "dry_allowed_length": 2,
  "dry_sequence_breakers": ["\n", ".", "!", "?", ";"]
}
</code></pre>
<h2 id="api-usage"><a class="header" href="#api-usage">API Usage</a></h2>
<p>All sampling parameters can be set in API requests:</p>
<h3 id="http-api-10"><a class="header" href="#http-api-10">HTTP API</a></h3>
<pre><code class="language-json">{
  "model": "default",
  "messages": [...],
  "temperature": 0.7,
  "top_p": 0.9,
  "top_k": 40,
  "min_p": 0.05,
  "repetition_penalty": 1.1,
  "frequency_penalty": 0.5,
  "presence_penalty": 0.5,
  "stop": ["END", "\n\n"],
  "dry_multiplier": 0.8,
  "dry_base": 1.75,
  "dry_allowed_length": 2,
  "dry_sequence_breakers": ["\n"]
}
</code></pre>
<h3 id="python-sdk-14"><a class="header" href="#python-sdk-14">Python SDK</a></h3>
<pre><code class="language-python">response = runner.send_chat_completion_request(
    ChatCompletionRequest(
        model="default",
        messages=[...],
        temperature=0.7,
        top_p=0.9,
        top_k=40,
        min_p=0.05,
        repetition_penalty=1.1,
        frequency_penalty=0.5,
        presence_penalty=0.5,
        stop_seqs=["END", "\n\n"],
        dry_multiplier=0.8,
        dry_base=1.75,
        dry_allowed_length=2,
        dry_sequence_breakers=["\n"],
    )
)
</code></pre>
<p>Please suggest more sampling techniques by <a href="https://github.com/EricLBuehler/mistral.rs/issues">raising an issue</a>!</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="structured-model-loading-with-toml-files"><a class="header" href="#structured-model-loading-with-toml-files">Structured model loading with .toml files</a></h1>
<p>Mistral.rs supports loading models from a .toml file, and the fields are the same as for the CLI. Please find some example toml selectors <a href="https://github.com/EricLBuehler/mistral.rs/tree/master/toml-selectors/">here</a>.</p>
<p>There are a few cases which add functionality that cannot be found in the CLI.</p>
<h2 id="speculative-decoding-1"><a class="header" href="#speculative-decoding-1">Speculative decoding</a></h2>
<h3 id="what-to-specify"><a class="header" href="#what-to-specify">What to specify</a></h3>
<p><strong>Under <code>[speculative]</code></strong></p>
<ul>
<li>Specify the <code>gamma</code> parameter</li>
</ul>
<p><strong>Under <code>[speculative.draft_model]</code></strong></p>
<ul>
<li>Choose a draft model, just like under <code>[model]</code> (only requirement is that they have the same tokenizer)</li>
</ul>
<pre><code class="language-toml">[model]
model_id = "mistralai/Mistral-7B-Instruct-v0.1"
arch = "mistral"

[speculative]
gamma = 32

[speculative.draft_model]
tok_model_id = "mistralai/Mistral-7B-Instruct-v0.1"
quantized_model_id = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
quantized_filename = "mistral-7b-instruct-v0.1.Q2_K.gguf"
</code></pre>
<pre><code class="language-bash">mistralrs from-config -f toml-selectors/speculative-gguf.toml
</code></pre>
<h2 id="anymoe"><a class="header" href="#anymoe">AnyMoE</a></h2>
<h3 id="what-to-specify-1"><a class="header" href="#what-to-specify-1">What to specify</a></h3>
<p><strong>Under <code>[anymoe]</code>, required unless specified</strong></p>
<ul>
<li>Specify the dataset</li>
<li>Find and specify the prefix/mlp values
<ul>
<li>Go to <code>https://huggingface.co/&lt;MODEL ID&gt;/tree/main?show_file_info=model.safetensors.index.json</code></li>
<li>Look for the mlp layers: For example <code>model.layers.27.mlp.down_proj.weight</code> means that the prefix is <code>model.layers</code> and the mlp is <code>mlp</code>.</li>
</ul>
</li>
<li>Specify the expert or LoRA adapter model IDs</li>
<li>(Optional) Specify layers to apply AnyMoE to.</li>
</ul>
<p><strong>Under <code>[anymoe.config]</code></strong></p>
<ul>
<li>Hidden size, typically found at <code>https://huggingface.co/&lt;BASE MODEL ID&gt;/blob/main/config.json</code></li>
</ul>
<p><strong>(For LoRA experts) Under <code>[anymoe.config.expert_type.lora_adapter]</code></strong></p>
<ul>
<li>Rank</li>
<li>Alpha</li>
<li>Target modules</li>
</ul>
<pre><code class="language-bash">mistralrs from-config -f toml-selectors/anymoe.toml
</code></pre>
<h3 id="with-fine-tuned-experts"><a class="header" href="#with-fine-tuned-experts">With fine-tuned experts</a></h3>
<pre><code class="language-toml">[model]
model_id = "mistralai/Mistral-7B-Instruct-v0.1"
arch = "mistral"

[anymoe]
dataset_json = "test.csv"
prefix = "model.layers"
mlp = "mlp"
model_ids = ["HuggingFaceH4/zephyr-7b-beta"]
layers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

[anymoe.config]
hidden_size = 4096
expert_type = "fine_tuned"
</code></pre>
<h3 id="with-lora-adapter-experts"><a class="header" href="#with-lora-adapter-experts">With LoRA adapter experts</a></h3>
<pre><code class="language-toml">[model]
model_id = "HuggingFaceH4/zephyr-7b-beta"
arch = "mistral"

[anymoe]
dataset_json = "test.csv"
prefix = "model.layers"
mlp = "mlp"
model_ids = ["EricB/example_adapter"]
layers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

[anymoe.config]
hidden_size = 4096

[anymoe.config.expert_type.lora_adapter]
rank = 16
alpha = 16
target_modules = ["gate_proj"]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="multi-model-support-1"><a class="header" href="#multi-model-support-1">Multi-Model Support</a></h1>
<p>The <code>mistralrs</code> CLI supports loading and serving multiple models simultaneously, allowing you to switch between different models in the same server instance.</p>
<ul>
<li>Each model runs in its own engine thread</li>
<li>Models can have different configurations (quantization, device layers, etc.)</li>
<li>Memory usage scales with the number of loaded models</li>
<li>All models share the same server configuration (port, logging, etc.)</li>
<li>Interactive mode uses the default model or the first model if no default is set</li>
<li>You can unload all models (including the last one) - they will auto-reload when accessed</li>
</ul>
<h2 id="usage-2"><a class="header" href="#usage-2">Usage</a></h2>
<h3 id="single-model-mode-default"><a class="header" href="#single-model-mode-default">Single-Model Mode (Default)</a></h3>
<pre><code class="language-bash"># Traditional usage - loads one model
mistralrs serve -p 1234 -m meta-llama/Llama-3.2-3B-Instruct
</code></pre>
<h3 id="multi-model-mode"><a class="header" href="#multi-model-mode">Multi-Model Mode</a></h3>
<pre><code class="language-bash"># Load multiple models from configuration file
mistralrs from-config --file config.toml
</code></pre>
<h2 id="configuration-file-format"><a class="header" href="#configuration-file-format">Configuration File Format</a></h2>
<p>Create a JSON file with model configurations as object keys:</p>
<pre><code class="language-json">{
  "llama3-3b": {
    "alias": "llama3-3b",
    "Plain": {
      "model_id": "meta-llama/Llama-3.2-3B-Instruct"
    }
  },
  "qwen3-4b": {
    "alias": "qwen3-4b",
    "Plain": {
      "model_id": "Qwen/Qwen3-4B"
    },
    "in_situ_quant": "Q4K"
  }
}
</code></pre>
<h3 id="configuration-structure"><a class="header" href="#configuration-structure">Configuration Structure</a></h3>
<ul>
<li><strong>Object keys</strong> (e.g., <code>"llama3-3b"</code>, <code>"qwen3-4b"</code>): Organizational labels (for human readability)</li>
<li><strong>API identifiers</strong>: By default the pipeline name (usually the <code>model_id</code> inside the model spec). You can override this with <code>alias</code>.</li>
<li><strong>Model specification</strong>: The model type and configuration (same format as CLI subcommands)</li>
<li><strong>Optional fields</strong>:
<ul>
<li><code>alias</code>: Custom model ID (nickname) used in API requests</li>
<li><code>chat_template</code>: Custom chat template</li>
<li><code>jinja_explicit</code>: JINJA template file</li>
<li><code>num_device_layers</code>: Device layer configuration</li>
<li><code>in_situ_quant</code>: In-situ quantization setting</li>
</ul>
</li>
</ul>
<p><strong>How API identifiers work:</strong></p>
<ul>
<li>✅ Object keys are <strong>organizational only</strong> (for config readability)</li>
<li>✅ If <code>alias</code> is set, it becomes the API model ID</li>
<li>✅ Otherwise, the pipeline name (usually the <code>model_id</code> field) is used</li>
<li>✅ The canonical pipeline name remains accepted as an alias for compatibility</li>
</ul>
<h2 id="api-usage-1"><a class="header" href="#api-usage-1">API Usage</a></h2>
<h3 id="selecting-models-in-requests"><a class="header" href="#selecting-models-in-requests">Selecting Models in Requests</a></h3>
<p>Use the <code>model</code> field in your requests to specify which model to use:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3-3b",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
</code></pre>
<h4 id="default-model-behavior"><a class="header" href="#default-model-behavior">Default Model Behavior</a></h4>
<ul>
<li><strong>Explicit model</strong>: Use the alias if configured (e.g., <code>"llama3-3b"</code>), otherwise the full pipeline name (e.g., <code>"meta-llama/Llama-3.2-3B-Instruct"</code>)</li>
<li><strong>Default model</strong>: Use <code>"default"</code> to explicitly request the default model</li>
<li><strong>Auto-fallback</strong>: If the <code>model</code> field is omitted entirely, the default model will be used</li>
</ul>
<pre><code class="language-bash"># Use default model explicitly
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
</code></pre>
<p>The default model is either:</p>
<ol>
<li>The model specified with <code>--default-model-id</code> when starting the server</li>
<li>The first model loaded (if no default is explicitly set)</li>
</ol>
<h3 id="list-available-models"><a class="header" href="#list-available-models">List Available Models</a></h3>
<pre><code class="language-bash">curl http://localhost:1234/v1/models
</code></pre>
<p>Returns:</p>
<pre><code class="language-json">{
  "object": "list",
  "data": [
    {
      "id": "default",
      "object": "model",
      "created": 1234567890,
      "owned_by": "local"
    },
    {
      "id": "llama3-3b",
      "object": "model",
      "created": 1234567890,
      "owned_by": "local"
    },
    {
      "id": "qwen3-4b", 
      "object": "model",
      "created": 1234567890,
      "owned_by": "local"
    }
  ]
}
</code></pre>
<p><strong>Note</strong>: The <code>"default"</code> model is always listed first and represents the server’s default model. If aliases are configured, they will appear in the list while the canonical pipeline names remain accepted.</p>
<h2 id="cli-arguments"><a class="header" href="#cli-arguments">CLI Arguments</a></h2>
<p>Use the <code>multi-model</code> subcommand with these options:</p>
<ul>
<li><code>--config &lt;PATH&gt;</code> (required): Path to the JSON configuration file</li>
<li><code>--default-model-id &lt;ID&gt;</code> (optional): Default model ID for requests that don’t specify a model (alias or pipeline name)</li>
</ul>
<p><strong>New syntax:</strong></p>
<pre><code class="language-bash">mistralrs from-config --file &lt;CONFIG&gt;
</code></pre>
<h2 id="examples-3"><a class="header" href="#examples-3">Examples</a></h2>
<h3 id="example-1-text-models"><a class="header" href="#example-1-text-models">Example 1: Text Models</a></h3>
<pre><code class="language-json">{
  "llama3-3b": {
    "Plain": {
      "model_id": "meta-llama/Llama-3.2-3B-Instruct"
    }
  },
  "qwen3-4b": {
    "Plain": {
      "model_id": "Qwen/Qwen3-4B"
    },
    "in_situ_quant": "Q4K"
  }
}
</code></pre>
<h3 id="example-2-mixed-model-types"><a class="header" href="#example-2-mixed-model-types">Example 2: Mixed Model Types</a></h3>
<pre><code class="language-json">{
  "text-model": {
    "Plain": {
      "model_id": "meta-llama/Llama-3.2-3B-Instruct"
    }
  },
  "vision-model": {
    "VisionPlain": {
      "model_id": "google/gemma-3-4b-it"
    }
  }
}
</code></pre>
<h3 id="example-3-gguf-models"><a class="header" href="#example-3-gguf-models">Example 3: GGUF Models</a></h3>
<pre><code class="language-json">{
  "llama-gguf": {
    "GGUF": {
      "tok_model_id": "meta-llama/Llama-3.2-3B-Instruct",
      "quantized_model_id": "bartowski/Llama-3.2-3B-Instruct-GGUF",
      "quantized_filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf"
    }
  }
}
</code></pre>
<h2 id="model-unloading-and-reloading-1"><a class="header" href="#model-unloading-and-reloading-1">Model Unloading and Reloading</a></h2>
<p>You can dynamically unload models to free memory and reload them on demand. This is useful for managing GPU memory when working with multiple large models.</p>
<h3 id="unload-a-model"><a class="header" href="#unload-a-model">Unload a Model</a></h3>
<p>Unload a model from memory while preserving its configuration for later reload:</p>
<pre><code class="language-bash">curl -X POST http://localhost:1234/v1/models/unload \
  -H "Content-Type: application/json" \
  -d '{"model_id": "meta-llama/Llama-3.2-3B-Instruct"}'
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "unloaded"
}
</code></pre>
<h3 id="reload-a-model"><a class="header" href="#reload-a-model">Reload a Model</a></h3>
<p>Manually reload a previously unloaded model:</p>
<pre><code class="language-bash">curl -X POST http://localhost:1234/v1/models/reload \
  -H "Content-Type: application/json" \
  -d '{"model_id": "meta-llama/Llama-3.2-3B-Instruct"}'
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "loaded"
}
</code></pre>
<h3 id="check-model-status"><a class="header" href="#check-model-status">Check Model Status</a></h3>
<p>Get the current status of a specific model:</p>
<pre><code class="language-bash">curl -X POST http://localhost:1234/v1/models/status \
  -H "Content-Type: application/json" \
  -d '{"model_id": "meta-llama/Llama-3.2-3B-Instruct"}'
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "loaded"
}
</code></pre>
<p>Possible status values:</p>
<ul>
<li><code>loaded</code>: Model is loaded and ready</li>
<li><code>unloaded</code>: Model is unloaded but can be reloaded</li>
<li><code>reloading</code>: Model is currently being reloaded</li>
<li><code>not_found</code>: Model ID not recognized</li>
<li><code>no_loader_config</code>: Model cannot be reloaded (missing loader configuration)</li>
<li><code>internal_error</code>: An internal error occurred</li>
</ul>
<h3 id="auto-reload"><a class="header" href="#auto-reload">Auto-Reload</a></h3>
<p>When a request is sent to an unloaded model, it will automatically reload before processing the request. This enables a “lazy loading” pattern where models are only loaded when needed.</p>
<h3 id="list-models-with-status"><a class="header" href="#list-models-with-status">List Models with Status</a></h3>
<p>The <code>/v1/models</code> endpoint now includes status information:</p>
<pre><code class="language-bash">curl http://localhost:1234/v1/models
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "object": "list",
  "data": [
    {
      "id": "default",
      "object": "model",
      "created": 1234567890,
      "owned_by": "local"
    },
    {
      "id": "meta-llama/Llama-3.2-3B-Instruct",
      "object": "model",
      "created": 1234567890,
      "owned_by": "local",
      "status": "loaded"
    },
    {
      "id": "Qwen/Qwen3-4B",
      "object": "model",
      "created": 1234567890,
      "owned_by": "local",
      "status": "unloaded"
    }
  ]
}
</code></pre>
<h2 id="rust-sdk-usage"><a class="header" href="#rust-sdk-usage">Rust SDK Usage</a></h2>
<p>The <code>mistralrs</code> crate provides <code>MultiModelBuilder</code> for loading multiple models and <code>Model</code> methods for multi-model management.</p>
<h3 id="loading-multiple-models"><a class="header" href="#loading-multiple-models">Loading Multiple Models</a></h3>
<p>By default, model IDs are the pipeline names (usually the HuggingFace model path, e.g., <code>"google/gemma-3-4b-it"</code>). You can provide custom aliases with <code>add_model_with_alias</code> for shorter IDs.</p>
<pre class="playground"><code class="language-rust">use mistralrs::{IsqType, MultiModelBuilder, TextModelBuilder, VisionModelBuilder, TextMessages, TextMessageRole};

#[tokio::main]
async fn main() -&gt; anyhow::Result&lt;()&gt; {
    // Build a multi-model instance with a vision model and a text model
    // Use aliases for shorter model IDs in requests
    let model = MultiModelBuilder::new()
        .add_model_with_alias(
            "gemma-vision",
            VisionModelBuilder::new("google/gemma-3-4b-it")  // Vision model
                .with_isq(IsqType::Q4K)
                .with_logging(),
        )
        .add_model_with_alias(
            "qwen-text",
            TextModelBuilder::new("Qwen/Qwen3-4B")  // Text model
                .with_isq(IsqType::Q4K),
        )
        .with_default_model("gemma-vision")
        .build()
        .await?;

    // Send request to default model
    let messages = TextMessages::new()
        .add_message(TextMessageRole::User, "Hello!");
    let response = model.send_chat_request(messages).await?;

    // Send request to specific model using its alias
    let messages = TextMessages::new()
        .add_message(TextMessageRole::User, "Hello from Qwen!");
    let response = model.send_chat_request_with_model(messages, Some("qwen-text")).await?;

    Ok(())
}</code></pre>
<h3 id="model-management-methods"><a class="header" href="#model-management-methods">Model Management Methods</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// List all models (returns aliases if configured, otherwise pipeline names)
let models = model.list_models()?;

// Get/set default model
let default = model.get_default_model_id()?;
model.set_default_model_id("qwen-text")?;

// List models with status
let status = model.list_models_with_status()?;
// Returns Vec&lt;(String, ModelStatus)&gt; where ModelStatus is Loaded, Unloaded, or Reloading

// Check if a model is loaded
let is_loaded = model.is_model_loaded("gemma-vision")?;

// Unload a model to free memory
model.unload_model("gemma-vision")?;

// Reload when needed
model.reload_model("gemma-vision").await?;
<span class="boring">}</span></code></pre>
<h3 id="available-_with_model-methods"><a class="header" href="#available-_with_model-methods">Available <code>_with_model</code> Methods</a></h3>
<p>All request methods have <code>_with_model</code> variants that accept an optional model ID:</p>
<ul>
<li><code>send_chat_request_with_model(request, model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>stream_chat_request_with_model(request, model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>generate_image_with_model(..., model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>generate_speech_with_model(prompt, model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>generate_embeddings_with_model(request, model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>tokenize_with_model(..., model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>detokenize_with_model(..., model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>config_with_model(model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>max_sequence_length_with_model(model_id: Option&lt;&amp;str&gt;)</code></li>
<li><code>re_isq_model_with_model(isq_type, model_id: Option&lt;&amp;str&gt;)</code></li>
</ul>
<p>When <code>model_id</code> is <code>None</code>, the default model is used. If aliases are configured, you can pass either the alias or the canonical pipeline name.</p>
<h2 id="python-sdk-usage"><a class="header" href="#python-sdk-usage">Python SDK Usage</a></h2>
<p>The Python <code>Runner</code> class supports multi-model operations directly.</p>
<h3 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h3>
<pre><code class="language-python">from mistralrs import Runner, Which, ChatCompletionRequest, VisionArchitecture, Architecture

# Create a runner with a vision model (Gemma 3 4B)
runner = Runner(
    which=Which.VisionPlain(
        model_id="google/gemma-3-4b-it",
        arch=VisionArchitecture.Gemma3,
    ),
    in_situ_quant="Q4K",
)

# Or create a runner with a text model (Qwen3 4B)
# runner = Runner(
#     which=Which.Plain(
#         model_id="Qwen/Qwen3-4B",
#         arch=Architecture.Qwen3,
#     ),
#     in_situ_quant="Q4K",
# )

# List models
models = runner.list_models()
print(f"Available models: {models}")

# Get/set default model
default = runner.get_default_model_id()
runner.set_default_model_id("google/gemma-3-4b-it")

# Send request with specific model_id
request = ChatCompletionRequest(
    messages=[{"role": "user", "content": "Hello!"}]
)
response = runner.send_chat_completion_request(request, model_id=models[0])
</code></pre>
<p>If aliases are configured (for example via the server config or Rust <code>MultiModelBuilder</code>), <code>list_models()</code> will return those aliases and you can pass them in <code>model_id</code>. The canonical pipeline names remain accepted.</p>
<h3 id="model-management"><a class="header" href="#model-management">Model Management</a></h3>
<pre><code class="language-python"># List models with their status
status = runner.list_models_with_status()
# Returns list of (model_id, status) tuples

# Check if a model is loaded
is_loaded = runner.is_model_loaded("google/gemma-3-4b-it")

# Unload a model to free memory
runner.unload_model("google/gemma-3-4b-it")

# Reload when needed
runner.reload_model("google/gemma-3-4b-it")
</code></pre>
<h3 id="request-methods-with-model_id-1"><a class="header" href="#request-methods-with-model_id-1">Request Methods with model_id</a></h3>
<p>All request methods accept an optional <code>model_id</code> parameter:</p>
<pre><code class="language-python"># Chat completion
response = runner.send_chat_completion_request(request, model_id="model-id")

# Completion
response = runner.send_completion_request(request, model_id="model-id")

# Embeddings
embeddings = runner.send_embedding_request(request, model_id="model-id")

# Image generation
image = runner.generate_image(prompt, response_format, model_id="model-id")

# Speech generation
audio = runner.generate_audio(prompt, model_id="model-id")

# Tokenization
tokens = runner.tokenize_text(text, add_special_tokens=True, model_id="model-id")
text = runner.detokenize_text(tokens, skip_special_tokens=True, model_id="model-id")
</code></pre>
<p>When <code>model_id</code> is <code>None</code> or omitted, the default model is used.</p>
<h2 id="migration-guide"><a class="header" href="#migration-guide">Migration Guide</a></h2>
<h3 id="from-multimodel-rust"><a class="header" href="#from-multimodel-rust">From <code>MultiModel</code> (Rust)</a></h3>
<p>The <code>MultiModel</code> struct has been removed. Use <code>Model</code> directly with <code>MultiModelBuilder</code>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Old (deprecated)
let multi = MultiModel::new(...);
multi.send_chat_request_to_model(request, "model-id").await?;

// New - model IDs are pipeline names by default (aliases optional)
let model = MultiModelBuilder::new()
    .add_model(VisionModelBuilder::new("google/gemma-3-4b-it"))
    .add_model(TextModelBuilder::new("Qwen/Qwen3-4B"))
    .build()
    .await?;
model.send_chat_request_with_model(request, Some("Qwen/Qwen3-4B")).await?;
<span class="boring">}</span></code></pre>
<h3 id="from-multimodelrunner-python"><a class="header" href="#from-multimodelrunner-python">From <code>MultiModelRunner</code> (Python)</a></h3>
<p>The <code>MultiModelRunner</code> class has been removed. Use <code>Runner</code> directly:</p>
<pre><code class="language-python"># Old (deprecated)
multi_runner = MultiModelRunner(runner)
multi_runner.send_chat_completion_request_to_model(request, "model-id")

# New - model IDs are the registered IDs (aliases if configured)
runner = Runner(which=Which.Plain(model_id="google/gemma-3-4b-it", ...))
runner.send_chat_completion_request(request, model_id="google/gemma-3-4b-it")
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mcp-model-context-protocol-client"><a class="header" href="#mcp-model-context-protocol-client">MCP (Model Context Protocol) Client</a></h1>
<p>mistral.rs includes a built-in MCP client that allows models to connect to external tools and services through the Model Context Protocol. This enables automatic tool discovery and usage from any MCP-compatible server.</p>
<h2 id="quick-start-2"><a class="header" href="#quick-start-2">Quick Start</a></h2>
<p>Examples below show HTTP (Hugging Face), Process (filesystem), and WebSocket transports. Replace <code>hf_xxx</code> with your actual Hugging Face token for HTTP examples.</p>
<h3 id="rust-sdk-14"><a class="header" href="#rust-sdk-14">Rust SDK</a></h3>
<pre class="playground"><code class="language-rust">use mistralrs::{
    TextModelBuilder, McpClientConfig, McpServerConfig, McpServerSource,
    TextMessages, TextMessageRole,
};

#[tokio::main]
async fn main() -&gt; anyhow::Result&lt;()&gt; {
    // Process example (filesystem server - recommended for getting started)
    let mcp_config = McpClientConfig {
        servers: vec![McpServerConfig {
            name: "Filesystem Tools".to_string(),
            source: McpServerSource::Process {
                command: "npx".to_string(),
                args: vec!["@modelcontextprotocol/server-filesystem".to_string(), ".".to_string()],
                work_dir: None,
                env: None,
            },
            ..Default::default()
        }],
        auto_register_tools: true,
        ..Default::default()
    };

    // Alternative HTTP example (Hugging Face MCP server)
    let _mcp_config_http = McpClientConfig {
        servers: vec![McpServerConfig {
            id: "hf_server".to_string(),
            name: "Hugging Face MCP".to_string(),
            source: McpServerSource::Http {
                url: "https://hf.co/mcp".to_string(),
                timeout_secs: Some(30),
                headers: None,
            },
            enabled: false, // Disabled by default
            tool_prefix: Some("hf".to_string()),
            resources: None,
            bearer_token: Some("hf_xxx".to_string()), // Your HF token
        }],
        auto_register_tools: true,
        tool_timeout_secs: Some(30),
        max_concurrent_calls: Some(5),
    };

    // Alternative WebSocket example
    let _mcp_config_websocket = McpClientConfig {
        servers: vec![McpServerConfig {
            name: "WebSocket Example".to_string(),
            source: McpServerSource::WebSocket {
                url: "wss://api.example.com/mcp".to_string(),
                timeout_secs: Some(30),
                headers: None,
            },
            enabled: false, // Disabled by default
            ..Default::default()
        }],
        auto_register_tools: true,
        ..Default::default()
    };

    // Build model with MCP support
    let model = TextModelBuilder::new("Qwen/Qwen3-4B")
        .with_mcp_client(mcp_config)
        .build()
        .await?;

    // Use the model - tools are automatically available
    let messages = TextMessages::new()
        .add_message(
            TextMessageRole::User,
            "List the files in the current directory and create a test.txt file"
        );

    let response = model.send_chat_request(messages).await?;
    println!("{}", response.choices[0].message.content.as_ref().unwrap());
    
    Ok(())
}</code></pre>
<h3 id="python-sdk-15"><a class="header" href="#python-sdk-15">Python SDK</a></h3>
<pre><code class="language-python">import mistralrs

# Process example (filesystem server - recommended for getting started)
filesystem_server = mistralrs.McpServerConfigPy(
    name="Filesystem Tools",
    source=mistralrs.McpServerSourcePy.Process(
        command="npx",
        args=["@modelcontextprotocol/server-filesystem", "."],
        work_dir=None,
        env=None
    )
)

# Alternative HTTP example (Hugging Face MCP server)
hf_server = mistralrs.McpServerConfigPy(
    id="hf_server",
    name="Hugging Face MCP",
    source=mistralrs.McpServerSourcePy.Http(
        url="https://hf.co/mcp",
        timeout_secs=30,
        headers=None
    ),
    enabled=False,  # Disabled by default
    tool_prefix="hf",
    resources=None,
    bearer_token="hf_xxx"  # Your HF token
)

# Alternative WebSocket example
websocket_server = mistralrs.McpServerConfigPy(
    name="WebSocket Example",
    source=mistralrs.McpServerSourcePy.WebSocket(
        url="wss://api.example.com/mcp",
        timeout_secs=30,
        headers=None
    ),
    enabled=False  # Disabled by default
)

# Create MCP client config using filesystem server (others are disabled)
mcp_config = mistralrs.McpClientConfigPy(
    servers=[filesystem_server], # hf_server, websocket_server can be added when enabled
    auto_register_tools=True,
    tool_timeout_secs=30,
    max_concurrent_calls=5
)

# Build model with MCP support
runner = mistralrs.Runner(
    which=mistralrs.Which.Plain(
        model_id="Qwen/Qwen3-4B",
        arch=mistralrs.Architecture.Qwen3,
    ),
    mcp_client_config=mcp_config
)

# Use the model - tools are automatically available
res = runner.send_chat_completion_request(
    mistralrs.ChatCompletionRequest(
        model="default",
        messages=[
            {"role": "user", "content": "List the files in the current directory and create a test.txt file"}
        ],
        max_tokens=500,
        temperature=0.1,
    )
)
print(res.choices[0].message.content)
</code></pre>
<h3 id="http-api-11"><a class="header" href="#http-api-11">HTTP API</a></h3>
<ol>
<li>Create <code>mcp-config.json</code>:</li>
</ol>
<p><strong>Process Example (Recommended for getting started):</strong></p>
<pre><code class="language-json">{
  "servers": [{
    "name": "Filesystem Tools",
    "source": {
      "type": "Process",
      "command": "npx",
      "args": ["@modelcontextprotocol/server-filesystem", "."]
    }
  }],
  "auto_register_tools": true
}
</code></pre>
<blockquote>
<p><strong>Note:</strong> To install the filesystem server, run: <code>npx @modelcontextprotocol/server-filesystem . -y</code></p>
</blockquote>
<p><strong>HTTP Example (Hugging Face MCP Server):</strong></p>
<pre><code class="language-json">{
  "servers": [
    {
      "name": "Hugging Face MCP",
      "source": {
        "type": "Http",
        "url": "https://hf.co/mcp",
        "timeout_secs": 30
      },
      "bearer_token": "hf_xxx",
      "tool_prefix": "hf",
      "enabled": false
    },
    {
      "name": "Filesystem Tools",
      "source": {
        "type": "Process",
        "command": "npx",
        "args": ["@modelcontextprotocol/server-filesystem", "."]
      }
    }
  ],
  "auto_register_tools": true,
  "tool_timeout_secs": 30,
  "max_concurrent_calls": 5
}
</code></pre>
<p><strong>WebSocket Example:</strong></p>
<pre><code class="language-json">{
  "servers": [
    {
      "name": "WebSocket Example",
      "source": {
        "type": "WebSocket",
        "url": "wss://api.example.com/mcp",
        "timeout_secs": 30
      },
      "enabled": false
    },
    {
      "name": "Filesystem Tools",
      "source": {
        "type": "Process",
        "command": "npx",
        "args": ["@modelcontextprotocol/server-filesystem", "."]
      }
    }
  ],
  "auto_register_tools": true
}
</code></pre>
<ol start="2">
<li>Start server with MCP:</li>
</ol>
<pre><code class="language-bash">mistralrs serve \
  -p 1234 \
  --mcp-config mcp-config.json \
  -m Qwen/Qwen3-4B
</code></pre>
<ol start="3">
<li>Use the API:</li>
</ol>
<pre><code class="language-bash">curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "List the files in the current directory and create a test.txt file"}
    ],
    "max_tokens": 500,
    "temperature": 0.1
  }'
</code></pre>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<ul>
<li><strong>Automatic Tool Discovery</strong>: Tools are discovered from MCP servers at startup</li>
<li><strong>Multi-Server Support</strong>: Connect to multiple MCP servers simultaneously</li>
<li><strong>Transport Flexibility</strong>: HTTP, WebSocket, and Process transports supported</li>
<li><strong>Authentication</strong>: Bearer token support for secure connections</li>
<li><strong>Tool Prefixing</strong>: Avoid naming conflicts between servers</li>
<li><strong>Concurrency Control</strong>: Limit parallel tool executions</li>
<li><strong>Timeout Management</strong>: Control individual tool execution timeouts</li>
</ul>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<ul>
<li><a href="#mcp-configuration-reference">Configuration Reference</a> - Detailed configuration options</li>
<li><a href="#mcp-transport-types-1">Transport Types</a> - HTTP, WebSocket, and Process transports</li>
<li><a href="#advanced-mcp-usage">Advanced Usage</a> - Multi-server setups, custom headers, and more</li>
<li><a href="#mcp-protocol-support">MCP Server Development</a> - Building your own MCP server</li>
</ul>
<h2 id="common-mcp-servers"><a class="header" href="#common-mcp-servers">Common MCP Servers</a></h2>
<ul>
<li><strong>Filesystem</strong>: <code>@modelcontextprotocol/server-filesystem</code> - Local file operations (Process)</li>
<li><strong>Hugging Face</strong>: <code>https://hf.co/mcp</code> - Access HF models, datasets, and spaces (HTTP)</li>
<li><strong>Postgres</strong>: <code>@modelcontextprotocol/server-postgres</code> - Database operations (Process)</li>
</ul>
<p><strong>Additional servers (install separately):</strong></p>
<ul>
<li><a href="https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search">Brave Search</a> - Web search capabilities</li>
<li><a href="https://github.com/modelcontextprotocol/servers/tree/main/src/github">GitHub</a> - GitHub API access</li>
</ul>
<p>Replace placeholder tokens and URLs with actual values for your use case.</p>
<h2 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h2>
<h3 id="common-issues-2"><a class="header" href="#common-issues-2">Common Issues</a></h3>
<p><strong>“MCP server failed to start” or “npx command not found”</strong></p>
<ul>
<li>Install Node.js and npm: <code>curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash - &amp;&amp; sudo apt-get install -y nodejs</code></li>
<li>Install the filesystem server: <code>npx @modelcontextprotocol/server-filesystem . -y</code></li>
</ul>
<p><strong>“No tools available” or “tools_available: false”</strong></p>
<ul>
<li>Check server logs for MCP connection errors</li>
<li>Verify the MCP config file path is correct</li>
<li>Ensure the MCP server process is running: <code>ps aux | grep mcp</code></li>
</ul>
<p><strong>“Tool call failed” or timeout errors</strong></p>
<ul>
<li>Increase <code>tool_timeout_secs</code> in your config (default: 30)</li>
<li>Check <code>max_concurrent_calls</code> setting (start with 1-5)</li>
<li>Verify file permissions for filesystem operations</li>
</ul>
<p><strong>Authentication errors with HTTP servers</strong></p>
<ul>
<li>Double-check <code>bearer_token</code> values (e.g., HF tokens start with <code>hf_</code>)</li>
<li>Verify API endpoints are accessible: <code>curl -H "Authorization: Bearer YOUR_TOKEN" https://hf.co/mcp</code></li>
</ul>
<p><strong>Need help?</strong></p>
<ul>
<li><a href="https://github.com/modelcontextprotocol/servers">MCP Server Registry</a> - Find more servers</li>
<li><a href="https://discord.gg/SZrecqK8qw">Discord Community</a> - Get support</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mcp-protocol-support"><a class="header" href="#mcp-protocol-support">MCP protocol support</a></h1>
<p><code>mistralrs serve</code> can speak the <strong>MCP – Model-Control-Protocol</strong> in addition to the regular OpenAI-compatible REST API.</p>
<p>At a high-level, MCP is an opinionated, tool-based JSON-RPC 2.0 protocol that lets clients interact with models through structured <em>tool calls</em> instead of specialised HTTP routes.<br>The implementation in Mistral.rs is powered by <a href="https://crates.io/crates/rust-mcp-sdk"><code>rust-mcp-sdk</code></a> and automatically registers tools based on the modalities supported by the loaded model (text, vision, …).</p>
<p>Exposed tools:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Minimum <code>input</code> -&gt; <code>output</code> modalities</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>chat</code></td><td><code>Text</code> -&gt; <code>Text</code></td><td>Wraps the OpenAI <code>/v1/chat/completions</code> endpoint</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="toc-7"><a class="header" href="#toc-7">ToC</a></h2>
<ul>
<li><a href="#mcp-protocol-support">MCP protocol support</a>
<ul>
<li><a href="#toc-7">ToC</a></li>
<li><a href="#running">Running</a></li>
<li><a href="#check-if-its-working">Check if it’s working</a></li>
<li><a href="#example-clients">Example clients</a>
<ul>
<li><a href="#python-17">Python</a></li>
<li><a href="#rust-15">Rust</a></li>
<li><a href="#http">HTTP</a></li>
</ul>
</li>
<li><a href="#limitations--roadmap">Limitations &amp; roadmap</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="running"><a class="header" href="#running">Running</a></h2>
<p>Start the normal HTTP server and add the <code>--mcp-port</code> flag to expose an MCP endpoint <strong>in parallel</strong> on a separate port:</p>
<pre><code class="language-bash">mistralrs serve \
  -p 1234 \
  --mcp-port 4321 \
  -m mistralai/Mistral-7B-Instruct-v0.3
</code></pre>
<h2 id="check-if-its-working"><a class="header" href="#check-if-its-working">Check if it’s working</a></h2>
<p>The following <code>curl</code> command lists the tools advertised by the server and therefore serves as a quick smoke-test:</p>
<pre><code>curl -X POST http://localhost:4321/mcp \
-H "Content-Type: application/json" \
-d '{
  "jsonrpc": "2.0",
  "id": 2,
  "method": "tools/list",
  "params": {}
}'      
</code></pre>
<h2 id="example-clients"><a class="header" href="#example-clients">Example clients</a></h2>
<h3 id="python-17"><a class="header" href="#python-17">Python</a></h3>
<p>The <a href="https://pypi.org/project/mcp/">reference Python SDK</a> can be installed via:</p>
<pre><code class="language-bash">pip install --upgrade mcp
</code></pre>
<p>Here is a minimal end-to-end example that initialises a session, lists the available tools and finally sends a chat request:</p>
<pre><code class="language-python">import asyncio

from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client


SERVER_URL = "http://localhost:4321/mcp"


async def main() -&gt; None:
    # The helper creates an SSE (Server-Sent-Events) transport under the hood
    async with streamablehttp_client(SERVER_URL) as (read, write, _):
        async with ClientSession(read, write) as session:

            # --- INITIALIZE ---
            init_result = await session.initialize()
            print("Server info:", init_result.serverInfo)

            # --- LIST TOOLS ---
            tools = await session.list_tools()
            print("Available tools:", [t.name for t in tools.tools])

            # --- CALL TOOL ---
            resp = await session.call_tool(
                "chat",
                arguments={
                    "messages": [
                        {"role": "user", "content": "Hello MCP 👋"},
                        {"role": "assistant", "content": "Hi there!"}
                    ],
                    "maxTokens": 50,
                    "temperature": 0.7,
                },
            )
            # resp.content is a list[CallToolResultContentItem]; extract text parts
            text = "\n".join(c.text for c in resp.content if c.type == "text")
            print("Model replied:", text)

if __name__ == "__main__":
    asyncio.run(main())
</code></pre>
<h3 id="rust-15"><a class="header" href="#rust-15">Rust</a></h3>
<pre class="playground"><code class="language-rust">use anyhow::Result;
use rust_mcp_sdk::{
    mcp_client::client_runtime,
    schema::{
        CallToolRequestParams, ClientCapabilities, CreateMessageRequest,
        Implementation, InitializeRequestParams, Message, LATEST_PROTOCOL_VERSION,
    },
    ClientSseTransport, ClientSseTransportOptions,
};

struct Handler;
#[async_trait::async_trait]
impl rust_mcp_sdk::mcp_client::ClientHandler for Handler {}

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let transport = ClientSseTransport::new(
        "http://localhost:4321/mcp",
        ClientSseTransportOptions::default(),
    )?;

    let details = InitializeRequestParams {
        capabilities: ClientCapabilities::default(),
        client_info: Implementation { name: "mcp-client".into(), version: "0.1".into() },
        protocol_version: LATEST_PROTOCOL_VERSION.into(),
    };

    let client = client_runtime::create_client(details, transport, Handler);
    client.clone().start().await?;

    let req = CreateMessageRequest {
        model: "mistralai/Mistral-7B-Instruct-v0.3".into(),
        messages: vec![Message::user("Explain Rust ownership.")],
        ..Default::default()
    };

    let result = client
        .call_tool(CallToolRequestParams::new("chat", req.into()))
        .await?;

    println!("{}", result.content[0].as_text_content()?.text);
    client.shut_down().await?;
    Ok(())
}</code></pre>
<h3 id="http"><a class="header" href="#http">HTTP</a></h3>
<p><strong>Call a tool:</strong></p>
<pre><code class="language-bash">curl -X POST http://localhost:4321/mcp \
-H "Content-Type: application/json" \
-d '{
  "jsonrpc": "2.0",
  "id": 3,
  "method": "tools/call",
  "params": {
    "name": "chat",
    "arguments": {
    "messages": [
      { "role": "system",    "content": "You are a helpful assistant." },
      { "role": "user",      "content": "Hello, what’s the time?" }
    ],
    "maxTokens": 50,
    "temperature": 0.7
  }
  }
}'
</code></pre>
<p><strong>Initialize:</strong></p>
<pre><code class="language-bash">curl -X POST http://localhost:4321/mcp \
-H "Content-Type: application/json" \
-d '{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {}
}'         
</code></pre>
<p><strong>List tools:</strong></p>
<pre><code class="language-bash">curl -X POST http://localhost:4321/mcp \
-H "Content-Type: application/json" \
-d '{
  "jsonrpc": "2.0",
  "id": 2,
  "method": "tools/list",
  "params": {}
}'      
</code></pre>
<h2 id="limitations--roadmap"><a class="header" href="#limitations--roadmap">Limitations &amp; roadmap</a></h2>
<p>The MCP support that ships with the current Mistral.rs release focuses on the <strong>happy-path</strong>.  A few niceties have not yet been implemented and PRs are more than welcome:</p>
<ol>
<li>Streaming token responses (similar to the <code>stream=true</code> flag in the OpenAI API).</li>
<li>An authentication layer – if you are exposing the MCP port publicly run it behind a reverse-proxy that handles auth (e.g.  nginx + OIDC).</li>
<li>Additional tools for other modalities such as vision or audio once the underlying crates stabilise.</li>
</ol>
<p>If you would like to work on any of the above please open an issue first so the work can be coordinated.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mcp-configuration-reference"><a class="header" href="#mcp-configuration-reference">MCP Configuration Reference</a></h1>
<p>This page provides a complete reference for configuring the MCP client in mistral.rs.</p>
<h2 id="quick-start---minimal-configuration"><a class="header" href="#quick-start---minimal-configuration">Quick Start - Minimal Configuration</a></h2>
<p>For simple use cases, you can now use a minimal configuration that leverages smart defaults:</p>
<pre><code class="language-json">{
  "servers": [{
    "name": "Hugging Face MCP Server",
    "source": {
      "type": "Http",
      "url": "https://hf.co/mcp"
    },
    "bearer_token": "hf_xxx"
  }]
}
</code></pre>
<p>This automatically provides:</p>
<ul>
<li><strong>UUID-based server ID</strong>: Unique identifier generated automatically</li>
<li><strong>Enabled by default</strong>: Server is active without explicit <code>enabled: true</code></li>
<li><strong>UUID-based tool prefix</strong>: Prevents naming conflicts automatically</li>
<li><strong>No timeouts</strong>: Tools and connections don’t timeout by default</li>
<li><strong>Sequential execution</strong>: Only 1 concurrent tool call to prevent overwhelming servers</li>
<li><strong>Auto-registration</strong>: Tools are automatically discovered and registered</li>
</ul>
<h2 id="configuration-structure-1"><a class="header" href="#configuration-structure-1">Configuration Structure</a></h2>
<h3 id="mcpclientconfig"><a class="header" href="#mcpclientconfig">McpClientConfig</a></h3>
<p>The top-level configuration for the MCP client:</p>
<pre><code class="language-json">{
  "servers": [...],                    // Array of MCP server configurations
  "auto_register_tools": true,         // Automatically register discovered tools (default: true)
  "tool_timeout_secs": null,           // Timeout for individual tool calls, null = no timeout (default: null)
  "max_concurrent_calls": 1            // Maximum concurrent tool executions (default: 1)
}
</code></pre>
<h3 id="mcpserverconfig"><a class="header" href="#mcpserverconfig">McpServerConfig</a></h3>
<p>Configuration for each MCP server:</p>
<pre><code class="language-json">{
  "id": "unique_id",                  // Unique identifier (default: UUID if not specified)
  "name": "Display Name",             // Human-readable name
  "source": {...},                    // Transport configuration (see below)
  "enabled": true,                    // Enable/disable this server (default: true)
  "tool_prefix": "mcp_abc123",         // Prefix for tool names (default: UUID-based if not specified)
  "resources": ["pattern"],           // Optional resource patterns
  "bearer_token": "token"             // Optional authentication token
}
</code></pre>
<h2 id="transport-source-configuration"><a class="header" href="#transport-source-configuration">Transport Source Configuration</a></h2>
<h3 id="http-transport"><a class="header" href="#http-transport">HTTP Transport</a></h3>
<pre><code class="language-json">{
  "type": "Http",
  "url": "https://api.example.com/mcp",
  "timeout_secs": null,               // Optional, null = no timeout (default)
  "headers": {                        // Optional custom headers
    "X-API-Version": "v1",
    "User-Agent": "mistral-rs/0.6.0"
  }
}
</code></pre>
<h3 id="websocket-transport"><a class="header" href="#websocket-transport">WebSocket Transport</a></h3>
<pre><code class="language-json">{
  "type": "WebSocket", 
  "url": "wss://realtime.example.com/mcp",
  "timeout_secs": null,               // Optional, null = no timeout (default)
  "headers": {                        // Optional WebSocket headers
    "Origin": "https://mistral.rs",
    "Sec-WebSocket-Protocol": "mcp"
  }
}
</code></pre>
<h3 id="process-transport"><a class="header" href="#process-transport">Process Transport</a></h3>
<pre><code class="language-json">{
  "type": "Process",
  "command": "mcp-server-filesystem",
  "args": ["--root", "/tmp"],         // Command arguments
  "work_dir": "/home/user",           // Optional working directory
  "env": {                            // Optional environment variables
    "MCP_LOG_LEVEL": "info"
  }
}
</code></pre>
<h2 id="field-reference"><a class="header" href="#field-reference">Field Reference</a></h2>
<h3 id="mcpclientconfig-fields"><a class="header" href="#mcpclientconfig-fields">McpClientConfig Fields</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>servers</code></td><td>Array</td><td>Yes</td><td>-</td><td>List of MCP server configurations</td></tr>
<tr><td><code>auto_register_tools</code></td><td>Boolean</td><td>No</td><td><code>true</code></td><td>Automatically discover and register tools at startup</td></tr>
<tr><td><code>tool_timeout_secs</code></td><td>Integer</td><td>No</td><td><code>null</code></td><td>Timeout in seconds for individual tool calls (null = no timeout)</td></tr>
<tr><td><code>max_concurrent_calls</code></td><td>Integer</td><td>No</td><td><code>1</code></td><td>Maximum number of concurrent tool executions</td></tr>
</tbody>
</table>
</div>
<h3 id="mcpserverconfig-fields"><a class="header" href="#mcpserverconfig-fields">McpServerConfig Fields</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>String</td><td>No</td><td>UUID</td><td>Unique identifier for the server (UUID generated if not provided)</td></tr>
<tr><td><code>name</code></td><td>String</td><td>Yes</td><td>-</td><td>Human-readable server name</td></tr>
<tr><td><code>source</code></td><td>Object</td><td>Yes</td><td>-</td><td>Transport configuration</td></tr>
<tr><td><code>enabled</code></td><td>Boolean</td><td>No</td><td><code>true</code></td><td>Whether to connect to this server</td></tr>
<tr><td><code>tool_prefix</code></td><td>String</td><td>No</td><td>UUID-based</td><td>Prefix to add to all tool names (UUID-based if not provided)</td></tr>
<tr><td><code>resources</code></td><td>Array</td><td>No</td><td>None</td><td>Resource URI patterns to subscribe to</td></tr>
<tr><td><code>bearer_token</code></td><td>String</td><td>No</td><td>None</td><td>Bearer token for authentication</td></tr>
</tbody>
</table>
</div>
<h3 id="transport-source-fields"><a class="header" href="#transport-source-fields">Transport Source Fields</a></h3>
<h4 id="http-source"><a class="header" href="#http-source">HTTP Source</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>type</code></td><td>String</td><td>Yes</td><td>-</td><td>Must be “Http”</td></tr>
<tr><td><code>url</code></td><td>String</td><td>Yes</td><td>-</td><td>HTTP/HTTPS URL of the MCP server</td></tr>
<tr><td><code>timeout_secs</code></td><td>Integer</td><td>No</td><td><code>null</code></td><td>Request timeout in seconds (null = no timeout)</td></tr>
<tr><td><code>headers</code></td><td>Object</td><td>No</td><td>None</td><td>Additional HTTP headers</td></tr>
</tbody>
</table>
</div>
<h4 id="websocket-source"><a class="header" href="#websocket-source">WebSocket Source</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>type</code></td><td>String</td><td>Yes</td><td>-</td><td>Must be “WebSocket”</td></tr>
<tr><td><code>url</code></td><td>String</td><td>Yes</td><td>-</td><td>WS/WSS URL of the MCP server</td></tr>
<tr><td><code>timeout_secs</code></td><td>Integer</td><td>No</td><td><code>null</code></td><td>Connection timeout in seconds (null = no timeout)</td></tr>
<tr><td><code>headers</code></td><td>Object</td><td>No</td><td>None</td><td>WebSocket handshake headers</td></tr>
</tbody>
</table>
</div>
<h4 id="process-source"><a class="header" href="#process-source">Process Source</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>type</code></td><td>String</td><td>Yes</td><td>-</td><td>Must be “Process”</td></tr>
<tr><td><code>command</code></td><td>String</td><td>Yes</td><td>-</td><td>Executable command to run</td></tr>
<tr><td><code>args</code></td><td>Array</td><td>No</td><td><code>[]</code></td><td>Command line arguments</td></tr>
<tr><td><code>work_dir</code></td><td>String</td><td>No</td><td>Current dir</td><td>Working directory</td></tr>
<tr><td><code>env</code></td><td>Object</td><td>No</td><td>None</td><td>Environment variables</td></tr>
</tbody>
</table>
</div>
<h2 id="authentication-1"><a class="header" href="#authentication-1">Authentication</a></h2>
<h3 id="bearer-token"><a class="header" href="#bearer-token">Bearer Token</a></h3>
<p>The <code>bearer_token</code> field is automatically added as an <code>Authorization: Bearer &lt;token&gt;</code> header for HTTP and WebSocket connections.</p>
<pre><code class="language-json">{
  "bearer_token": "hf_AbCdEfGhIjKlMnOpQrStUvWxYz"
}
</code></pre>
<h3 id="custom-headers"><a class="header" href="#custom-headers">Custom Headers</a></h3>
<p>For other authentication schemes, use the <code>headers</code> field:</p>
<pre><code class="language-json">{
  "source": {
    "type": "Http",
    "url": "https://api.example.com/mcp",
    "headers": {
      "X-API-Key": "your-api-key",
      "X-Client-ID": "your-client-id"
    }
  }
}
</code></pre>
<h2 id="tool-naming"><a class="header" href="#tool-naming">Tool Naming</a></h2>
<h3 id="without-prefix"><a class="header" href="#without-prefix">Without Prefix</a></h3>
<p>Tools are registered with their original names:</p>
<ul>
<li>MCP tool: <code>search</code> -&gt; Registered as: <code>search</code></li>
</ul>
<h3 id="with-prefix"><a class="header" href="#with-prefix">With Prefix</a></h3>
<p>When <code>tool_prefix</code> is set, all tools from that server get prefixed:</p>
<ul>
<li>MCP tool: <code>search</code> with prefix <code>web</code> -&gt; Registered as: <code>web_search</code></li>
</ul>
<p>This prevents conflicts when multiple servers provide tools with the same name.</p>
<h2 id="resource-patterns"><a class="header" href="#resource-patterns">Resource Patterns</a></h2>
<p>The <code>resources</code> field accepts glob-like patterns:</p>
<pre><code class="language-json">{
  "resources": [
    "file://**/*.txt",      // All .txt files
    "file://data/**",       // Everything under data/
    "db://users/*",         // All user records
    "api://v1/metrics"      // Specific endpoint
  ]
}
</code></pre>
<h2 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h2>
<h3 id="using-environment-variables-in-configuration"><a class="header" href="#using-environment-variables-in-configuration">Using Environment Variables in Configuration</a></h3>
<p>While JSON doesn’t support environment variables directly, you can use them when building configurations programmatically:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpServerConfig {
    bearer_token: std::env::var("HF_TOKEN").ok(),
    source: McpServerSource::Http {
        url: std::env::var("MCP_SERVER_URL")
            .unwrap_or_else(|_| "https://hf.co/mcp".to_string()),
        // ...
    },
    // ...
}
<span class="boring">}</span></code></pre>
<pre><code class="language-python">import os

McpServerConfigPy(
    bearer_token=os.getenv("HF_TOKEN"),
    source=McpServerSourcePy.Http(
        url=os.getenv("MCP_SERVER_URL", "https://hf.co/mcp")
    )
)
</code></pre>
<h3 id="mcp-related-environment-variables"><a class="header" href="#mcp-related-environment-variables">MCP-Related Environment Variables</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>MCP_CONFIG_PATH</code></td><td>Path to MCP configuration file</td></tr>
<tr><td><code>MCP_LOG_LEVEL</code></td><td>Logging level for MCP operations</td></tr>
<tr><td><code>MCP_POOL_SIZE</code></td><td>Connection pool size for HTTP/WebSocket</td></tr>
</tbody>
</table>
</div>
<h2 id="validation-rules"><a class="header" href="#validation-rules">Validation Rules</a></h2>
<ol>
<li><strong>Unique Server IDs</strong>: All server <code>id</code> values must be unique</li>
<li><strong>Valid URLs</strong>: HTTP URLs must start with <code>http://</code> or <code>https://</code></li>
<li><strong>Valid WebSocket URLs</strong>: Must start with <code>ws://</code> or <code>wss://</code></li>
<li><strong>Executable Commands</strong>: Process commands must be executable</li>
<li><strong>Tool Name Conflicts</strong>: Use <code>tool_prefix</code> to avoid conflicts</li>
</ol>
<h2 id="example-configurations"><a class="header" href="#example-configurations">Example Configurations</a></h2>
<h3 id="single-server-hugging-face---minimal"><a class="header" href="#single-server-hugging-face---minimal">Single Server (Hugging Face) - Minimal</a></h3>
<pre><code class="language-json">{
  "servers": [{
    "name": "Hugging Face MCP Server",
    "source": {
      "type": "Http",
      "url": "https://hf.co/mcp"
    },
    "bearer_token": "hf_xxx"
  }]
}
</code></pre>
<h3 id="single-server-hugging-face---full-configuration"><a class="header" href="#single-server-hugging-face---full-configuration">Single Server (Hugging Face) - Full Configuration</a></h3>
<pre><code class="language-json">{
  "servers": [{
    "id": "hf",
    "name": "Hugging Face MCP",
    "source": {
      "type": "Http",
      "url": "https://hf.co/mcp",
      "timeout_secs": 30
    },
    "enabled": true,
    "tool_prefix": "hf",
    "bearer_token": "hf_xxx"
  }],
  "auto_register_tools": true,
  "tool_timeout_secs": 30,
  "max_concurrent_calls": 5
}
</code></pre>
<h3 id="multi-server-setup"><a class="header" href="#multi-server-setup">Multi-Server Setup</a></h3>
<pre><code class="language-json">{
  "servers": [
    {
      "id": "hf",
      "name": "Hugging Face",
      "source": {"type": "Http", "url": "https://hf.co/mcp"},
      "tool_prefix": "hf",
      "bearer_token": "hf_xxx"
    },
    {
      "id": "github",
      "name": "GitHub API",
      "source": {"type": "Http", "url": "https://api.github.com/mcp"},
      "tool_prefix": "gh",
      "bearer_token": "ghp_xxx"
    },
    {
      "id": "local_fs",
      "name": "Filesystem",
      "source": {
        "type": "Process",
        "command": "mcp-server-filesystem",
        "args": ["--root", "/data", "--readonly"]
      },
      "tool_prefix": "fs"
    }
  ],
  "auto_register_tools": true,
  "tool_timeout_secs": 30,
  "max_concurrent_calls": 10
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mcp-transport-types-1"><a class="header" href="#mcp-transport-types-1">MCP Transport Types</a></h1>
<p>mistral.rs supports three transport types for connecting to MCP servers, each optimized for different use cases.</p>
<h2 id="http-transport-1"><a class="header" href="#http-transport-1">HTTP Transport</a></h2>
<p>Best for public APIs, RESTful services, and servers behind load balancers.</p>
<h3 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h3>
<pre><code class="language-json">{
  "source": {
    "type": "Http",
    "url": "https://api.example.com/mcp",
    "timeout_secs": 30,
    "headers": {
      "X-API-Version": "v1",
      "User-Agent": "mistral-rs/0.6.0"
    }
  },
  "bearer_token": "your-api-token"
}
</code></pre>
<h3 id="features-2"><a class="header" href="#features-2">Features</a></h3>
<ul>
<li>Server-Sent Events (SSE) support for streaming responses</li>
<li>Custom headers for API versioning or client identification</li>
<li>Bearer token authentication (added as <code>Authorization: Bearer &lt;token&gt;</code>)</li>
<li>Configurable timeouts</li>
<li>Standard HTTP semantics</li>
</ul>
<h3 id="example-hugging-face-mcp"><a class="header" href="#example-hugging-face-mcp">Example: Hugging Face MCP</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpServerSource::Http {
    url: "https://hf.co/mcp".to_string(),
    timeout_secs: Some(30),
    headers: None,
}
<span class="boring">}</span></code></pre>
<h2 id="websocket-transport-1"><a class="header" href="#websocket-transport-1">WebSocket Transport</a></h2>
<p>Best for real-time applications, bidirectional communication, and low-latency requirements.</p>
<h3 id="configuration-1-1"><a class="header" href="#configuration-1-1">Configuration</a></h3>
<pre><code class="language-json">{
  "source": {
    "type": "WebSocket",
    "url": "wss://realtime.example.com/mcp",
    "timeout_secs": 60,
    "headers": {
      "Origin": "https://mistral.rs",
      "Sec-WebSocket-Protocol": "mcp"
    }
  },
  "bearer_token": "your-websocket-token"
}
</code></pre>
<h3 id="features-1-1"><a class="header" href="#features-1-1">Features</a></h3>
<ul>
<li>Persistent connections reduce handshake overhead</li>
<li>Server-initiated notifications</li>
<li>Lower latency for frequent tool calls</li>
<li>Automatic reconnection handling</li>
<li>WebSocket-specific headers support</li>
</ul>
<h3 id="example-real-time-data-feed"><a class="header" href="#example-real-time-data-feed">Example: Real-time Data Feed</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpServerSource::WebSocket {
    url: "wss://data.example.com/mcp".to_string(),
    timeout_secs: Some(60),
    headers: Some(headers),
}
<span class="boring">}</span></code></pre>
<h2 id="process-transport-1"><a class="header" href="#process-transport-1">Process Transport</a></h2>
<p>Best for local tools, development servers, and sandboxed environments.</p>
<h3 id="configuration-2-1"><a class="header" href="#configuration-2-1">Configuration</a></h3>
<pre><code class="language-json">{
  "source": {
    "type": "Process",
    "command": "mcp-server-filesystem",
    "args": ["--root", "/tmp", "--readonly"],
    "work_dir": "/home/user/workspace",
    "env": {
      "MCP_LOG_LEVEL": "info",
      "MCP_TIMEOUT": "30"
    }
  }
}
</code></pre>
<h3 id="features-2-1"><a class="header" href="#features-2-1">Features</a></h3>
<ul>
<li>No network overhead</li>
<li>Process isolation for security</li>
<li>Direct stdin/stdout communication</li>
<li>Environment variable configuration</li>
<li>Working directory control</li>
<li>No authentication needed (process inherits permissions)</li>
</ul>
<h3 id="example-filesystem-server"><a class="header" href="#example-filesystem-server">Example: Filesystem Server</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpServerSource::Process {
    command: "mcp-server-filesystem".to_string(),
    args: vec!["--root".to_string(), "/tmp".to_string()],
    work_dir: None,
    env: None,
}
<span class="boring">}</span></code></pre>
<h2 id="transport-selection-guide"><a class="header" href="#transport-selection-guide">Transport Selection Guide</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Use Case</th><th>Recommended Transport</th><th>Why</th></tr>
</thead>
<tbody>
<tr><td>Public APIs</td><td>HTTP</td><td>Standard auth, caching, load balancing</td></tr>
<tr><td>Local tools</td><td>Process</td><td>No network, process isolation</td></tr>
<tr><td>Real-time data</td><td>WebSocket</td><td>Low latency, server push</td></tr>
<tr><td>Corporate proxies</td><td>HTTP</td><td>Proxy support, standard ports</td></tr>
<tr><td>Development</td><td>Process</td><td>Easy debugging, no network setup</td></tr>
<tr><td>Interactive apps</td><td>WebSocket</td><td>Bidirectional, persistent connection</td></tr>
</tbody>
</table>
</div>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<h3 id="http-1"><a class="header" href="#http-1">HTTP</a></h3>
<ul>
<li>Always use HTTPS in production</li>
<li>Bearer tokens transmitted with each request</li>
<li>Consider token rotation strategies</li>
</ul>
<h3 id="websocket"><a class="header" href="#websocket">WebSocket</a></h3>
<ul>
<li>Use WSS (WebSocket Secure) in production</li>
<li>Bearer token sent during handshake</li>
<li>Connection persists with authenticated state</li>
</ul>
<h3 id="process"><a class="header" href="#process">Process</a></h3>
<ul>
<li>Inherits user permissions</li>
<li>Sandboxing via work_dir and env</li>
<li>No network exposure</li>
</ul>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<ol>
<li><strong>HTTP</strong>: Enable keep-alive, use connection pooling</li>
<li><strong>WebSocket</strong>: Reuse connections, handle reconnection gracefully</li>
<li><strong>Process</strong>: Minimize startup time, use long-running processes</li>
</ol>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>All transports implement automatic retry with exponential backoff:</p>
<ul>
<li>Initial retry: 1 second</li>
<li>Max retry: 60 seconds</li>
<li>Max attempts: 5</li>
</ul>
<p>Custom retry behavior can be configured per server.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="advanced-mcp-usage"><a class="header" href="#advanced-mcp-usage">Advanced MCP Usage</a></h1>
<p>This guide covers advanced MCP client configurations and usage patterns.</p>
<h2 id="multi-server-configuration"><a class="header" href="#multi-server-configuration">Multi-Server Configuration</a></h2>
<p>Connect to multiple MCP servers simultaneously to access different tool sets:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mcp_config = McpClientConfig {
    servers: vec![
        // Hugging Face for ML tools
        McpServerConfig {
            id: "hf_server".to_string(),
            name: "Hugging Face MCP".to_string(),
            source: McpServerSource::Http {
                url: "https://hf.co/mcp".to_string(),
                timeout_secs: Some(30),
                headers: None,
            },
            enabled: true,
            tool_prefix: Some("hf".to_string()),
            resources: None,
            bearer_token: Some("hf_xxx".to_string()),
        },
        // Local filesystem access
        McpServerConfig {
            id: "fs_server".to_string(),
            name: "Filesystem MCP".to_string(),
            source: McpServerSource::Process {
                command: "mcp-server-filesystem".to_string(),
                args: vec!["--root".to_string(), "/data".to_string()],
                work_dir: None,
                env: None,
            },
            enabled: true,
            tool_prefix: Some("fs".to_string()),
            resources: Some(vec!["file://**".to_string()]),
            bearer_token: None,
        },
        // GitHub API access
        McpServerConfig {
            id: "github_server".to_string(),
            name: "GitHub MCP".to_string(),
            source: McpServerSource::Http {
                url: "https://api.github.com/mcp".to_string(),
                timeout_secs: Some(45),
                headers: Some(HashMap::from([
                    ("Accept".to_string(), "application/vnd.github.v3+json".to_string()),
                ])),
            },
            enabled: true,
            tool_prefix: Some("gh".to_string()),
            resources: None,
            bearer_token: Some("ghp_xxx".to_string()),
        },
    ],
    auto_register_tools: true,
    tool_timeout_secs: Some(30),
    max_concurrent_calls: Some(10),
};
<span class="boring">}</span></code></pre>
<h2 id="tool-prefixing-strategy"><a class="header" href="#tool-prefixing-strategy">Tool Prefixing Strategy</a></h2>
<p>When using multiple servers, tool prefixes prevent naming conflicts:</p>
<pre><code class="language-json">{
  "servers": [
    {
      "id": "server1",
      "tool_prefix": "s1",
      // Tool "search" becomes "s1_search"
    },
    {
      "id": "server2", 
      "tool_prefix": "s2",
      // Tool "search" becomes "s2_search"
    }
  ]
}
</code></pre>
<h2 id="custom-headers-and-authentication"><a class="header" href="#custom-headers-and-authentication">Custom Headers and Authentication</a></h2>
<h3 id="api-key-in-headers"><a class="header" href="#api-key-in-headers">API Key in Headers</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut headers = HashMap::new();
headers.insert("X-API-Key".to_string(), "your-api-key".to_string());
headers.insert("X-Client-Version".to_string(), "1.0.0".to_string());

McpServerSource::Http {
    url: "https://api.example.com/mcp".to_string(),
    timeout_secs: Some(30),
    headers: Some(headers),
}
<span class="boring">}</span></code></pre>
<h3 id="oauth2-bearer-token"><a class="header" href="#oauth2-bearer-token">OAuth2 Bearer Token</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpServerConfig {
    // ...
    bearer_token: Some("your-oauth2-token".to_string()),
    // Automatically added as: Authorization: Bearer your-oauth2-token
}
<span class="boring">}</span></code></pre>
<h2 id="resource-subscriptions"><a class="header" href="#resource-subscriptions">Resource Subscriptions</a></h2>
<p>Subscribe to specific resource patterns from MCP servers:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpServerConfig {
    id: "data_server".to_string(),
    // ...
    resources: Some(vec![
        "file://data/**/*.json".to_string(),  // All JSON files in data/
        "db://users/*".to_string(),            // All user records
        "api://v1/metrics".to_string(),        // Specific API endpoint
    ]),
    // ...
}
<span class="boring">}</span></code></pre>
<h2 id="concurrency-and-rate-limiting"><a class="header" href="#concurrency-and-rate-limiting">Concurrency and Rate Limiting</a></h2>
<h3 id="global-concurrency-control"><a class="header" href="#global-concurrency-control">Global Concurrency Control</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpClientConfig {
    // ...
    max_concurrent_calls: Some(5),  // Max 5 tools executing simultaneously
}
<span class="boring">}</span></code></pre>
<h3 id="per-tool-timeouts"><a class="header" href="#per-tool-timeouts">Per-Tool Timeouts</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>McpClientConfig {
    // ...
    tool_timeout_secs: Some(30),  // Each tool call times out after 30s
}
<span class="boring">}</span></code></pre>
<h3 id="custom-rate-limiting"><a class="header" href="#custom-rate-limiting">Custom Rate Limiting</a></h3>
<pre><code class="language-python"># Python example with custom rate limiting
import time
from collections import deque

class RateLimitedMcpRunner:
    def __init__(self, runner, max_calls_per_minute=60):
        self.runner = runner
        self.max_calls = max_calls_per_minute
        self.call_times = deque()
    
    def send_chat_completion_request(self, request):
        # Remove calls older than 1 minute
        now = time.time()
        while self.call_times and self.call_times[0] &lt; now - 60:
            self.call_times.popleft()
        
        # Check rate limit
        if len(self.call_times) &gt;= self.max_calls:
            sleep_time = 60 - (now - self.call_times[0])
            time.sleep(sleep_time)
        
        # Make the call
        self.call_times.append(now)
        return self.runner.send_chat_completion_request(request)
</code></pre>
<h2 id="environment-specific-configuration"><a class="header" href="#environment-specific-configuration">Environment-Specific Configuration</a></h2>
<h3 id="development-vs-production"><a class="header" href="#development-vs-production">Development vs Production</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mcp_config = if cfg!(debug_assertions) {
    McpClientConfig {
        servers: vec![/* development servers */],
        tool_timeout_secs: Some(60),  // Longer timeouts for debugging
        max_concurrent_calls: Some(1), // Sequential execution for debugging
        // ...
    }
} else {
    McpClientConfig {
        servers: vec![/* production servers */],
        tool_timeout_secs: Some(10),   // Strict timeouts
        max_concurrent_calls: Some(20), // Higher concurrency
        // ...
    }
};
<span class="boring">}</span></code></pre>
<h3 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment Variables</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mcp_config = McpClientConfig {
    servers: vec![
        McpServerConfig {
            // ...
            bearer_token: std::env::var("HF_TOKEN").ok(),
            source: McpServerSource::Http {
                url: std::env::var("MCP_SERVER_URL")
                    .unwrap_or_else(|_| "https://hf.co/mcp".to_string()),
                // ...
            },
            // ...
        },
    ],
    // ...
};
<span class="boring">}</span></code></pre>
<h2 id="error-handling-and-fallbacks"><a class="header" href="#error-handling-and-fallbacks">Error Handling and Fallbacks</a></h2>
<h3 id="graceful-degradation"><a class="header" href="#graceful-degradation">Graceful Degradation</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mcp_config = McpClientConfig {
    servers: vec![
        // Primary server
        McpServerConfig {
            id: "primary".to_string(),
            enabled: true,
            // ...
        },
        // Fallback server
        McpServerConfig {
            id: "fallback".to_string(),
            enabled: check_primary_health().is_err(),
            // ...
        },
    ],
    // ...
};
<span class="boring">}</span></code></pre>
<h3 id="tool-specific-error-handling"><a class="header" href="#tool-specific-error-handling">Tool-Specific Error Handling</a></h3>
<pre><code class="language-python"># Handle specific tool errors
try:
    response = runner.send_chat_completion_request(request)
except Exception as e:
    if "tool_timeout" in str(e):
        print("Tool execution timed out, trying with longer timeout...")
        # Retry with extended timeout
    elif "tool_not_found" in str(e):
        print("Tool not available, falling back to built-in response...")
        # Fallback logic
</code></pre>
<h2 id="monitoring-and-debugging"><a class="header" href="#monitoring-and-debugging">Monitoring and Debugging</a></h2>
<h3 id="enable-debug-logging"><a class="header" href="#enable-debug-logging">Enable Debug Logging</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>std::env::set_var("RUST_LOG", "mistralrs_mcp=debug");
env_logger::init();
<span class="boring">}</span></code></pre>
<h3 id="tool-call-inspection"><a class="header" href="#tool-call-inspection">Tool Call Inspection</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let response = model.send_chat_request(messages).await?;

// Check if tools were called
if let Some(tool_calls) = &amp;response.choices[0].message.tool_calls {
    for call in tool_calls {
        println!("Tool: {}", call.function.name);
        println!("Args: {}", call.function.arguments);
        println!("ID: {}", call.id);
    }
}
<span class="boring">}</span></code></pre>
<h2 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h2>
<h3 id="connection-pooling"><a class="header" href="#connection-pooling">Connection Pooling</a></h3>
<p>HTTP and WebSocket transports automatically use connection pooling. Configure pool size:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Set via environment variable
std::env::set_var("MCP_POOL_SIZE", "10");
<span class="boring">}</span></code></pre>
<h3 id="caching-tool-responses"><a class="header" href="#caching-tool-responses">Caching Tool Responses</a></h3>
<pre><code class="language-python">from functools import lru_cache
import json

@lru_cache(maxsize=100)
def cached_tool_call(tool_name, args_json):
    args = json.loads(args_json)
    # Tool execution logic
    return result

# Use with MCP tools that have deterministic outputs
</code></pre>
<h2 id="security-best-practices"><a class="header" href="#security-best-practices">Security Best Practices</a></h2>
<ol>
<li><strong>Token Rotation</strong>: Implement automatic token refresh for long-running applications</li>
<li><strong>Least Privilege</strong>: Only enable required tools and resources</li>
<li><strong>Audit Logging</strong>: Log all tool calls for security monitoring</li>
<li><strong>Network Isolation</strong>: Use Process transport for sensitive local operations</li>
<li><strong>Input Validation</strong>: MCP servers should validate all tool inputs</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="configuration-reference"><a class="header" href="#configuration-reference">Configuration Reference</a></h1>
<p>This document covers environment variables and server configuration for mistral.rs.</p>
<h2 id="runtime-environment-variables"><a class="header" href="#runtime-environment-variables">Runtime Environment Variables</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_DEBUG=1</code></td><td>Enable debug mode: outputs tensor info files for GGUF/GGML models, increases logging verbosity</td></tr>
<tr><td><code>MISTRALRS_NO_MMAP=1</code></td><td>Disable memory-mapped file loading, forcing all tensor data into memory</td></tr>
<tr><td><code>MISTRALRS_NO_MLA=1</code></td><td>Disable <a href="#multi-head-latent-attention-mla-in-mistralrs">MLA</a> (Multi-head Latent Attention) optimization for DeepSeek V2/V3 and GLM-4.7-Flash</td></tr>
<tr><td><code>MISTRALRS_ISQ_SINGLETHREAD=1</code></td><td>Force ISQ (In-Situ Quantization) to run single-threaded</td></tr>
<tr><td><code>MISTRALRS_IGPU_MEMORY_FRACTION</code></td><td>Memory fraction for integrated/unified-memory CUDA GPUs (e.g. NVIDIA Grace Blackwell, Jetson). Float between 0.0 and 1.0, default: <code>0.75</code></td></tr>
<tr><td><code>MCP_CONFIG_PATH</code></td><td>Fallback path for MCP client configuration (used if <code>--mcp-config</code> not provided)</td></tr>
<tr><td><code>KEEP_ALIVE_INTERVAL</code></td><td>SSE keep-alive interval in milliseconds (default: 10000)</td></tr>
<tr><td><code>HF_HUB_CACHE</code></td><td>Override Hugging Face Hub cache directory</td></tr>
</tbody>
</table>
</div>
<h2 id="build-time-environment-variables"><a class="header" href="#build-time-environment-variables">Build-Time Environment Variables</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_METAL_PRECOMPILE=0</code></td><td>Skip Metal kernel precompilation (useful for CI)</td></tr>
<tr><td><code>NVCC_CCBIN</code></td><td>Set CUDA compiler path</td></tr>
<tr><td><code>CUDA_NVCC_FLAGS=-fPIE</code></td><td>Required on some Linux distributions</td></tr>
<tr><td><code>CUDA_COMPUTE_CAP</code></td><td>Override CUDA compute capability (e.g., “80” for RTX 3090)</td></tr>
</tbody>
</table>
</div>
<h2 id="server-defaults"><a class="header" href="#server-defaults">Server Defaults</a></h2>
<p>When running the HTTP server with <code>mistralrs serve</code>, these defaults apply:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Default Value</th></tr>
</thead>
<tbody>
<tr><td>Server IP</td><td><code>0.0.0.0</code> (all interfaces)</td></tr>
<tr><td>Max request body</td><td>50 MB</td></tr>
<tr><td>Max running sequences</td><td>16</td></tr>
<tr><td>Prefix cache count</td><td>16</td></tr>
<tr><td>SSE keep-alive</td><td>10 seconds</td></tr>
<tr><td>PagedAttention (CUDA)</td><td>Enabled</td></tr>
<tr><td>PagedAttention (Metal)</td><td>Disabled</td></tr>
<tr><td>PA GPU memory usage</td><td>90% of free memory</td></tr>
<tr><td>PA block size</td><td>32 tokens</td></tr>
</tbody>
</table>
</div>
<h2 id="multi-node-distributed-configuration"><a class="header" href="#multi-node-distributed-configuration">Multi-Node Distributed Configuration</a></h2>
<p>For multi-node setups, configure the head node and workers using environment variables.</p>
<h3 id="head-node"><a class="header" href="#head-node">Head Node</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_MN_GLOBAL_WORLD_SIZE</code></td><td>Total number of devices across all nodes</td></tr>
<tr><td><code>MISTRALRS_MN_HEAD_NUM_WORKERS</code></td><td>Number of worker nodes</td></tr>
<tr><td><code>MISTRALRS_MN_HEAD_PORT</code></td><td>Port for head node communication</td></tr>
</tbody>
</table>
</div>
<h3 id="worker-nodes"><a class="header" href="#worker-nodes">Worker Nodes</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>MISTRALRS_MN_WORKER_SERVER_ADDR</code></td><td>Address of head server to connect to</td></tr>
<tr><td><code>MISTRALRS_MN_WORKER_ID</code></td><td>This worker’s ID</td></tr>
<tr><td><code>MISTRALRS_MN_LOCAL_WORLD_SIZE</code></td><td>Number of GPUs on this node</td></tr>
<tr><td><code>MISTRALRS_NO_NCCL=1</code></td><td>Disable NCCL (use alternative backend)</td></tr>
</tbody>
</table>
</div>
<h2 id="see-also-1"><a class="header" href="#see-also-1">See Also</a></h2>
<ul>
<li><a href="#mistralrs-cli-reference">CLI Reference</a> - Command-line options</li>
<li><a href="#mistralrs-cli-toml-config">CLI TOML Configuration</a> - File-based configuration</li>
<li><a href="#distributed-inference-in-mistralrs">Distributed Inference</a> - Multi-node setup guide</li>
<li><a href="#pagedattention-in-mistralrs">PagedAttention</a> - Memory management options</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="engine-internals"><a class="header" href="#engine-internals">Engine Internals</a></h1>
<p>This document describes internal engine behaviors in mistral.rs.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The mistral.rs engine manages model inference through a background thread pool. Each loaded model runs in its own engine thread, which handles request queuing, batching, and execution.</p>
<h2 id="warmup-run"><a class="header" href="#warmup-run">Warmup Run</a></h2>
<p>When a text or vision model is loaded in a multi-threaded runtime, mistral.rs automatically performs a warmup (“dummy”) run:</p>
<ul>
<li>Sends a short completion request (“hello” with max 1 token) to initialize CUDA kernels and caches</li>
<li>Logs “Beginning dummy run.” when starting and “Dummy run completed in Xs.” when finished</li>
<li>Helps ensure more consistent performance for the first real user request</li>
<li>Only runs for text and vision models (not diffusion/speech)</li>
</ul>
<p>This warmup ensures that CUDA kernel compilation and memory allocation happens during model loading rather than during the first user request.</p>
<h2 id="automatic-engine-recovery"><a class="header" href="#automatic-engine-recovery">Automatic Engine Recovery</a></h2>
<p>If the inference engine thread dies unexpectedly (e.g., due to a panic), mistral.rs can automatically recover:</p>
<ul>
<li>Detects dead engine threads when sending requests</li>
<li>Automatically reboots the engine using saved configuration</li>
<li>Logs “Engine {model_id} is dead, rebooting” followed by “Successfully rebooted engine {model_id}”</li>
<li>Preserves all original configuration including KV cache settings, prefix cache, and tool callbacks</li>
</ul>
<p>This ensures high availability without manual intervention.</p>
<h2 id="thread-model"><a class="header" href="#thread-model">Thread Model</a></h2>
<p>Each model loaded in mistral.rs runs in its own dedicated engine thread:</p>
<ol>
<li><strong>Main Thread</strong>: Handles HTTP requests, CLI interaction, and dispatches work to engine threads</li>
<li><strong>Engine Threads</strong>: Each loaded model has a dedicated thread for inference</li>
<li><strong>Background Workers</strong>: Tokenization and other preprocessing can run in parallel</li>
</ol>
<p>For multi-model setups, each model gets its own engine thread, allowing true parallel inference across different models.</p>
<h2 id="see-also-2"><a class="header" href="#see-also-2">See Also</a></h2>
<ul>
<li><a href="#multi-model-support-1">Multi-Model Support</a> - Load and manage multiple models</li>
<li><a href="#configuration-reference">Configuration</a> - Environment variables affecting engine behavior</li>
<li><a href="#pagedattention-in-mistralrs">PagedAttention</a> - Memory management for high throughput</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
